{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T13:53:02.889290Z",
     "start_time": "2024-09-02T13:53:02.860996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18052,
     "status": "ok",
     "timestamp": 1678388233869,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "2OLjDqbgadV7",
    "outputId": "a38c57aa-e4dd-46fe-aca2-9e6bd6d0cd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/temporal')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Extra_and_Unused')\n",
    "    root = \"/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project\"\n",
    "else:\n",
    "    print(\"Running local\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4523,
     "status": "ok",
     "timestamp": 1678388238390,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "ucIGvQ7GczZb",
    "outputId": "0bdf55db-463b-4047-8259-f7dc96b407f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "from Zamuro_DataLoader import SoundscapeData\n",
    "from Models import Model\n",
    "from Models import Encoder\n",
    "from Models import Decoder\n",
    "from Models import VectorQuantizer\n",
    "from Models import VectorQuantizerEMA\n",
    "from VQ_VAE_training_functions import TestModel, TrainModel # For VQ-VAE\n",
    "# from PosAE_training_functions import posautoencoding_m1 as AE, TestModel, TrainModel # For PosAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jPCv40Ugk0I",
    "outputId": "b093c1e1-5419-407e-a180-ab158e8f79ba"
   },
   "outputs": [],
   "source": [
    "def featuring_autoencoders(dataset, date_format, model, len_features=None, save=True, identifier=None):\n",
    "    training_loader = DataLoader(dataset, batch_size=1)\n",
    "    iterator = iter(training_loader)\n",
    "    testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n",
    "\n",
    "    training_recorder_list = []\n",
    "    training_hour_list = []\n",
    "    training_minute_list = []\n",
    "    delete_samples = []\n",
    "    training_path_samples = []\n",
    "    training_samples_list_torch = []\n",
    "    \n",
    "    if len_features == None:\n",
    "        len_features = len(training_loader)\n",
    "    else:\n",
    "        len_features = len_features\n",
    "        \n",
    "    batch = int(len_features*0.1)\n",
    "    \n",
    "    for id in range(len_features):\n",
    "        if (id+1)% batch == 0:\n",
    "            print(f\"id: {id + 1} of {len_features}\")\n",
    "        try:\n",
    "            originals, reconstructions, encodings, label, loss, path = testing.reconstruct()\n",
    "        except:\n",
    "            print(f\"error id: {id}\")\n",
    "            delete_samples.append(id)\n",
    "            continue\n",
    "        print(encodings.shape)\n",
    "    #     encodings_size = encodings[0].shape\n",
    "        encodings = encodings.to(\"cpu\").detach()\n",
    "        encodings = encodings.reshape(encodings.shape[0],\n",
    "                                    encodings.shape[1]*encodings.shape[2]*encodings.shape[3])\n",
    "        training_samples_list_torch.append(encodings)\n",
    "        label[\"recorder\"]=np.repeat(label[\"recorder\"][0][0],5)\n",
    "#         training_recorder_list.append(label[\"recorder\"])\n",
    "#         training_hour_list.append(label[\"hour\"].reshape(label[\"hour\"].shape[0]*label[\"hour\"].shape[1]))\n",
    "#         training_minute_list.append(label[\"minute\"].reshape(label[\"minute\"].shape[0]*label[\"minute\"].shape[1]))\n",
    "\n",
    "\n",
    "        path = np.asarray(path, dtype=\"U32\")\n",
    "        path = np.repeat(path, 5)\n",
    "        indexer = [\"_1\",\"_2\",\"_3\",\"_4\",\"_5\"]\n",
    "        indexer = np.asarray(indexer)\n",
    "        # path = path.astype(\"float64\")\n",
    "        for i in range(len(path)):\n",
    "            path[i] = label[\"recorder\"][0] + \"_\" + path[i] + indexer[i]\n",
    "\n",
    "        training_path_samples.append(path)\n",
    "    \n",
    "#     training_recorder_list = torch.cat(training_recorder_list,dim=0)\n",
    "#     training_hour_list = torch.cat(training_hour_list,dim=0)\n",
    "#     training_minute_list = torch.cat(training_minute_list,dim=0)\n",
    "    training_samples_list_torch = torch.cat(training_samples_list_torch, dim=0)\n",
    "    \n",
    "    if save == True:\n",
    "    \n",
    "        if \"filters\" in dataset.kwargs.keys():\n",
    "            for value in dataset.filters.values():\n",
    "                date_format = f\"{date_format}_{value}\" \n",
    "        if identifier != None:\n",
    "            date_format = f\"{date_format}_{identifier}\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        torch.save(training_path_samples, f\"temporal_zamuro/Features/VQ-VAE/VQ_test_path_samples_{date_format}.pth\")\n",
    "        torch.save(training_samples_list_torch, f\"temporal_zamuro/Features/VQ-VAE/VQ_features_{date_format}.pth\")\n",
    "#         training_labels_list = {\"recorder\": training_recorder_list, \"hour\": training_hour_list, \"minute\": training_minute_list}\n",
    "#         torch.save(training_labels_list, f\"temporal_zamuro/Features/epochs_50/AE_labels_{date_format}.pth\")\n",
    "#         torch.save(delete_samples, f\"temporal_zamuro/Features/epochs_50/AE_test_corrupted_samples_list_{date_format}.pth\")\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8199,
     "status": "ok",
     "timestamp": 1678388246582,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "qemaBluJa22A",
    "outputId": "30519d6b-ea58-4423-cb89-b2c5ffb7d854",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 10\n",
      "RZUD06.csv\n",
      "/media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "error id: 74\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "id: 87 of 871\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n",
      "torch.Size([5, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# models = [\"epoch_50_training.pth\", \"epoch_40_training.pth\", \"epoch_30_training.pth\", \"epoch_20_training.pth\", \"epoch_10_training.pth\"]\n",
    "\n",
    "date_format = \"month_8_day_29_hour_12\"\n",
    "epochs = [10]\n",
    "for i, epoch in enumerate(epochs):\n",
    "    print(f\"Running epoch: {epoch}\")\n",
    "    model_name = f\"{root}/Zamuro/temporal_zamuro/models/model_VQ_batch_size_14_num_embeddings_256_embedding_dim_128__month_8_day_29_hour_12_training/epoch_{epoch}_training.pth\"\n",
    "    model = Model(num_hiddens=64, num_embeddings=256, embedding_dim=128, commitment_cost=0.25, decay=0.99).to(device)\n",
    "    model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "    import os\n",
    "    folders = os.listdir(f\"Complementary_Files/Audios_Zamuro/\")\n",
    "\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        fold = folder.split(\".\")[0]        \n",
    "        filters = {\"rain_FI\": \"NO\"}\n",
    "        dataset = SoundscapeData('media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/',\n",
    "                             dataframe_path=f\"Complementary_Files/Audios_Zamuro/{folder}\",\n",
    "                             audio_length=12, ext=\"wav\",\n",
    "                             win_length=1028, filters=filters)\n",
    "        featuring_autoencoders(dataset, f\"{date_format}_rain\", model=model, len_features=None, save=True, identifier=f\"{fold}_epoch_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Ruta de la carpeta que contiene los archivos\n",
    "input_folder = '/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Zamuro/temporal_zamuro/Features/VQ-VAE/'\n",
    "\n",
    "# Ruta donde se crearán las nuevas carpetas\n",
    "output_folder = '/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Zamuro/temporal_zamuro/Features/VQ-VAE//ordered'\n",
    "\n",
    "# Listar todos los archivos en la carpeta de entrada\n",
    "files = os.listdir(input_folder)\n",
    "\n",
    "# Iterar sobre todos los archivos\n",
    "for file in files:\n",
    "    # Verificar si el archivo termina en '.pth'\n",
    "    if file.endswith('.pth'):\n",
    "        # Extraer el número del sufijo (el último número antes de .pth)\n",
    "        suffix = file.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        # Crear el nombre de la carpeta de destino basado en el sufijo\n",
    "        target_folder = os.path.join(output_folder, suffix)\n",
    "        \n",
    "        # Crear la carpeta si no existe\n",
    "        os.makedirs(target_folder, exist_ok=True)\n",
    "        \n",
    "        # Mover el archivo a la carpeta de destino\n",
    "        shutil.move(os.path.join(input_folder, file), os.path.join(target_folder, file))\n",
    "\n",
    "print(\"Archivos organizados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "audios_zamuro = pd.read_csv(f\"{root}/Zamuro/Complementary_Files/zamuro_audios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = list(set(audios_zamuro[\"field_number_PR\"]))\n",
    "lista.sort()\n",
    "\n",
    "month = 7\n",
    "day = 25\n",
    "hour = 50\n",
    "date_format = f\"month_{month}_day_{day}_{hour}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load(f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_features_{date_format}_rain_NO_{lista[0]}_epoch_7.pth\",  map_location=torch.device('cpu'))\n",
    "y = torch.load(f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_test_path_samples_{date_format}_rain_NO_{lista[0]}_epoch_7.pth\",  map_location=torch.device('cpu'))\n",
    "for i, csv in enumerate(lista[1::]):\n",
    "    X_i = torch.load(f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_features_{date_format}_rain_NO_{csv}_epoch_7.pth\",  map_location=torch.device('cpu'))\n",
    "    y_i = torch.load(f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_test_path_samples_{date_format}_rain_NO_{csv}_epoch_7.pth\",  map_location=torch.device('cpu'))\n",
    "    X = torch.cat((X,X_i),0)\n",
    "    y = y + y_i\n",
    "torch.save(X, f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_features_Zamuro.pth\")\n",
    "torch.save(y, f\"temporal_zamuro/Features/standarized_epochs_50/7/AE_test_path_samples_Zamuro.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
