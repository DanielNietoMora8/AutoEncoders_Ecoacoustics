{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KwQ8Zp3llyi-",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipython: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f298b33a440>\n",
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "print(f\"ipython: {str(get_ipython())}\")\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = '/media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/'\n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'Complementary_Files/zamuro_audios.csv'\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZeYXtNUUhRWh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielnieto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from six.moves import xrange\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Zamuro_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cMG_87FKb26h",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/\n"
     ]
    }
   ],
   "source": [
    "filters = {\"rain_FI\": \"NO\"}\n",
    "dataset = SoundscapeData('media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/',\n",
    "                         dataframe_path=\"Complementary_Files/zamuro_audios.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.98), len(dataset) - round(len(dataset)*0.98)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"AE-Zamuro\",\n",
    "    \"audio_length\": dataset.audio_length,\n",
    "    \"batch_size\" : 20,\n",
    "    \"num_epochs\": 10,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"gamma_lr\" : 0.1,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\" : \"Audios Zamuro\",\n",
    "    \"architecture\": \"AE128\",\n",
    "    \"win_length\" : dataset.win_length,\n",
    "    \"step_size\": 3,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = config[\"step_size\"], gamma = config[\"gamma_lr\"] )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"num_training_updates\"] = len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6YtKCwnNnDYP",
    "outputId": "d5292f0e-2945-4d1b-c15c-25211c656186",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Zamuro/wandb/run-20240718_163216-v1zethtm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielnieto/AE-Zamuro/runs/v1zethtm' target=\"_blank\">spring-eon-58</a></strong> to <a href='https://wandb.ai/danielnieto/AE-Zamuro' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielnieto/AE-Zamuro' target=\"_blank\">https://wandb.ai/danielnieto/AE-Zamuro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielnieto/AE-Zamuro/runs/v1zethtm' target=\"_blank\">https://wandb.ai/danielnieto/AE-Zamuro/runs/v1zethtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 1 of 3189 \t loss: 1.269\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 2 of 3189 \t loss: 1.2684\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 3 of 3189 \t loss: 1.2677\n",
      "error\n",
      "Trying to create tensor with negative dimension -154028: [1, -154028]\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 5 of 3189 \t loss: 1.2669\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 6 of 3189 \t loss: 1.2662\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 7 of 3189 \t loss: 1.2653\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 8 of 3189 \t loss: 1.2644\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 9 of 3189 \t loss: 1.2634\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 10 of 3189 \t loss: 1.2623\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 11 of 3189 \t loss: 1.261\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 12 of 3189 \t loss: 1.2595\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 13 of 3189 \t loss: 1.2577\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 14 of 3189 \t loss: 1.2552\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 15 of 3189 \t loss: 1.2524\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 16 of 3189 \t loss: 1.2486\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 17 of 3189 \t loss: 1.2425\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 18 of 3189 \t loss: 1.2358\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 19 of 3189 \t loss: 1.2233\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 20 of 3189 \t loss: 1.2086\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 21 of 3189 \t loss: 1.1754\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 22 of 3189 \t loss: 1.1612\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 23 of 3189 \t loss: 1.1278\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 24 of 3189 \t loss: 1.1054\n",
      "encoder_shape:  torch.Size([100, 64, 9, 9])\n",
      "decoder_shape:  torch.Size([100, 1, 515, 515])\n",
      "epoch: 1 of 10 \t iteration: 25 of 3189 \t loss: 1.0848\n"
     ]
    }
   ],
   "source": [
    "Training = TrainModel(model)\n",
    "model, logs, run_name = Training.fordward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "torch.save(config,f'temporal/configs/config_{run_name}_day_{time.day}_hour_{time.hour}.pth')\n",
    "torch.save(dataset_test, f\"temporal/datasets/dataset_test_ae_zamuro_{time.day}_70%.pth\")\n",
    "torch.save(dataset_train, f\"temporal/datasets/dataset_train_ae_zamuro_{time.day}_70%.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5sjhlDScl3G"
   },
   "outputs": [],
   "source": [
    "dataset_test = torch.load(f'temporal/dataset_test_ae_jaguas')\n",
    "dataset_train = torch.load(f'temporal/dataset_train_ae_jaguas')\n",
    "model.load_state_dict(torch.load(f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBePtMUv5MD7"
   },
   "outputs": [],
   "source": [
    "# root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Porce_2019'\n",
    "\n",
    "\n",
    "# dataset = SoundscapeData(root_path, audio_length=12, ext=\"WAV\", win_length=1028)\n",
    "# dataset_train, dataset_test = random_split(dataset,\n",
    "#                                            [round(len(dataset)*0.7), len(dataset) - round(len(dataset)*0.7)], \n",
    "#                                            generator=torch.Generator().manual_seed(1024))\n",
    "# Dataset_train = DataLoader(dataset_train, batch_size=54, shuffle=True)\n",
    "# Dataset = DataLoader(dataset_test, batch_size=54, shuffle=True)\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=100)\n",
    "test_loader = DataLoader(dataset_test, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWREIMjnUf9x"
   },
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, label, loss= testing.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFQvOLxb7Ytc"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin[0:4],2)\n",
    "testing.plot_psd(wav_origin[10:14],2)\n",
    "#testing.plot_psd(wav_origin[18:22],2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons[0:4],2)\n",
    "testing.plot_psd(wav_recons[10:14],2)\n",
    "#testing.plot_psd(wav_recons[18:22],2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBEMoCDtUhDZ"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin,2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons,2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0E_jW1OyNlm"
   },
   "outputs": [],
   "source": [
    "plt.plot(wav_origin[12,0])\n",
    "plt.plot(wav_recons[12,0], color='red', alpha = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68wxgeupqxBv"
   },
   "outputs": [],
   "source": [
    "# encodings_size = [64,9,9]\n",
    "# plt.figure(figsize=(18, 18))\n",
    "# model.to(\"cpu\")\n",
    "# for i, spec in enumerate(kmeans.cluster_centers_):\n",
    "#     encodings = spec.reshape(encodings_size)\n",
    "#     encodings = torch.tensor(encodings).float()\n",
    "#     decodings = model.decoder(encodings).detach().numpy()\n",
    "#     plt.subplot(9, 9, i + 1)\n",
    "#     plt.imshow(decodings[0,:,:], cmap=\"inferno\", interpolation=\"nearest\",vmin=0, vmax=0.02)\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
