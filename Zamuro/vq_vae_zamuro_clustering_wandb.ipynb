{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "In9UTndxqdMe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    !pip install torchaudio\n",
    "    !pip install umap\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    output.clear()\n",
    "    root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = 'media/mirp_ai/DATA1/Jaguas_2018'\n",
    "    \n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JABwOJv4qVAS"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from six.moves import xrange\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Zamuro_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxU_YNsDqVAZ"
   },
   "source": [
    "## Train\n",
    "\n",
    "We use the hyperparameters from the author's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kxoZkEI0-qvh"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "class TestModel:\n",
    "\n",
    "    def __init__(self, model, iterator, num_views):\n",
    "        self._model = model\n",
    "        self._iterator = iterator\n",
    "        self.num_views = num_views\n",
    "\n",
    "    def plot_waveform(self, waveform, n_rows=2, directory=None):\n",
    "        fig, axs = plt.subplots(n_rows, figsize=(10, 6))\n",
    "        for i in range(len(waveform)):\n",
    "            axs[i].plot(waveform[i,0])\n",
    "            if directory != None:\n",
    "                scaled = np.int16(waveform[i,0]/np.max(np.abs(waveform[i,0])) * 32767)\n",
    "                write(directory + str(i) + '.wav', 22050, scaled)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def waveform_generator(self, spec, n_fft=1028, win_length=1028, base_win=256, plot=False):\n",
    "        spec = spec.cdouble()\n",
    "        spec = spec.to(\"cpu\")\n",
    "        hop_length = int(np.round(base_win/win_length * 172.3))\n",
    "        transformation = audio_transform.InverseSpectrogram(n_fft=n_fft, win_length=win_length)\n",
    "        waveform = transformation(spec)\n",
    "        waveform = waveform.cpu().detach().numpy()\n",
    "        return waveform\n",
    "    \n",
    "    def plot_psd(self, waveform):\n",
    "        for wave in waveform:\n",
    "            plt.psd(wave)\n",
    "\n",
    "    def plot_reconstructions(self, imgs_original, imgs_reconstruction, num_views:int = 8):\n",
    "        output = torch.cat((imgs_original[0:self.num_views], imgs_reconstruction[0:self.num_views]), 0)\n",
    "        img_grid = make_grid(output, nrow=self.num_views, pad_value=20)\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        ax.imshow(librosa.power_to_db(img_grid[1, :, :].cpu()), origin=\"lower\")\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def reconstruct(self):\n",
    "        self._model.eval()\n",
    "        (valid_originals, _, label, _) = next(self._iterator)\n",
    "        valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1]\n",
    "                                                          * valid_originals.shape[2], valid_originals.shape[3],\n",
    "                                                          valid_originals.shape[4]))\n",
    "        valid_originals = torch.unsqueeze(valid_originals,1)\n",
    "\n",
    "        valid_originals = valid_originals.to(device)\n",
    "\n",
    "        vq_output_eval = self._model._pre_vq_conv(self._model._encoder(valid_originals))\n",
    "        _, valid_quantize, _, _ = self._model._vq_vae(vq_output_eval)\n",
    "        \n",
    "        valid_encodings = self._model._encoder(valid_originals)\n",
    "        print(valid_quantize.shape)\n",
    "        \n",
    "        valid_reconstructions = self._model._decoder(valid_quantize)\n",
    "\n",
    "        recon_error = F.mse_loss(valid_originals, valid_reconstructions)\n",
    "\n",
    "        return valid_originals, valid_reconstructions, valid_encodings, recon_error\n",
    "\n",
    "    def run(self, plot=True, wave_return=True, wave_plot=True, directory=None):\n",
    "        wave_original = []\n",
    "        wave_reconstructions = []\n",
    "        originals, reconstructions, error = self.reconstruct() \n",
    "        if plot:\n",
    "            self.plot_reconstructions(originals, reconstructions)\n",
    "        if wave_return:\n",
    "            wave_original = self.waveform_generator(originals)\n",
    "            wave_reconstructions = self.waveform_generator(reconstructions)\n",
    "            if wave_plot:\n",
    "                self.plot_waveform(wave_original, len(wave_original), directory=\"originals\")\n",
    "                self.plot_waveform(wave_reconstructions, len(wave_reconstructions), directory=\"reconstructions\")\n",
    "\n",
    "        return originals, reconstructions, wave_original, wave_reconstructions, error\n",
    "\n",
    "\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def wandb_init(self, config, keys=[\"batch_size\", \"num_embeddings\", \"embedding_dim\"]):\n",
    "        try:\n",
    "            run_name = \"VQ_\"\n",
    "            for key in keys:\n",
    "                if key in config.keys():\n",
    "                    run_name = run_name + key + \"_\" + str(config[key]) + \"_\"\n",
    "                else:\n",
    "                    run_name = run_name + str(key)\n",
    "\n",
    "            wandb.login()\n",
    "            wandb.finish()\n",
    "            wandb.init(project=\"VQ-VAE-Jaguas\", config=config)\n",
    "            wandb.run.name = run_name\n",
    "            wandb.run.save()\n",
    "            wandb.watch(self._model, F.mse_loss, log=\"all\", log_freq=1)\n",
    "            is_wandb_enable = True         \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            is_wandb_enable = False\n",
    "\n",
    "        return is_wandb_enable, run_name\n",
    "\n",
    "    def wandb_logging(self, dict):\n",
    "        for keys in dict:\n",
    "            wandb.log({keys: dict[keys]})\n",
    "\n",
    "\n",
    "    def fordward(self, training_loader, test_loader, config):\n",
    "#         iterator = iter(test_loader)\n",
    "        wandb_enable, run_name = self.wandb_init(config)\n",
    "        optimizer = config[\"optimizer\"]\n",
    "        scheduler = config[\"scheduler\"]\n",
    "\n",
    "        train_res_recon_error = []\n",
    "        train_res_perplexity = []\n",
    "        logs = []\n",
    "        best_loss = 10000\n",
    "\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            iterator = iter(test_loader)\n",
    "            iterator_train = iter(training_loader)\n",
    "            for i in xrange(config[\"num_training_updates\"]):\n",
    "                self._model.train()\n",
    "                try:\n",
    "                    (data, _, _, _) = next(iterator_train)\n",
    "                except Exception as e:\n",
    "                    print(\"error\")\n",
    "                    print(e)\n",
    "                    logs.append(e)\n",
    "                    continue\n",
    "\n",
    "                data = torch.reshape(data, (data.shape[0] * data.shape[1] * data.shape[2], data.shape[3], data.shape[4]))\n",
    "                data = torch.unsqueeze(data,1)\n",
    "                data = data.to(device)\n",
    "                # print(data.shape)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                vq_loss, data_recon, perplexity = self._model(data)\n",
    "                # print(data_recon.shape)\n",
    "                \n",
    "                recon_error = F.mse_loss(data_recon, data) #/ data_variance\n",
    "                loss = recon_error + vq_loss\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                print(f'epoch: {epoch+1} of {config[\"num_epochs\"]} \\t iteration: {(i+1)} of {config[\"num_training_updates\"]} \\t loss: {np.round(loss.item(),4)} \\t recon_error: {np.round(recon_error.item(),4)} \\t vq_loss: {np.round(vq_loss.item(),4)}')\n",
    "                dict = {\"loss\":loss.item(),\n",
    "                        \"perplexity\":perplexity.item(),\n",
    "                        \"recon_error\": recon_error,\n",
    "                        \"vq_loss\": vq_loss}\n",
    "#                 step = epoch*config[\"num_training_updates\"] + i\n",
    "                self.wandb_logging(dict)\n",
    "                \n",
    "                period = 200\n",
    "                if (i+1) % period == 0:\n",
    "                    try:\n",
    "                        test_ = TestModel(self._model, iterator, 8)\n",
    "                        #torch.save(model.state_dict(),f'model_{epoch}_{i}.pkl')\n",
    "                        originals, reconstructions, encodings, test_error = test_.reconstruct()\n",
    "                        fig = test_.plot_reconstructions(originals, reconstructions, 8)\n",
    "                        images = wandb.Image(fig, caption= f\"recon_error: {np.round(test_error.item(),4)}\")\n",
    "                        self.wandb_logging({\"examples\": images, \"step\":(i+1)//period})\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"error\")\n",
    "                        logs.append(e)\n",
    "                        continue\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "#                 if recon_error < 0.05:\n",
    "#                     wandb.alert(\n",
    "#                     title='High accuracy',\n",
    "#                     text=f'Recon error {recon_error} is lower than 0.05',\n",
    "#                     level=AlertLevel.WARN,\n",
    "#                     wait_duration=timedelta(minutes=5)\n",
    "#                                 )\n",
    "#                     _time = datetime.datetime.now()       \n",
    "#                     torch.save(self._model.state_dict(),f'{run_name}_low_error.pkl')\n",
    "#                 else:\n",
    "#                     pass\n",
    "\n",
    "            scheduler.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            time = datetime.datetime.now()\n",
    "            torch.save(self._model.state_dict(),\n",
    "                       f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_epoch_{epoch+1}.pth')\n",
    "            clear_output()\n",
    "            print(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "\n",
    "        wandb.finish()\n",
    "        return self._model, logs, run_name\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S9UACcjeqVAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mirp_ai/DATA1/Jaguas_2018\n"
     ]
    }
   ],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.2), len(dataset) - round(len(dataset)*0.2)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"VQ-VAE-Jaguas\",\n",
    "    \"batch_size\" : 20,\n",
    "    \"num_epochs\": 10,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"embedding_dim\" : 128,\n",
    "    \"num_embeddings\" : 256,\n",
    "    \"commitment_cost\" : 0.25,\n",
    "    \"decay\" : 0.99,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\": \"Audios Jaguas\",\n",
    "    \"architecture\": \"VQ-VAE\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle = False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = Model(config[\"num_hiddens\"],\n",
    "              config[\"num_embeddings\"], config[\"embedding_dim\"], \n",
    "              config[\"commitment_cost\"], config[\"decay\"]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1 )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"audio_length\"] = dataset.audio_length\n",
    "config[\"num_training_updates\"] = len(training_loader)\n",
    "config[\"win_length\"] = dataset.win_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jInS0IjTqVAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▆▃▁▂▄▄▄▂▃▄▆▂▁▄▄▄▅▄▆▃▄▆▆▁▄▆▄▆▂▄▃▂▂▄█▅▂▂▄</td></tr><tr><td>perplexity</td><td>▁▁▁▂▂▂▃▃▃█▃▅▄▅▄▅▅▄▆▄▄▄▅▅▃▅█▃▄▅▃▅▄▄▄▅▅▇▂▅</td></tr><tr><td>recon_error</td><td>▇▆▃▁▂▄▄▄▂▃▄▆▂▁▄▄▄▅▄▆▃▄▆▆▁▄▆▄▆▂▄▃▂▂▄█▅▂▂▄</td></tr><tr><td>step</td><td>▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅▆▁▃▅█</td></tr><tr><td>vq_loss</td><td>▆▅█▂▁▁▁▃▁▃▂▃▃▄▂▃▃▃▄▃▃▃▃▄▂▄▅▃▄▄▂▃▂▃▂▄▃▅▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.16909</td></tr><tr><td>perplexity</td><td>22.14787</td></tr><tr><td>recon_error</td><td>0.16751</td></tr><tr><td>step</td><td>5</td></tr><tr><td>vq_loss</td><td>0.00158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-river-459</strong> at: <a href='https://wandb.ai/danielnieto/VQ-VAE-Jaguas/runs/npfzsgt1' target=\"_blank\">https://wandb.ai/danielnieto/VQ-VAE-Jaguas/runs/npfzsgt1</a><br/>Synced 7 W&B file(s), 50 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_090014-npfzsgt1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    4917 KB |    8140 MB |   97251 GB |   97251 GB |\\n|       from large pool |       0 KB |    8135 MB |   97120 GB |   97120 GB |\\n|       from small pool |    4917 KB |       6 MB |     130 GB |     130 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    4917 KB |    8140 MB |   97251 GB |   97251 GB |\\n|       from large pool |       0 KB |    8135 MB |   97120 GB |   97120 GB |\\n|       from small pool |    4917 KB |       6 MB |     130 GB |     130 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    4754 MB |    9320 MB |   50062 MB |   45308 MB |\\n|       from large pool |    4748 MB |    9312 MB |   50036 MB |   45288 MB |\\n|       from small pool |       6 MB |       8 MB |      26 MB |      20 MB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |    1227 KB |    1324 MB |   23456 GB |   23456 GB |\\n|       from large pool |       0 KB |    1323 MB |   23229 GB |   23229 GB |\\n|       from small pool |    1227 KB |       2 MB |     227 GB |     227 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |      53    |      96    |    3816 K  |    3816 K  |\\n|       from large pool |       0    |      38    |     548 K  |     548 K  |\\n|       from small pool |      53    |      64    |    3268 K  |    3268 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |      53    |      96    |    3816 K  |    3816 K  |\\n|       from large pool |       0    |      38    |     548 K  |     548 K  |\\n|       from small pool |      53    |      64    |    3268 K  |    3268 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      20    |      41    |     204    |     184    |\\n|       from large pool |      17    |      37    |     191    |     174    |\\n|       from small pool |       3    |       4    |      13    |      10    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       6    |      34    |    1556 K  |    1556 K  |\\n|       from large pool |       0    |      27    |     237 K  |     237 K  |\\n|       from small pool |       6    |       9    |    1318 K  |    1318 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training = TrainModel(model)\n",
    "model, logs, run_name = Training.fordward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'{run_name}_day_{time.day}_hour_{time.hour}_final.pkl')\n",
    "torch.save(config,f'config_{run_name}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "#np.savetxt(\"corrupted_files.csv\", logs, delimiter=\",\")\n",
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IL0WeGtiVSha"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/Best_Model_Embedding_256_VQ_audio_length 12_win_length 1028_batch_size 8__5.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wandb.finish()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModels/Best_Model_Embedding_256_VQ_audio_length 12_win_length 1028_batch_size 8__5.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/Best_Model_Embedding_256_VQ_audio_length 12_win_length 1028_batch_size 8__5.pkl'"
     ]
    }
   ],
   "source": [
    "# wandb.finish()\n",
    "model.load_state_dict(torch.load(f'Models/Best_Model_Embedding_256_VQ_audio_length 12_win_length 1028_batch_size 8__5.pkl', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_id8rZguQ7g"
   },
   "outputs": [],
   "source": [
    "root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Porce_2019'\n",
    "\n",
    "\n",
    "dataset = SoundscapeData(root_path=root_path, audio_length=12, ext='WAV', win_length=config[\"win_length\"])\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                          [round(len(dataset)*0.10), len(dataset) - round(len(dataset)*0.10)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle = False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "iterator = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8POCP5-vLY_8"
   },
   "outputs": [],
   "source": [
    "spec, record, _ = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx3veUvuULAM"
   },
   "outputs": [],
   "source": [
    "spec_2 = audio_transform.Spectrogram(n_fft=1028, win_length=1028, window_fn=torch.hamming_window,power=2)(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiqdVEpbmRSj"
   },
   "outputs": [],
   "source": [
    "a = reconstruction.permute(1,0,2,3)\n",
    "b = a.squeeze(dim=0)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJKTdxXUmxgC"
   },
   "outputs": [],
   "source": [
    "b = b.type(torch.complex64).to(\"cpu\")\n",
    "wav = audio_transform.InverseSpectrogram(n_fft=1028, win_length=1028, hop_length=514)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T4SbzcU214s"
   },
   "outputs": [],
   "source": [
    "Test = TestModel(model, iterator, 8)\n",
    "originals, reconstruction, wav_ori, wav_recons, error = Test.run(wave_plot=True)\n",
    "a = Test.waveform_generator(originals)\n",
    "b = Test.waveform_generator(reconstruction)\n",
    "waves = [a,b]\n",
    "Test.plot_psd(waves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6EH9JEPaU_V"
   },
   "outputs": [],
   "source": [
    "import torchaudio.transforms as audio_transform\n",
    "reconstruction = reconstruction.cdouble()\n",
    "reconstruction = reconstruction.to(\"cpu\")\n",
    "originals = originals.cdouble()\n",
    "originals = originals.to(\"cpu\")\n",
    "transformation = audio_transform.InverseSpectrogram(n_fft=1025, win_length = 1025)\n",
    "waveform = transformation(reconstruction)\n",
    "waveform_original = transformation(originals)\n",
    "waveform = waveform.cpu().detach().numpy()\n",
    "waveform_original = waveform_original.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF1EaIJINt6J"
   },
   "outputs": [],
   "source": [
    "diff = waveform[0] - waveform_original[0]\n",
    "diff = diff**2\n",
    "plt.psd(waveform_original)\n",
    "plt.psd(waveform)\n",
    "#plt.psd(diff[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1wMLc0TeGV62ac_9fyr69EM2ODYm3ygO4",
     "timestamp": 1656687641496
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
