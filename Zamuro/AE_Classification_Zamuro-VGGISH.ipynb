{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1a613c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T16:04:40.092443Z",
     "start_time": "2024-04-22T16:04:40.078736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "\n",
    "    !pip install joypy\n",
    "\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics\"\n",
    "    root_path = \"/media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido\"\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1eb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Normalizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from Zamuro_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023cc930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_audios_Zamuro = f\"{root}/Zamuro/Complementary_Files/zamuro_audios.csv\"\n",
    "root_recorders_Zamuro = f\"{root}/Zamuro/Complementary_Files/zamuro_recorders.csv\"\n",
    "root_vggish_Zamuro = f\"{root}/Zamuro/vggish/Features_vggish\"\n",
    "\n",
    "audios = pd.read_csv(root_audios_Zamuro, index_col=0)\n",
    "recorders = pd.read_csv(root_recorders_Zamuro, index_col=0)\n",
    "\n",
    "def combinar_nombre_ubicacion(row):\n",
    "    return f\"{row['field_number_PR']}_{row['Filename']}\"\n",
    "\n",
    "# Aplicando la función a cada fila del DataFrame para crear la nueva columna\n",
    "audios['Filename_'] = audios.apply(combinar_nombre_ubicacion, axis=1)\n",
    "\n",
    "audios.set_index(\"Filename_\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7ccb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_features.pth\")\n",
    "y_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_filenames.pth\")\n",
    "y_vgg = [i.split('/')[6:8] for i in y_vgg]\n",
    "for i in range(len(y_vgg)):\n",
    "    y_vgg[i] = f\"{y_vgg[i][0]}_{y_vgg[i][1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f16cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_add = []\n",
    "for i in range(len(X_vgg)):\n",
    "    if(X_vgg[i].shape == torch.Size([62, 128])):\n",
    "        X_add.append(i)\n",
    "X_vgg2 = []\n",
    "y_vgg2 = []\n",
    "for i, value in enumerate(X_add):\n",
    "    X_vgg2.append(X_vgg[value])\n",
    "    y_vgg2.append(y_vgg[value])\n",
    "X_vgg2 = torch.stack(X_vgg2)\n",
    "X_vgg2 = X_vgg2.numpy()\n",
    "X_vgg2 = X_vgg2.reshape(X_vgg2.shape[0], X_vgg2.shape[1]*X_vgg2.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8072123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vgg = pd.DataFrame(X_vgg2)\n",
    "df_vgg[\"y\"] = y_vgg2\n",
    "df_vgg\n",
    "df_vgg.set_index(\"y\", inplace=True, drop=False)\n",
    "df_vgg['cover'] = df_vgg.index.map(audios['cover'])\n",
    "df_vgg['rain_FI'] = df_vgg.index.map(audios['rain_FI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8041d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(row):\n",
    "    parts = row.split('_')\n",
    "    location = parts[0]\n",
    "    date = parts[1]\n",
    "    time = parts[2].split('.')[0]  # Eliminar la extensión .WAV\n",
    "    day = date[-2:]  # Últimos dos caracteres para el día\n",
    "    hour = time[:2]\n",
    "    return pd.Series([location, day, hour])\n",
    "\n",
    "# Aplicar la función a la columna 'y' y crear nuevas columnas\n",
    "df_vgg[['location', 'day', 'hour']] =df_vgg['y'].apply(extract_parts)\n",
    "\n",
    "def define_hour_stage(hour):\n",
    "    hour = int(hour)\n",
    "    if 5 <= hour <= 8:\n",
    "        return 'morning'\n",
    "    elif 9 <= hour <= 16:\n",
    "        return 'day'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "df_vgg['hour_stage'] =df_vgg['hour'].apply(define_hour_stage)\n",
    "df_vgg.set_index(\"y\", inplace=True, drop=False)\n",
    "df_vgg['cover'] = df_vgg.index.map(audios['cover'])\n",
    "df_vgg = df_vgg[df_vgg['rain_FI'] == 'NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8549f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7445887445887446\n",
      "f1: 0.47520272058022445\n",
      "recall 0.45817307692307696\n",
      "Accuracy: 0.7348560079443893\n",
      "f1: 0.5510218591597013\n",
      "recall 0.5216553221136321\n",
      "Accuracy: 0.6949585194639438\n",
      "f1: 0.5513061507053496\n",
      "recall 0.5320303323778464\n",
      "Accuracy: 0.6902298850574713\n",
      "f1: 0.5680789811128818\n",
      "recall 0.5508497812247198\n",
      "Accuracy: 0.6886467889908257\n",
      "f1: 0.5562443397539469\n",
      "recall 0.5407705627538045\n",
      "Accuracy: 0.672939649578196\n",
      "f1: 0.512846129733476\n",
      "recall 0.5004684600207076\n",
      "Accuracy: 0.7117117117117117\n",
      "f1: 0.6145351814079758\n",
      "recall 0.6024766836331318\n",
      "Accuracy: 0.7027632561613144\n",
      "f1: 0.5843040242426039\n",
      "recall 0.5652644558852652\n",
      "Accuracy: 0.6933139534883721\n",
      "f1: 0.538519657203254\n",
      "recall 0.5189712455566114\n",
      "Accuracy: 0.6822857142857143\n",
      "f1: 0.5250763537892251\n",
      "recall 0.5081107702308473\n",
      "Accuracy: 0.75\n",
      "f1: 0.5220664392642104\n",
      "recall 0.4889752407644405\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "accuracies_vgg = []\n",
    "f1_scores_vgg = []\n",
    "recalls_vgg = []\n",
    "df_day={}\n",
    "for i in [\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\"]:\n",
    "    df_day = df_vgg[df_vgg['day'].isin([i])]\n",
    "    X = np.asarray(df_day.loc[:,0:7935])\n",
    "    y = np.asarray(df_day.loc[:,\"cover\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "    clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "    clf_rf.fit(X_train, y_train)\n",
    "    y_pred_rf = clf_rf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "    recall = metrics.recall_score(y_test, y_pred_rf, average=\"macro\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"f1:\", f1_score)\n",
    "    print(\"recall\", recall)\n",
    "\n",
    "    accuracies_vgg.append(accuracy)\n",
    "    f1_scores_vgg.append(f1_score)\n",
    "    recalls_vgg.append(recall)\n",
    "    \n",
    "np.save(f\"{root}/Zamuro/temporal_zamuro/zamuro_classification_results/accuracies_vgg.npy\", accuracies_vgg)\n",
    "np.save(f\"{root}/Zamuro/temporal_zamuro/zamuro_classification_results/f1_scores_vgg.npy\", f1_scores_vgg)\n",
    "np.save(f\"{root}/Zamuro/temporal_zamuro/zamuro_classification_results/recalls_vgg.npy\", recalls_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7069b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/numpy/lib/function_base.py:5071: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = asarray(arr)\n",
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# y_ = np.asarray(y_path)\n",
    "# y_2 = y_[:,0]\n",
    "# for i in range(len(y_2)):\n",
    "#     y_2[i] = y_2[i][0:-2] \n",
    "# y_3 = list(y_2)\n",
    "\n",
    "X_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_features.pth\")\n",
    "y_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_filenames.pth\")\n",
    "cont = 0\n",
    "vgg_corrupted = []\n",
    "X_corrupted = []\n",
    "for i in range(len(X_vgg)):\n",
    "    if(X_vgg[i].shape!= torch.Size([62, 128])):\n",
    "        vgg_corrupted.append(y_vgg[i])\n",
    "        X_corrupted.append(i)\n",
    "for i in range(len(vgg_corrupted)):\n",
    "    y_vgg.remove(vgg_corrupted[i])        \n",
    "X_vgg = np.delete(X_vgg, X_corrupted)\n",
    "\n",
    "\n",
    "y_vgg2 = [i.split('/')[6:8] for i in y_vgg]\n",
    "X_vgg2 = []\n",
    "for i in range(len(y_vgg2)):\n",
    "    y_vgg2[i] = f\"{y_vgg2[i][0]}_{y_vgg2[i][1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5843740",
   "metadata": {},
   "source": [
    "### VGGish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ee69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/numpy/lib/function_base.py:5071: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = asarray(arr)\n",
      "/home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323415 323415\n",
      "Accuracy: 0.6902682229264899\n",
      "f1: 0.5483968905627624\n",
      "recall 0.5307180470440016\n"
     ]
    }
   ],
   "source": [
    "y_ = np.asarray(y_path)\n",
    "y_2 = y_[:,0]\n",
    "for i in range(len(y_2)):\n",
    "    y_2[i] = y_2[i][0:-2] \n",
    "y_3 = list(y_2)\n",
    "\n",
    "X_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_features.pth\")\n",
    "y_vgg = torch.load(f\"{root_vggish_Zamuro}/vggish_filenames.pth\")\n",
    "cont = 0\n",
    "vgg_corrupted = []\n",
    "X_corrupted = []\n",
    "for i in range(len(X_vgg)):\n",
    "    if(X_vgg[i].shape!= torch.Size([62, 128])):\n",
    "        vgg_corrupted.append(y_vgg[i])\n",
    "        X_corrupted.append(i)\n",
    "for i in range(len(vgg_corrupted)):\n",
    "    y_vgg.remove(vgg_corrupted[i])        \n",
    "X_vgg = np.delete(X_vgg, X_corrupted)\n",
    "\n",
    "\n",
    "y_vgg2 = [i.split('/')[6:8] for i in y_vgg]\n",
    "X_vgg2 = []\n",
    "for i in range(len(y_vgg2)):\n",
    "    y_vgg2[i] = f\"{y_vgg2[i][0]}_{y_vgg2[i][1]}\"\n",
    "    \n",
    "X_vgg2 = []\n",
    "remove = []\n",
    "keep_idx = []\n",
    "y_vggp = []\n",
    "\n",
    "for i in range(len(y_vgg2)):\n",
    "    #a[i] = X_vgg2[i]\n",
    "    if y_vgg2[i] not in y_3:\n",
    "        remove.append(y_vgg2[i])\n",
    "    else:\n",
    "        keep_idx.append(i)\n",
    "        \n",
    "# for i in range(len(remove)): \n",
    "#     y_vgg2.remove(remove[i])\n",
    "    \n",
    "for i in keep_idx:\n",
    "    X_vgg2.append(X_vgg[i])\n",
    "    y_vggp.append(y_vgg2[i])\n",
    "    \n",
    "\n",
    "\n",
    "y_vgg2 = y_vggp\n",
    "\n",
    "remove = []\n",
    "keep_idx = []\n",
    "X_2 = []\n",
    "y_vgg3 = []\n",
    "for i in range(len(y_3)):\n",
    "    if y_3[i] not in y_vgg2:\n",
    "        remove.append(y_vgg2[i])\n",
    "    else:\n",
    "        keep_idx.append(i)\n",
    "\n",
    "X_1 = np.reshape(X_scaled, (X_scaled.shape[0]//5, 5, X_scaled.shape[1]))\n",
    "\n",
    "# for i in range(len(remove)): \n",
    "#     y_3.remove(remove[i])\n",
    "    \n",
    "for i in keep_idx:\n",
    "    y_vgg3.append(y_3[i])\n",
    "    X_2.append(X_1[i])\n",
    "    \n",
    "X_3 = np.asarray(X_2)\n",
    "X_3 = np.reshape(X_3, (X_3.shape[0]*X_3.shape[1], X_3.shape[2]))\n",
    "y_4 = np.repeat(y_vgg3, 5)\n",
    "        \n",
    "print(len(y_4), len(X_3))\n",
    "    \n",
    "X_vgg3 = np.zeros([len(X_vgg2),62,128])\n",
    "for i in range (len(X_vgg2)):\n",
    "    X_vgg3[i] = X_vgg2[i].numpy()\n",
    "    \n",
    "X_vgg4 = X_vgg3.reshape(X_vgg3.shape[0], X_vgg3.shape[1]*X_vgg3.shape[2])\n",
    "labels_vgg = []\n",
    "for i in range(len(y_vgg2)):\n",
    "    labels_vgg.append(audios.loc[y_vgg2[i], \"cover\"])\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, labels_train, labels_test = train_test_split(X_vgg4, labels_vgg, test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, labels_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy_vggish = metrics.accuracy_score(labels_test, y_pred_rf)\n",
    "f1_score_vggish = metrics.f1_score(labels_test, y_pred_rf, average=\"macro\")\n",
    "recall_vggish = metrics.recall_score(labels_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy_vggish)\n",
    "print(\"f1:\", f1_score_vggish)\n",
    "print(\"recall\", recall_vggish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcee1a",
   "metadata": {},
   "source": [
    "### Acoustic Indices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c174fef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815322643117205\n",
      "f1: 0.7591179914466922\n",
      "recall 0.7317228748513335\n"
     ]
    }
   ],
   "source": [
    "X_ai = []\n",
    "remove = []\n",
    "s=0\n",
    "for i in range(len(y_3)):\n",
    "    \n",
    "    try:\n",
    "        X_ai.append(ai.loc[y_3[i]])\n",
    "    except:\n",
    "        s+=1\n",
    "        remove.append(y_3[i])\n",
    "for i in range(len(remove)):\n",
    "    y_3.remove(remove[i])\n",
    "X_ai = np.asarray(X_ai)\n",
    "\n",
    "labels_ai = []\n",
    "# audios.set_index(\"Filename\", inplace=True)\n",
    "for i in range(len(y_3)):\n",
    "    labels_ai.append(audios.loc[y_3[i], \"cover\"])\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ai, labels_ai, test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy_ai = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "f1_score_ai = metrics.f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "recall_ai = metrics.recall_score(y_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy_ai)\n",
    "print(\"f1:\", f1_score_ai)\n",
    "print(\"recall\", recall_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc2d1e",
   "metadata": {},
   "source": [
    "### Autoencoders Features and Labels using independent segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64533d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7849357636473262\n",
      "f1: 0.7120130488177298\n",
      "recall 0.6865794895942456\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# audios.set_index(\"Filename\", inplace=True)\n",
    "labels_ae = []\n",
    "for i in range(len(y_4)):\n",
    "    labels_ae.append(audios.loc[y_4[i], \"cover\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_3, labels_ae, test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "f1_score = metrics.f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "recall = metrics.recall_score(y_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"f1:\", f1_score)\n",
    "print(\"recall\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda32a30",
   "metadata": {},
   "source": [
    "### Autoencoders Features and Labels using 5 segments of the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e27e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = X_3\n",
    "y_c = y_4\n",
    "\n",
    "X_batch = np.reshape(X_c, (X_c.shape[0]//5,5,X_c.shape[1]))\n",
    "y_path = np.reshape(y_c, (y_c.shape[0]//5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_batch, y_path, test_size=0.2,random_state=0)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "y_train = y_train.reshape(y_train.shape[0]*y_train.shape[1])\n",
    "y_test = y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
    "\n",
    "labels_train = []\n",
    "for i in range(len(y_train)):\n",
    "    labels_train.append(audios.loc[y_train[i], \"cover\"])\n",
    "labels_test = []\n",
    "for i in range(len(y_test)):\n",
    "    labels_test.append(audios.loc[y_test[i], \"cover\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, labels_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(labels_test, y_pred_rf)\n",
    "f1_score = metrics.f1_score(labels_test, y_pred_rf, average=\"macro\")\n",
    "recall = metrics.recall_score(labels_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"f1:\", f1_score)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b62a25",
   "metadata": {},
   "source": [
    "# Autoencoders Features and Labels using voting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf2 = np.asarray(y_pred_rf)\n",
    "y_pred_rf2 = np.reshape(y_pred_rf2,(y_pred_rf2.shape[0]//5,5))\n",
    "y_test2 = np.asarray(labels_test)\n",
    "y_test2 = np.reshape(y_test2,(y_test2.shape[0]//5,5))\n",
    "\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "labels_test2 = []\n",
    "labels_pred = []\n",
    "for i in range(len(y_pred_rf2)):\n",
    "    labels_pred.append(most_frequent(list(y_pred_rf2[i])))\n",
    "    labels_test2.append(most_frequent(list(y_test2[i])))\n",
    "accuracy_ae = metrics.accuracy_score(labels_test2, labels_pred)\n",
    "f1_score_ae = metrics.f1_score(labels_test2, labels_pred, average=\"macro\")\n",
    "recall_ae = metrics.recall_score(labels_test2, labels_pred, average=\"macro\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_ae)\n",
    "print(\"f1:\", f1_score_ae)\n",
    "print(\"recall\", recall_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7020650",
   "metadata": {},
   "source": [
    "# Autoencoders Features and Labels using mean of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31817518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_New = np.load(f\"temporal_zamuro/Features/X_ae_norm_new.npy\")\n",
    "y_New = np.load(f\"temporal_zamuro/Features/y_ae_norm_new.npy\")\n",
    "labels_aenew = audios.loc[y_New]\n",
    "labels_aenew = list(labels_aenew[\"cover\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_B =np.asarray(X_2)\n",
    "X_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_New = np.mean(X_B, axis=1)\n",
    "# X_New = X_New[0:len(y_3)]\n",
    "# y_New = y_New[0:len(y_3)]\n",
    "\n",
    "X_New.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_New, labels_aenew, test_size=0.2,random_state=0)\n",
    "clf_rf = RandomForestClassifier(max_depth=32, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "f1_score = metrics.f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "recall = metrics.recall_score(y_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"f1:\", f1_score)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = [f1_score_ai, f1_score_ae, f1_score_vggish] # [ai, AE_norm, AE_PCA, AE_UMAP, VGGISH, VGGISH_PCA, VGGISH_UMAP]\n",
    "accuracy_scores = [accuracy_ai, accuracy_ae, accuracy_vggish]\n",
    "recall_scores = [recall_ai, recall_ae, recall_vggish]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34847a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for F1 scores, recall, and accuracy\n",
    "methods = [\"Acoustic\\nIndices\", \"AE Normalized\", \"vggish\"]\n",
    "\n",
    "# Create an array for the x-axis positions\n",
    "x = np.arange(len(methods))\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(11, 6))\n",
    "\n",
    "# Width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "# Define professional color palettes\n",
    "f1_color = 'steelblue'  # You can adjust the color as needed\n",
    "recall_color = 'indianred'  # You can adjust the color as needed\n",
    "accuracy_color = 'orange'  # You can adjust the color as needed\n",
    "\n",
    "# Create the bar chart for F1 scores\n",
    "bars_f1 = plt.bar(x - bar_width, f1_scores, bar_width, label='F1 Score', color=f1_color)\n",
    "\n",
    "# Create the bar chart for recall\n",
    "bars_recall = plt.bar(x, recall_scores, bar_width, label='Recall', color=recall_color)\n",
    "\n",
    "# Create the bar chart for accuracy\n",
    "bars_accuracy = plt.bar(x + bar_width, accuracy_scores, bar_width, label='Accuracy', color=accuracy_color)\n",
    "\n",
    "# Set the y-axis limits\n",
    "plt.ylim(0.5, 1)\n",
    "\n",
    "# Set the x-axis labels and their positions\n",
    "plt.xticks(x, methods, fontsize=14)  # Increase label size\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Set the title\n",
    "# plt.title('F1 Score, Recall, and Accuracy Comparison')\n",
    "\n",
    "# Set the legend with increased size\n",
    "plt.legend(fontsize=14)  # Increase legend size\n",
    "\n",
    "# Add values on top of the bars\n",
    "for bar, value in zip(bars_f1, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height()+0.01, f'{value:.3f}', ha='center', va='bottom', fontsize=11, rotation=90)\n",
    "\n",
    "for bar, value in zip(bars_recall, recall_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height()+0.01, f'{value:.3f}', ha='center', va='bottom', fontsize=11, rotation=90)\n",
    "\n",
    "for bar, value in zip(bars_accuracy, accuracy_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height()+0.01, f'{value:.3f}', ha='center', va='bottom', fontsize=11, rotation=90)\n",
    "\n",
    "# Add a legend\n",
    "# plt.legend()\n",
    "\n",
    "# Display the figure\n",
    "\n",
    "# plt.savefig(f\"{root}/Zamuro/temporal_zamuro/zamuro_classification_results/zamuro_classification.pdf\", format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
