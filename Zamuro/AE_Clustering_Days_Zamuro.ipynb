{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902a9c0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "\n",
    "    !pip install joypy\n",
    "\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics\"\n",
    "    root_path = \"/media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido\"\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b64ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Normalizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from Zamuro_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import pacmap\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d20e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpvc(X, labels):\n",
    "    unique_clusters = set(labels) - {-1}  # Excluye ruido (-1)\n",
    "    density_peaks = {}\n",
    "\n",
    "    # ðŸ”¹ Paso 1: Calcular densidad local (usando k-vecinos mÃ¡s cercanos)\n",
    "    k = 5  # Vecinos para densidad local\n",
    "    dist_matrix = pairwise_distances(X)\n",
    "    local_density = np.mean(np.sort(dist_matrix, axis=1)[:, 1:k+1], axis=1)\n",
    "\n",
    "    # ðŸ”¹ Paso 2: Identificar picos de densidad por cluster\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_points = X[labels == cluster]\n",
    "        cluster_density = local_density[labels == cluster]\n",
    "        density_peaks[cluster] = cluster_points[np.argmin(cluster_density)]\n",
    "\n",
    "    # ðŸ”¹ Paso 3: Calcular DPVC para cada cluster\n",
    "    dpvc_scores = []\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_points = X[labels == cluster]\n",
    "        peak = density_peaks[cluster]\n",
    "        distances_to_peak = np.linalg.norm(cluster_points - peak, axis=1)\n",
    "        score = np.mean(distances_to_peak)\n",
    "        dpvc_scores.append(score)\n",
    "\n",
    "    return np.mean(dpvc_scores)  # Promedio de los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"AE\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 4\n",
    "hour = 9\n",
    "date_format = f\"day_{day}_hour_{hour}\"\n",
    "\n",
    "model_name = f\"{root}/Zamuro/temporal_zamuro/models/log_standarization_model_epochs_10/model_{model_type}_{identifier}_{date_format}_final.pth\"\n",
    "model = AE(num_hiddens=64).to(device)\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "audios = pd.read_csv(f\"Complementary_Files/zamuro_audios_complete.csv\", index_col=0)\n",
    "recorders = pd.read_csv(f\"Complementary_Files/zamuro_recorders_satelites.csv\", index_col=0)\n",
    "df_ae = pd.read_csv(f\"temporal_zamuro/Features/New_df_ae_unflat.csv\")\n",
    "X = np.asarray(df_ae.loc[:,\"0\":\"25919\"])\n",
    "X = np.reshape(X, [X.shape[0], 5, X.shape[1]//5])\n",
    "X = np.mean(X, axis=1)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalizer_ = Normalizer().fit(X)\n",
    "X_norm = Normalizer_.transform(X)\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X)\n",
    "X_TSNE_norm = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_norm)\n",
    "X_TSNE_scaled = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_scaled)\n",
    "\n",
    "X_TSNE_norm_norm = Normalizer().fit_transform(X_TSNE_norm_norm)\n",
    "X_TSNE_norm_scaled = StandardScaler().fit_transform(X_TSNE_norm_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X)\n",
    "X_UMAP = mapper.transform(X)\n",
    "\n",
    "mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_norm)\n",
    "X_UMAP_norm = mapper.transform(X_norm)\n",
    "\n",
    "mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_scaled)\n",
    "X_UMAP_scaled = mapper.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y_hour = df_ae[\"hour\"]\n",
    "# y_hour = list(np.repeat(y_hour, 5))\n",
    "y_cover = df_ae[\"cover\"]\n",
    "# y_cover = list(np.repeat(y_cover, 5))\n",
    "y_site = df_ae[\"location\"]\n",
    "# y_site = list(np.repeat(y_site, 5))\n",
    "y_stage = df_ae[\"hour_stage\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_site = encoder.fit_transform(y_site)\n",
    "y_cover = encoder.fit_transform(y_cover)\n",
    "y_stage = encoder.fit_transform(y_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c14324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "y_n = y_stage\n",
    "labels = np.unique(y)\n",
    "cmap = plt.get_cmap(\"inferno\", len(labels))  # Cambia \"viridis\" por cualquier colormap que prefieras\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(labels) + 1) - 0.5, len(labels))\n",
    "# Primer scatter plot\n",
    "\n",
    "axes[0].scatter(X_TSNE[:, 0], X_TSNE[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[0].set_title(\"Original TSNE\")\n",
    "\n",
    "# Segundo scatter plot\n",
    "axes[1].scatter(X_TSNE_norm[:, 0], X_TSNE_norm[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[1].set_title(\"Normalized TSNE\")\n",
    "\n",
    "# Tercer scatter plot\n",
    "axes[2].scatter(X_TSNE_scaled[:, 0], X_TSNE_scaled[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[2].set_title(\"Scaled TSNE\")\n",
    "\n",
    "# Ajustar diseÃ±o\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "y_n = y_stage\n",
    "labels = np.unique(y_n)\n",
    "cmap = plt.get_cmap(\"inferno\", len(labels))  # Cambia \"viridis\" por cualquier colormap que prefieras\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(labels) + 1) - 0.5, len(labels))\n",
    "# Primer scatter plot\n",
    "\n",
    "axes[0].scatter(X_UMAP[:, 0], X_UMAP[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[0].set_title(\"Original UMAP\")\n",
    "\n",
    "# Segundo scatter plot\n",
    "axes[1].scatter(X_UMAP_norm[:, 0], X_UMAP_norm[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[1].set_title(\"Normalized UMAP\")\n",
    "\n",
    "# Tercer scatter plot\n",
    "axes[2].scatter(X_UMAP_scaled[:, 0], X_UMAP_scaled[:, 1], s=0.1, alpha=0.1, c=y_n, cmap=cmap, norm=norm)\n",
    "axes[2].set_title(\"Scaled UMAP\")\n",
    "\n",
    "# Ajustar diseÃ±o\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = pacmap.PaCMAP(n_components=2, n_neighbors=75, MN_ratio=1, FP_ratio=20) \n",
    "X_pacmap = embedded.fit_transform(X, init=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95308cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"rain_FI\": \"NO\"}\n",
    "dataset = SoundscapeData('media/mirp_ai/Seagate Desktop Drive/Datos Rey Zamuro/Ultrasonido/',\n",
    "                         dataframe_path=\"Complementary_Files/zamuro_audios.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721eff7",
   "metadata": {},
   "source": [
    "## Traditional Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3322cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import pickle as pkl\n",
    "# Single methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from Modules.Clustering_Utils_Zamuro import plot_silhouette\n",
    "from Modules.Clustering_Utils_Zamuro import plot_centroids\n",
    "from Modules.Clustering_Utils_Zamuro import ClusteringResults\n",
    "from sklearn.metrics import davies_bouldin_score as DB\n",
    "from sklearn.metrics import calinski_harabasz_score as CH\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f855c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "Kmeans_Results = ClusteringResults(Kmeans, df_ae, y_label=\"hour\")\n",
    "Kmeans_Results.histograms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f88bae",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc530fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [3, 5, 7, 9, 15, 21, 33]\n",
    "days = list(set(df_ae[\"day\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766b0f2",
   "metadata": {},
   "source": [
    "## Kmeans Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed663b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for day in days[0:1]:\n",
    "    \n",
    "    silhouette_score_Kmeans = []\n",
    "    CH_score_Kmeans = []\n",
    "    DB_score_Kmeans = []\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_day)\n",
    "#     X_UMAP = mapper.transform(X_day)\n",
    "#     X_n = X_norm\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    for id_, n_cluster in enumerate(clusters):\n",
    "        Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_n)\n",
    "        silhouette_score_Kmeans.append(metrics.silhouette_score(X_n, Kmeans.labels_))\n",
    "        DB_score_Kmeans.append(DB(X_n, Kmeans.labels_))\n",
    "        CH_score_Kmeans.append(CH(X_n, Kmeans.labels_))\n",
    "        print(f\"Silhouette: {silhouette_score_Kmeans}, DB: {DB_score_Kmeans}, CH: {CH_score_Kmeans}\")\n",
    "        plot_silhouette(X_n, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id_],\n",
    "                        method=f\"Kmeans\", save=True, root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/\")\n",
    "        cluster_centers = Kmeans.cluster_centers_ \n",
    "        cluster_centers_inverse = Normalizer_.inverse_transform(cluster_centers)\n",
    "#         cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "        plot_centroids(cluster_centers_inverse, testing, save=True, #cluster_centers_inverse for UMAP \n",
    "                       root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/\")\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=Kmeans.labels_, cmap='inferno', alpha=1, s=1)\n",
    "        centroids_pca = pca.transform(cluster_centers)\n",
    "#         plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/Scatter_Normal_{n_cluster}\")\n",
    "        Kmeans_Results = ClusteringResults(Kmeans, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "        Kmeans_Results.histograms(save=True,\n",
    "                                  root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/\")#root=\"temporal/clustering_results/Kmeans/\")\n",
    "    #     Kmeans_Results.joyplot()\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/DB_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_Normal/{str(day)}/CH_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_Kmeans, file)\n",
    "        \n",
    "    clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a27e6",
   "metadata": {},
   "source": [
    "## Kmeans UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c13b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for day in days:\n",
    "    \n",
    "    silhouette_score_Kmeans = []\n",
    "    CH_score_Kmeans = []\n",
    "    DB_score_Kmeans = []\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_n) \n",
    "    X_UMAP = mapper.transform(X_n)\n",
    "    X_n = X_UMAP\n",
    "    \n",
    "    \n",
    "#     Normalizer_ = Normalizer().fit(X_day)\n",
    "#     X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    for id_, n_cluster in enumerate(clusters):\n",
    "        Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_n)\n",
    "        silhouette_score_Kmeans.append(metrics.silhouette_score(X_n, Kmeans.labels_))\n",
    "        DB_score_Kmeans.append(DB(X_n, Kmeans.labels_))\n",
    "        CH_score_Kmeans.append(CH(X_n, Kmeans.labels_))\n",
    "        print(f\"Silhouette: {silhouette_score_Kmeans}, DB: {DB_score_Kmeans}, CH: {CH_score_Kmeans}\")\n",
    "        plot_silhouette(X_n, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id_],\n",
    "                        method=f\"Kmeans\", save=True, root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/\")\n",
    "        cluster_centers = Kmeans.cluster_centers_ \n",
    "        cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "        cluster_centers_inverse = Normalizer_.inverse_transform(cluster_centers_inverse)\n",
    "        plot_centroids(cluster_centers_inverse, testing, save=True, #cluster_centers_inverse for UMAP \n",
    "                       root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/\")\n",
    "        pca = PCA(n_components=2)\n",
    "#         X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.scatter(X_n[:, 0], X_n[:, 1], c=Kmeans.labels_, cmap='inferno', alpha=1, s=1)\n",
    "#         centroids_pca = pca.transform(cluster_centers)\n",
    "#         plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/Scatter_Normal_{n_cluster}\")\n",
    "        Kmeans_Results = ClusteringResults(Kmeans, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "        Kmeans_Results.histograms(save=True,\n",
    "                                  root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/\")#root=\"temporal/clustering_results/Kmeans/\")\n",
    "    #     Kmeans_Results.joyplot()\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/DB_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_UMAP/{str(day)}/CH_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_Kmeans, file)\n",
    "        \n",
    "    clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457b21f",
   "metadata": {},
   "source": [
    "# Kmeans PacMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "for day in days:\n",
    "    \n",
    "    silhouette_score_Kmeans = []\n",
    "    CH_score_Kmeans = []\n",
    "    DB_score_Kmeans = []\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    pacmap_reducer = pacmap.PaCMAP(n_components=2, n_neighbors=75, MN_ratio=1, FP_ratio=20)\n",
    "    X_PacMAP = pacmap_reducer.fit_transform(X_n)\n",
    "    \n",
    "    X_n = X_PacMAP\n",
    "    \n",
    "    for id_, n_cluster in enumerate(clusters):\n",
    "        Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_n)\n",
    "        silhouette_score_Kmeans.append(metrics.silhouette_score(X_n, Kmeans.labels_))\n",
    "        DB_score_Kmeans.append(DB(X_n, Kmeans.labels_))\n",
    "        CH_score_Kmeans.append(CH(X_n, Kmeans.labels_))\n",
    "        print(f\"Silhouette: {silhouette_score_Kmeans}, DB: {DB_score_Kmeans}, CH: {CH_score_Kmeans}\")\n",
    "        plot_silhouette(X_n, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id_],\n",
    "                        method=f\"Kmeans\", save=True, root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/\")\n",
    "        cluster_centers = Kmeans.cluster_centers_ \n",
    "#         cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "#         cluster_centers_inverse = Normalizer_.inverse_transform(cluster_centers_inverse)\n",
    "#         plot_centroids(cluster_centers_inverse, testing, save=True, #cluster_centers_inverse for PacMAP \n",
    "#                        root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/\")\n",
    "#         pca = PCA(n_components=2)\n",
    "#         X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.scatter(X_n[:, 0], X_n[:, 1], c=Kmeans.labels_, cmap='inferno', alpha=1, s=1)\n",
    "#         centroids_pca = pca.transform(cluster_centers)\n",
    "#         plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "        plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/Scatter_Normal_{n_cluster}\")\n",
    "        Kmeans_Results = ClusteringResults(Kmeans, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "        Kmeans_Results.histograms(save=True,\n",
    "                                  root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/\")#root=\"temporal/clustering_results/Kmeans/\")\n",
    "    #     Kmeans_Results.joyplot()\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/DB_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_Kmeans, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/Kmeans_PacMAP/{str(day)}/CH_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_Kmeans, file)\n",
    "        \n",
    "    clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d373e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "for day in days:  \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:, \"0\":\"5183\"])\n",
    "    \n",
    "    # Normalization\n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_n) \n",
    "    X_UMAP = mapper.transform(X_n)\n",
    "    X_n = X_UMAP\n",
    "\n",
    "    print(f\"---------- Running for day: {day}\")\n",
    "\n",
    "    # OPTICS Clustering\n",
    "    optics = OPTICS(min_samples=75).fit(X_n)\n",
    "    reachability = optics.reachability_[optics.ordering_]\n",
    "\n",
    "    # Plot improvement\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Color gradient for points\n",
    "    plt.scatter(range(len(reachability)), reachability, c=reachability, cmap=\"viridis\", edgecolor=\"k\", s=5, alpha=0.5)\n",
    "\n",
    "    # Smoothed line\n",
    "    plt.plot(reachability, color=\"darkblue\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    # Highlighting key points\n",
    "    min_reach = np.min(reachability[np.isfinite(reachability)])\n",
    "    max_reach = np.max(reachability[np.isfinite(reachability)])\n",
    "    plt.axhline(y=min_reach, color=\"green\", linestyle=\"--\", alpha=0.8, label=f\"Min: {min_reach:.3f}\")\n",
    "    plt.axhline(y=max_reach, color=\"red\", linestyle=\"--\", alpha=0.8, label=f\"Max: {max_reach:.3f}\")\n",
    "\n",
    "    # Labels & Title\n",
    "    plt.title(f\"Reachability Plot (OPTICS) - Day {day}\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Points\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Reachability Distance\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.colorbar(label=\"Reachability Distance\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/OPTICS/reachability_{day}\")  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482132aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pacmap\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))  # TamaÃ±o del grÃ¡fico\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(days)))  # Generar colores distintos\n",
    "\n",
    "for day, color in zip(days, colors):  \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:, \"0\":\"5183\"])\n",
    "    \n",
    "    # NormalizaciÃ³n\n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    # Reducir dimensiÃ³n con PaCMAP\n",
    "    pacmap_reducer = pacmap.PaCMAP(n_components=2, n_neighbors=75, MN_ratio=1, FP_ratio=20)\n",
    "    X_PacMAP = pacmap_reducer.fit_transform(X_n)\n",
    "    X_n = X_PacMAP\n",
    "\n",
    "    print(f\"---------- Running for day: {day}\")\n",
    "\n",
    "    # OPTICS Clustering\n",
    "    optics = OPTICS(min_samples=75).fit(X_n)\n",
    "    reachability = optics.reachability_[optics.ordering_]\n",
    "    \n",
    "    # Graficar reachability\n",
    "    plt.plot(reachability, color=color, alpha=0.7, linewidth=1.5, label=f\"Day {day}\")\n",
    "\n",
    "# Etiquetas y formato\n",
    "plt.title(\"Reachability Plot (OPTICS) for All Days\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Points\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"Reachability Distance\", fontsize=12, fontweight=\"bold\")\n",
    "plt.legend(title=\"Days\", fontsize=10)\n",
    "plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/OPTICS/reachability_all_days\")  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66204723",
   "metadata": {},
   "source": [
    "# DBSCAN Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Definir parÃ¡metros para DBSCAN (eps y min_samples)\n",
    "eps = [0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5]  # Distancia mÃ¡xima para que dos puntos sean considerados vecinos\n",
    "min_samples = [5, 25, 150, 300, 500, 1000]  # NÃºmero mÃ­nimo de puntos para formar un cluster\n",
    "\n",
    "for day in days[0:2]:\n",
    "    \n",
    "    silhouette_score_DBSCAN = {}\n",
    "    CH_score_DBSCAN = {}\n",
    "    DB_score_DBSCAN = {}\n",
    "    DPVC_score_DBSCAN = {}\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "\n",
    "    # Iterar a travÃ©s de los valores de parÃ¡metros para DBSCAN (si quieres hacer varias pruebas con distintos eps)\n",
    "    for id_, min_sample in enumerate(min_samples):  # clusters ahora contiene posibles combinaciones de eps y min_samples\n",
    "        for ep in eps:\n",
    "\n",
    "            print(f\"----------Running for day: {day} min_samples: {min_sample}, eps: {ep}-------------\")\n",
    "                       \n",
    "\n",
    "            DBSCAN_model = DBSCAN(eps=ep, min_samples=min_sample).fit(X_n)\n",
    "\n",
    "            # Obtener las etiquetas de los clusters\n",
    "            labels = DBSCAN_model.labels_\n",
    "\n",
    "            # Filtrar etiquetas vÃ¡lidas (excluyendo ruido etiquetado como -1)\n",
    "            unique_labels = set(labels)\n",
    "            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Excluir el ruido\n",
    "\n",
    "            # Verificar si el algoritmo ha encontrado mÃ¡s de un cluster vÃ¡lido (excluyendo el ruido)\n",
    "            if n_clusters > 1 and n_clusters<30:\n",
    "                # Calcular las mÃ©tricas solo si se ha formado mÃ¡s de un cluster\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "\n",
    "                print(f\"Silhouette: {silhouette_score_DBSCAN}, DB: {DB_score_DBSCAN}, CH: {CH_score_DBSCAN}, DPVC: {DPVC_score_DBSCAN}\")\n",
    "\n",
    "                # Graficar el silhouette plot\n",
    "                plot_silhouette(X_n, labels, n_clusters, silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"],\n",
    "                                method=f\"DBSCAN\", \n",
    "                                extra= f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                save=True,\n",
    "                               root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}\")\n",
    "\n",
    "                # Calcular los centroides de cada cluster (excluyendo los puntos de ruido etiquetados como -1)\n",
    "                cluster_centers = np.array([X_n[labels == label].mean(axis=0) for label in unique_labels if label != -1])\n",
    "\n",
    "                # Asignar los centroides como atributo del modelo DBSCAN\n",
    "                DBSCAN_model.centroids = cluster_centers  # AquÃ­ agregamos el nuevo atributo\n",
    "\n",
    "                # Graficar los centroides\n",
    "    #             cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "                plot_centroids(cluster_centers, testing, f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                save=True, \n",
    "                               root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/\")\n",
    "\n",
    "                pca = PCA(n_components=2)\n",
    "                X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "                plt.figure(figsize=(10, 7))\n",
    "                plt.scatter(X_pca[:, 0], X_pca[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=1, s=1)\n",
    "    #             plt.scatter(X_n[:, 0], X_n[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=0.2, s=1)\n",
    "                centroids_pca = pca.transform(cluster_centers)\n",
    "                plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "    #             plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "                plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/Scatter_DBSCAN_eps_{ep}_min_samples_{min_sample}.png\")\n",
    "\n",
    "                # Generar resultados del clustering usando la clase ClusteringResults\n",
    "                DBSCAN_Results = ClusteringResults(DBSCAN_model, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "                DBSCAN_Results.histograms(extra=f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                          save=True, \n",
    "                                          root =f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/\")\n",
    "    #     #         DBSCAN_Results.joyplot()\n",
    "            elif n_clusters<=1:\n",
    "                print(f\"Not working with these eps and min_samples values - Only one cluster\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                print(f\"Not working with these eps and min_samples values - Many clusters\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "                continue\n",
    "        #         print(f\"DBSCAN no encontrÃ³ suficientes clusters para eps={eps_value} y min_samples={min_samples_value}\")\n",
    "\n",
    "        # Guardar los resultados en archivos pickle\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/silhouette_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/DB_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/CH_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_Normal/{day}/DPVC_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DPVC_score_DBSCAN, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7178af",
   "metadata": {},
   "source": [
    "# DBSCAN UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Definir parÃ¡metros para DBSCAN (eps y min_samples)\n",
    "eps = [0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 5, 10, 20, 50]  # Distancia mÃ¡xima para que dos puntos sean considerados vecinos\n",
    "min_samples = [5, 25, 50, 150, 300, 500, 1000]  # NÃºmero mÃ­nimo de puntos para formar un cluster\n",
    "\n",
    "for day in days:\n",
    "    \n",
    "    silhouette_score_DBSCAN = {}\n",
    "    CH_score_DBSCAN = {}\n",
    "    DB_score_DBSCAN = {}\n",
    "    DPVC_score_DBSCAN = {}\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    mapper = umap.UMAP(n_components=2, min_dist=0.01,\n",
    "                            metric=\"euclidean\", n_neighbors=75,\n",
    "                            random_state=0, n_jobs=-1).fit(X_n) \n",
    "    X_UMAP = mapper.transform(X_n)\n",
    "    X_n = X_UMAP\n",
    "\n",
    "    # Iterar a travÃ©s de los valores de parÃ¡metros para DBSCAN (si quieres hacer varias pruebas con distintos eps)\n",
    "    for id_, min_sample in enumerate(min_samples):  # clusters ahora contiene posibles combinaciones de eps y min_samples\n",
    "        for ep in eps:\n",
    "\n",
    "            print(f\"----------Running for day: {day} min_samples: {min_sample}, eps: {ep}-------------\")\n",
    "\n",
    "\n",
    "            DBSCAN_model = DBSCAN(eps=ep, min_samples=min_sample).fit(X_n)\n",
    "\n",
    "            # Obtener las etiquetas de los clusters\n",
    "            labels = DBSCAN_model.labels_\n",
    "\n",
    "            # Filtrar etiquetas vÃ¡lidas (excluyendo ruido etiquetado como -1)\n",
    "            unique_labels = set(labels)\n",
    "            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Excluir el ruido\n",
    "\n",
    "            # Verificar si el algoritmo ha encontrado mÃ¡s de un cluster vÃ¡lido (excluyendo el ruido)\n",
    "            if n_clusters > 1 and n_clusters<30:\n",
    "                # Calcular las mÃ©tricas solo si se ha formado mÃ¡s de un cluster\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "\n",
    "                print(f\"Silhouette: {silhouette_score_DBSCAN}, DB: {DB_score_DBSCAN}, CH: {CH_score_DBSCAN}\")\n",
    "\n",
    "                # Graficar el silhouette plot\n",
    "                plot_silhouette(X_n, labels, n_clusters, silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"],\n",
    "                                method=f\"DBSCAN\", \n",
    "                                extra= f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                save=True,\n",
    "                               root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}\")\n",
    "\n",
    "                # Calcular los centroides de cada cluster (excluyendo los puntos de ruido etiquetados como -1)\n",
    "                cluster_centers = np.array([X_n[labels == label].mean(axis=0) for label in unique_labels if label != -1])\n",
    "\n",
    "                # Asignar los centroides como atributo del modelo DBSCAN\n",
    "                DBSCAN_model.centroids = cluster_centers  # AquÃ­ agregamos el nuevo atributo\n",
    "\n",
    "                # Graficar los centroides\n",
    "                cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "                plot_centroids(cluster_centers_inverse, testing, f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                save=True, \n",
    "                               root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/\")\n",
    "\n",
    "#                 pca = PCA(n_components=2)\n",
    "#                 X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "#                 plt.figure(figsize=(10, 7))\n",
    "#                 plt.scatter(X_pca[:, 0], X_pca[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=1, s=3)\n",
    "                plt.scatter(X_n[:, 0], X_n[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=1, s=1)\n",
    "#                 centroids_pca = pca.transform(cluster_centers)\n",
    "#                 plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "                plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "                plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/Scatter_DBSCAN_eps_{ep}_min_samples_{min_sample}.png\")\n",
    "\n",
    "                # Generar resultados del clustering usando la clase ClusteringResults\n",
    "                DBSCAN_Results = ClusteringResults(DBSCAN_model, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "                DBSCAN_Results.histograms(extra=f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                          save=True, \n",
    "                                          root =f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/\")\n",
    "    #     #         DBSCAN_Results.joyplot()\n",
    "            elif n_clusters<=1:\n",
    "                print(f\"Not working with these eps and min_samples values - Only one cluster\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                print(f\"Not working with these eps and min_samples values - Many clusters\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "                continue\n",
    "\n",
    "        # Guardar los resultados en archivos pickle\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/silhouette_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/DB_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/CH_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_UMAP/{day}/DPVC_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DPVC_score_DBSCAN, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b82fa",
   "metadata": {},
   "source": [
    "# DBSCAN PacMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5f7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Definir parÃ¡metros para DBSCAN (eps y min_samples)\n",
    "eps = [0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 5, 10, 20, 50]  # Distancia mÃ¡xima para que dos puntos sean considerados vecinos\n",
    "min_samples = [5, 25, 150, 300, 500, 1000]  # NÃºmero mÃ­nimo de puntos para formar un cluster\n",
    "\n",
    "for day in days:\n",
    "    \n",
    "    silhouette_score_DBSCAN = {}\n",
    "    CH_score_DBSCAN = {}\n",
    "    DB_score_DBSCAN = {}\n",
    "    DPVC_score_DBSCAN = {}\n",
    "    \n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:,\"0\":\"5183\"])\n",
    "    \n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    pacmap_reducer = pacmap.PaCMAP(n_components=2, n_neighbors=75, MN_ratio=1, FP_ratio=20)\n",
    "    X_PacMAP = pacmap_reducer.fit_transform(X_n)\n",
    "    \n",
    "    X_n = X_PacMAP\n",
    "    \n",
    "\n",
    "    # Iterar a travÃ©s de los valores de parÃ¡metros para DBSCAN (si quieres hacer varias pruebas con distintos eps)\n",
    "    for id_, min_sample in enumerate(min_samples):  # clusters ahora contiene posibles combinaciones de eps y min_samples\n",
    "        for ep in eps:\n",
    "\n",
    "            print(f\"----------Running for day: {day} min_samples: {min_sample}, eps: {ep}-------------\")\n",
    "\n",
    "\n",
    "            DBSCAN_model = DBSCAN(eps=ep, min_samples=min_sample).fit(X_n)\n",
    "\n",
    "            # Obtener las etiquetas de los clusters\n",
    "            labels = DBSCAN_model.labels_\n",
    "\n",
    "            # Filtrar etiquetas vÃ¡lidas (excluyendo ruido etiquetado como -1)\n",
    "            unique_labels = set(labels)\n",
    "            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Excluir el ruido\n",
    "\n",
    "            # Verificar si el algoritmo ha encontrado mÃ¡s de un cluster vÃ¡lido (excluyendo el ruido)\n",
    "            if n_clusters > 1 and n_clusters<30:\n",
    "                # Calcular las mÃ©tricas solo si se ha formado mÃ¡s de un cluster\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "\n",
    "                print(f\"Silhouette: {silhouette_score_DBSCAN}, DB: {DB_score_DBSCAN}, CH: {CH_score_DBSCAN}\")\n",
    "\n",
    "                # Graficar el silhouette plot\n",
    "                plot_silhouette(X_n, labels, n_clusters, silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"],\n",
    "                                method=f\"DBSCAN\", \n",
    "                                extra= f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                save=True,\n",
    "                               root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}\")\n",
    "\n",
    "                # Calcular los centroides de cada cluster (excluyendo los puntos de ruido etiquetados como -1)\n",
    "                cluster_centers = np.array([X_n[labels == label].mean(axis=0) for label in unique_labels if label != -1])\n",
    "\n",
    "                # Asignar los centroides como atributo del modelo DBSCAN\n",
    "                DBSCAN_model.centroids = cluster_centers  # AquÃ­ agregamos el nuevo atributo\n",
    "\n",
    "                # Graficar los centroides\n",
    "#                 cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "#                 plot_centroids(cluster_centers_inverse, testing, f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "#                                 save=True, \n",
    "#                                root=f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/\")\n",
    "\n",
    "#                 pca = PCA(n_components=2)\n",
    "#                 X_pca = pca.fit_transform(X_n)  # Reducir las dimensiones a 2\n",
    "#                 plt.figure(figsize=(10, 7))\n",
    "#                 plt.scatter(X_pca[:, 0], X_pca[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=1, s=3)\n",
    "                plt.scatter(X_n[:, 0], X_n[:, 1], c=DBSCAN_model.labels_, cmap='inferno', alpha=1, s=1)\n",
    "#                 centroids_pca = pca.transform(cluster_centers)\n",
    "#                 plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "                plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "                plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/Scatter_DBSCAN_eps_{ep}_min_samples_{min_sample}.png\")\n",
    "\n",
    "                # Generar resultados del clustering usando la clase ClusteringResults\n",
    "                DBSCAN_Results = ClusteringResults(DBSCAN_model, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "                DBSCAN_Results.histograms(extra=f\"eps_{DBSCAN_model.eps}_min_samples_{DBSCAN_model.min_samples}\",\n",
    "                                          save=True, \n",
    "                                          root =f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/\")\n",
    "    #     #         DBSCAN_Results.joyplot()\n",
    "            elif n_clusters<=1:\n",
    "                print(f\"Not working with these eps and min_samples values - Only one cluster\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = None\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                print(f\"Not working with these eps and min_samples values - Many clusters\")\n",
    "                silhouette_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_n, labels)\n",
    "                DB_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = DB(X_n, labels)\n",
    "                CH_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = CH(X_n, labels)\n",
    "                DPVC_score_DBSCAN[f\"eps_{ep}_min_samples_{min_sample}\"] = dpvc(X_n, labels)\n",
    "                continue\n",
    "\n",
    "        # Guardar los resultados en archivos pickle\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/silhouette_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/DB_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DB_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/CH_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(CH_score_DBSCAN, file)\n",
    "    with open(f\"temporal_zamuro/zamuro_clustering_results/Days_Results/AE/DBSCAN_PacMAP/{day}/DPVC_eps_{eps}_min_samples_{min_samples}\", \"wb\") as file:\n",
    "        pkl.dump(DPVC_score_DBSCAN, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a16456",
   "metadata": {},
   "source": [
    "# OPTICS PacMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986888e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for day, color in zip(days, colors): \n",
    "    \n",
    "    print(f\"--------Running Day {day}-----------\")\n",
    "    df_day = df_ae[df_ae['day'].isin([day])]\n",
    "    X_day = np.asarray(df_day.loc[:, \"0\":\"5183\"])\n",
    "    \n",
    "    # NormalizaciÃ³n\n",
    "    Normalizer_ = Normalizer().fit(X_day)\n",
    "    X_n = Normalizer_.transform(X_day)\n",
    "    \n",
    "    print(f\"Computing PacMAP\")\n",
    "    pacmap_reducer = pacmap.PaCMAP(n_components=2, n_neighbors=75, MN_ratio=1, FP_ratio=20)\n",
    "    X_PacMAP = pacmap_reducer.fit_transform(X_n)\n",
    "    X_n = X_PacMAP\n",
    "    \n",
    "    print(f\"Computing OPTICS\")\n",
    "    \n",
    "    optics = OPTICS(min_samples=75, xi=0.05, min_cluster_size=25)\n",
    "    optics.fit(X_n)\n",
    "\n",
    "    # ðŸ”¹ Extraer etiquetas de los clusters detectados automÃ¡ticamente\n",
    "    labels = optics.labels_  # -1 indica ruido\n",
    "\n",
    "    # ðŸ”¹ Visualizar resultados\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_n[:, 0], X_n[:, 1], c=labels, cmap=\"plasma\", s=10)\n",
    "    plt.title(\"Clustering con OPTICS (sin eps, usando xi)\")\n",
    "    plt.colorbar(label=\"Cluster ID\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d254ad4",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e184bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "# Inicializamos las listas de scores\n",
    "silhouette_score_HDBSCAN = {}\n",
    "DB_score_HDBSCAN = {}\n",
    "CH_score_HDBSCAN = {}\n",
    "\n",
    "X_norm = X\n",
    "\n",
    "# ParÃ¡metros para HDBSCAN\n",
    "min_samples = [5, 25, 150]  # NÃºmero mÃ­nimo de puntos en cada cluster\n",
    "min_cluster_sizes = [2, 5, 10, 25, 50]  # TamaÃ±o mÃ­nimo de los clusters\n",
    "\n",
    "# Iterar a travÃ©s de los valores de parÃ¡metros para HDBSCAN\n",
    "for id_, min_sample in enumerate(min_samples):\n",
    "    for min_cluster_size in min_cluster_sizes:\n",
    "        \n",
    "        print(f\"Running for min_samples: {min_sample}, min_cluster_size: {min_cluster_size}\")\n",
    "    \n",
    "        # Ajustar el modelo HDBSCAN\n",
    "        HDBSCAN_model = hdbscan.HDBSCAN(min_samples=min_sample, min_cluster_size=min_cluster_size).fit(X_norm)\n",
    "\n",
    "        # Obtener las etiquetas de los clusters\n",
    "        labels = HDBSCAN_model.labels_\n",
    "\n",
    "        # Filtrar etiquetas vÃ¡lidas (excluyendo ruido etiquetado como -1)\n",
    "        unique_labels = set(labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Excluir el ruido\n",
    "\n",
    "        # Verificar si el algoritmo ha encontrado mÃ¡s de un cluster vÃ¡lido (excluyendo el ruido)\n",
    "        if n_clusters > 1 and n_clusters < 31:\n",
    "            # Calcular las mÃ©tricas solo si se ha formado mÃ¡s de un cluster\n",
    "            silhouette_score_HDBSCAN[f\"min_cluster_size_{min_cluster_size}_min_samples_{min_sample}\"] = metrics.silhouette_score(X_norm, labels)\n",
    "            DB_score_HDBSCAN[f\"min_cluster_size_{min_cluster_size}_min_samples_{min_sample}\"] = DB(X_norm, labels)\n",
    "            CH_score_HDBSCAN[f\"min_cluster_size_{min_cluster_size}_min_samples_{min_sample}\"] = CH(X_norm, labels)\n",
    "\n",
    "            print(f\"Silhouette: {silhouette_score_HDBSCAN}, DB: {DB_score_HDBSCAN}, CH: {CH_score_HDBSCAN}\")\n",
    "\n",
    "            # Graficar el silhouette plot\n",
    "            plot_silhouette(X_norm, labels, n_clusters, silhouette_score_HDBSCAN[f\"min_cluster_size_{min_cluster_size}_min_samples_{min_sample}\"],\n",
    "                            save=True,\n",
    "                           root=f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/\")\n",
    "\n",
    "            # Calcular los \"centroides\" aproximados de cada cluster (excluyendo ruido)\n",
    "            cluster_centers = np.array([X_norm[labels == label].mean(axis=0) for label in unique_labels if label != -1])\n",
    "\n",
    "            # Asignar los centroides como atributo del modelo HDBSCAN\n",
    "#             cluster_centers_inverse = mapper.inverse_transform(cluster_centers)\n",
    "            HDBSCAN_model.centroids = cluster_centers  # AquÃ­ agregamos el nuevo atributo\n",
    "\n",
    "#             Graficar los centroides\n",
    "            plot_centroids(cluster_centers, testing, f\"min_cluster_size_{HDBSCAN_model.min_cluster_size}_min_samples_{HDBSCAN_model.min_samples}\",\n",
    "                            save=True,\n",
    "                           root=f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/\")\n",
    "            \n",
    "            # ProyecciÃ³n PCA para visualizaciÃ³n\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X_norm)  # Reducir las dimensiones a 2\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=HDBSCAN_model.labels_, cmap='inferno', alpha=0.2, s=1)\n",
    "#             plt.scatter(X_norm[:, 0], X_norm[:, 1], c=HDBSCAN_model.labels_, cmap='inferno', alpha=0.2, s=1)\n",
    "            plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "            plt.savefig(f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/Scatter_HDBSCAN_min_cluster_size_{min_cluster_size}_min_samples_{min_sample}\")\n",
    "            \n",
    "            # Transformar los centroides a la proyecciÃ³n PCA\n",
    "            centroids_pca = pca.transform(cluster_centers)\n",
    "            plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', s=50, alpha=0.75, marker='x')\n",
    "\n",
    "#             plt.title(f'HDBSCAN clustering with min_samples={min_sample}, min_cluster_size={min_cluster_size} (PCA projection)')\n",
    "#             plt.xlabel('PCA Component 1')\n",
    "#             plt.ylabel('PCA Component 2')\n",
    "#             plt.colorbar(label='Cluster label')\n",
    "#             plt.show()\n",
    "\n",
    "            # Generar resultados del clustering usando la clase ClusteringResults\n",
    "            HDBSCAN_Results = ClusteringResults(HDBSCAN_model, df_ae, y_label=\"hour\", hist_library=\"plt\")\n",
    "            HDBSCAN_Results.histograms(extra=f\"min_cluster_size_{HDBSCAN_model.min_cluster_size}_min_samples_{HDBSCAN_model.min_samples}\",\n",
    "                                      save=True, \n",
    "                                      root =f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/\")  # root=\"temporal/clustering_results/HDBSCAN/\")\n",
    "    #         HDBSCAN_Results.joyplot()\n",
    "        else:\n",
    "            print(f\"Not enough clusters found with these min_samples and min_cluster_size values\")\n",
    "            continue\n",
    "\n",
    "    # Guardar los resultados en archivos pickle\n",
    "with open(f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/silhouette_min_samples_{min_sample}_min_cluster_size_{min_cluster_size}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_HDBSCAN, file)\n",
    "with open(f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/DB_min_samples_{min_sample}_min_cluster_size_{min_cluster_size}\", \"wb\") as file:\n",
    "    pkl.dump(DB_score_HDBSCAN, file)\n",
    "with open(f\"temporal_zamuro/zamuro_clustering_results/HDBSCAN_Normal/CH_min_samples_{min_sample}_min_cluster_size_{min_cluster_size}\", \"wb\") as file:\n",
    "    pkl.dump(CH_score_HDBSCAN, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
