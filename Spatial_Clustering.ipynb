{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP/zXuqLZpse+LBmNB+BT5b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KwQ8Zp3llyi-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677780932080,
     "user_tz": 300,
     "elapsed": 96816,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    },
    "outputId": "803522a7-0921-4069-d762-a656cc5dcca5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on colab\n",
      "/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "    !pip install -U ipykernel\n",
    "    !pip install joypy\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Complementary_Files/Datos.csv')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZeYXtNUUhRWh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677780949092,
     "user_tz": 300,
     "elapsed": 17020,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Jaguas_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dir = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'"
   ],
   "metadata": {
    "id": "VJs1xpccgTMm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677780949092,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40137,
     "status": "ok",
     "timestamp": 1677780989227,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "zF81k-h1e7or",
    "outputId": "9d8f0b9f-1434-42a9-bbd5-33ec8f321fec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grabadora: Jaguas_2018 Cantidad Grabaciones: 0\n",
      "Grabadora: G03_m Cantidad Grabaciones: 565\n",
      "Grabadora: G04_m Cantidad Grabaciones: 418\n",
      "Grabadora: G06_m Cantidad Grabaciones: 887\n",
      "Grabadora: G07_m Cantidad Grabaciones: 621\n",
      "Grabadora: G08_m Cantidad Grabaciones: 648\n",
      "Grabadora: G09_m Cantidad Grabaciones: 908\n",
      "Grabadora: G13_m Cantidad Grabaciones: 636\n",
      "Grabadora: G15_m Cantidad Grabaciones: 874\n",
      "Grabadora: G17_m Cantidad Grabaciones: 870\n",
      "Grabadora: G19_m Cantidad Grabaciones: 886\n",
      "Grabadora: G23_m Cantidad Grabaciones: 555\n",
      "Grabadora: G24_m Cantidad Grabaciones: 554\n",
      "Grabadora: G25_m Cantidad Grabaciones: 645\n",
      "Grabadora: G27_m Cantidad Grabaciones: 644\n",
      "Grabadora: G28_m Cantidad Grabaciones: 553\n",
      "Grabadora: G29_m Cantidad Grabaciones: 553\n",
      "Grabadora: G34_m Cantidad Grabaciones: 664\n",
      "Grabadora: G35_m Cantidad Grabaciones: 627\n",
      "Grabadora: G36_m Cantidad Grabaciones: 545\n",
      "Grabadora: G37_m Cantidad Grabaciones: 541\n",
      "Grabadora: G40_m Cantidad Grabaciones: 555\n",
      "Grabadora: G41_m Cantidad Grabaciones: 553\n",
      "Grabadora: G46_m Cantidad Grabaciones: 735\n",
      "Grabadora: G47_m Cantidad Grabaciones: 804\n",
      "Grabadora: G49_m Cantidad Grabaciones: 801\n",
      "Grabadora: G50_m Cantidad Grabaciones: 620\n",
      "Grabadora: G51_m Cantidad Grabaciones: 408\n",
      "Grabadora: G52_m Cantidad Grabaciones: 465\n",
      "Grabadora: G54_m Cantidad Grabaciones: 576\n",
      "Grabadora: G57_m Cantidad Grabaciones: 621\n",
      "Grabadora: G58_m Cantidad Grabaciones: 735\n",
      "Total grabaciones de todas las grabadoras: 20067\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cuenta la cantidad de archivos en cada carpeta de un directorio\"\"\"\n",
    "total_archivos = 0\n",
    "for raiz, carpetas, archivos in os.walk(dir):\n",
    "  carpetas.sort()\n",
    "  # Obtener la cantidad de archivos en la carpeta actual\n",
    "  cantidad_archivos = len(archivos)  \n",
    "  # Sumar la cantidad de archivos actual a la variable total_archivos\n",
    "  total_archivos += cantidad_archivos      \n",
    "  # Imprimir el resultado\n",
    "  print(f\"Grabadora: {os.path.basename(raiz)} Cantidad Grabaciones: {cantidad_archivos}\")\n",
    "print(f\"Total grabaciones de todas las grabadoras: {total_archivos}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Complementary_Files/Datos.csv')"
   ],
   "metadata": {
    "id": "Ki6C-2UTgxHb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677781061656,
     "user_tz": 300,
     "elapsed": 234,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_archivos = pd.DataFrame(columns=['Recorder', 'Date', 'Time', 'Path'])\n",
    "\n",
    "# Ruta del directorio principal que contiene las carpetas y archivos\n",
    "ruta_principal = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "\n",
    "# Recorrer las carpetas y archivos dentro de la ruta principal\n",
    "for raiz, carpetas, archivos in os.walk(ruta_principal):\n",
    "    carpetas.sort()\n",
    "    # Recorrer los archivos de cada carpeta\n",
    "    for archivo in archivos:\n",
    "        # Obtener la ruta completa del archivo\n",
    "        ruta_archivo = os.path.join(raiz, archivo)\n",
    "        \n",
    "        # Obtener el nombre del archivo sin la extensi√≥n\n",
    "        nombre_archivo, extension = os.path.splitext(archivo)\n",
    "        \n",
    "        # Separar el nombre del archivo en partes utilizando el caracter \"_\"\n",
    "        partes_nombre = nombre_archivo.split(\"_\")\n",
    "        \n",
    "        # Obtener los valores deseados de cada parte del nombre del archivo\n",
    "        recorder = partes_nombre[0]\n",
    "        fecha = partes_nombre[1]\n",
    "        hora = partes_nombre[2]\n",
    "        \n",
    "        # Crear un diccionario con los valores del archivo actual\n",
    "        datos_archivo = {\n",
    "            'Recorder': recorder,\n",
    "            'Date': fecha,\n",
    "            'Time': hora,\n",
    "            'Path': ruta_archivo\n",
    "        }\n",
    "        \n",
    "        # Agregar el diccionario al dataframe\n",
    "        df_archivos = df_archivos.append(datos_archivo, ignore_index=True)\n",
    "\n",
    "# Convertir la columna 'Date' al formato 'yyyy/mm/dd'\n",
    "df_archivos['Date'] = df_archivos['Date'].apply(lambda x: pd.to_datetime(x, format='%Y%m%d').strftime('%Y/%m/%d'))\n",
    "\n",
    "# Convertir la columna 'Time' al formato 'hh:mm:ss'\n",
    "df_archivos['Time'] = df_archivos['Time'].apply(lambda x: pd.to_datetime(x, format='%H%M%S').strftime('%H:%M:%S'))\n",
    "\n",
    "#Cargamos informacion adicional de las grabadoras\n",
    "df = pd.read_csv('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Complementary_Files/Datos.csv')\n",
    "\n",
    "#hacemos un Marge con el dataframe anteriormente creado, a base de recorrer los archivos\n",
    "df_archivos = pd.merge(df_archivos, df, on=\"Recorder\")"
   ],
   "metadata": {
    "id": "XrubBMu6TVJX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677781165161,
     "user_tz": 300,
     "elapsed": 95963,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "d2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-Bf9HKmlwMd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677781830737,
     "user_tz": 300,
     "elapsed": 237,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    },
    "outputId": "1ee59371-8f5f-42b5-dc9a-61c6f298414f"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'g1': 12}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "d = {'col1': [\"g1\", \"g2\", \"g3\", \"g4\", \"g5\"], 'col2': [3, 4, 5, 6, 7]}\n",
    "d2 = {'col1': [\"g4\", \"g1\"], 'col2': [15, 20]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "label = 12\n",
    "label_recorder = \"g1\"\n",
    "d3 = {label_recorder: label}\n",
    "\n",
    "df2 = pd.DataFrame(data=d2)\n",
    "df =df.merge(df2, how=\"left\")\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NwFQLTRpi69u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1677782302767,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    },
    "outputId": "b3e54004-314d-4130-ec0c-f1c93d70fe42"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  col1  col2\n",
       "0   g1     3\n",
       "1   g2     4\n",
       "2   g3     5\n",
       "3   g4     6\n",
       "4   g5     7"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-36c1c9db-3038-4c1c-843c-d4175637f38c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36c1c9db-3038-4c1c-843c-d4175637f38c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-36c1c9db-3038-4c1c-843c-d4175637f38c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-36c1c9db-3038-4c1c-843c-d4175637f38c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cMG_87FKb26h",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1677782629203,
     "user_tz": 300,
     "elapsed": 4,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    },
    "outputId": "1e220403-2d56-468f-8069-c555a1168fab"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-27d97cce5078>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"AE_batch_size_56_num_hiddens_64__day_17_hour_0_final.pth\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Models/config_{model_name}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAE\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_hiddens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"num_hiddens\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdataset_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'temporal/dataset_test_ae_jaguas_new.pth'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdataset_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'temporal/dataset_train_ae_jaguas_new.pth'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    769\u001B[0m         \u001B[0mpickle_load_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'encoding'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 771\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    772\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    773\u001B[0m             \u001B[0;31m# The zipfile reader is going to advance the current file position.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m'w'\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Models/config_AE_batch_size_56_num_hiddens_64__day_17_hour_0_final.pth'"
     ]
    }
   ],
   "source": [
    "model_name = \"AE_batch_size_56_num_hiddens_64__day_17_hour_0_final.pth\"\n",
    "config = torch.load(f'Models/config_{model_name}')\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "dataset_test = torch.load(f'temporal/dataset_test_ae_jaguas_new.pth')\n",
    "dataset_train = torch.load(f'temporal/dataset_train_ae_jaguas_new.pth')\n",
    "model.load_state_dict(torch.load(f'Models/{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(\"Features/training_labels_list.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(\"Features/training_samples_list_torch.pth\",  map_location=torch.device('cpu'))\n",
    "X = X[0:6000]\n",
    "y[\"recorder\"] = y[\"recorder\"][0:6000]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_n = X\n",
    "X_scaled = scaler.transform(X)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXhfknkgChqU",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1677780989563,
     "user_tz": 300,
     "elapsed": 155036,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Single methods\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0av3CxthbxXg",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1677780989563,
     "user_tz": 300,
     "elapsed": 155033,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "def plot_silhouette( X, cluster_labels, n_clusters, silhouette_avg, method, extra=\"\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0,\n",
    "                    0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    print(\"Ya debio plotear\")\n",
    "    #plt.savefig(f\"Clustering_Results/{method}/Figures/Silhouette_plot_{n_clusters}.pdf\", format=\"pdf\")\n",
    "    #plt.show()\n",
    "\n",
    "def plot_centroids(cluster_centers, testing, method, extra=\"\"):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    testing._model.to(\"cpu\")\n",
    "    for i, spec in enumerate(cluster_centers):\n",
    "        encodings = spec.reshape(64,9,9)\n",
    "        encodings = torch.tensor(encodings).float()\n",
    "        decodings = testing._model.decoder(encodings).detach().numpy()\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.imshow(decodings[0, :, :], origin=\"lower\", cmap=\"viridis\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    n_cluster = len(cluster_centers)\n",
    "    #plt.savefig(f\"Clustering_Results/{method}/Figures/Centroids_plot_{n_cluster}_{extra}.pdf\", format=\"pdf\")\n",
    "    #plt.show()\n",
    "\n",
    "import math\n",
    "\n",
    "def num_rows_cols(num_elements):\n",
    "    num_rows = int(math.sqrt(num_elements))\n",
    "    num_cols = (num_elements + num_rows - 1) // num_rows\n",
    "    return (num_rows, num_cols)\n",
    "\n",
    "def get_row_col(pos, cols):\n",
    "    row = pos // cols\n",
    "    col = pos % cols\n",
    "    return row, col\n",
    "\n",
    "class Clustering_Results:\n",
    "    def __init__(self, model, y, y_label=\"hour\", hist_library=\"plt\"):\n",
    "        self._labels_cluster = None\n",
    "        self._n_labels = None\n",
    "        self._hist_library = hist_library\n",
    "        self._label = y_label\n",
    "        self._model = model\n",
    "        self._n_clusters = len(set(model.labels_))\n",
    "        self.y = y\n",
    "        self._y = self.converter(y[self._label])\n",
    "        self._n_labels = set(self._y)\n",
    "\n",
    "    def converter(self, var):\n",
    "        aux = []\n",
    "        for i in range(len(var)):\n",
    "            aux.append(var[i].item())\n",
    "        return np.array(aux)\n",
    "\n",
    "    def one_cluster_eval(self, cluster):\n",
    "        index = np.where(self._model.labels_ == cluster)\n",
    "        index = list(index[0])\n",
    "        self._labels_cluster = self._y[index]\n",
    "        return self._labels_cluster\n",
    "\n",
    "    def joyplot(self):\n",
    "        size_x = 8\n",
    "        size_y = 6\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "                labels_cluster.append(y_aux[i][index])\n",
    "            df = pd.DataFrame({'recorder':labels_cluster[0], \"hour\":labels_cluster[1]})\n",
    "            joypy.joyplot(df, by=\"hour\", column=\"recorder\", range_style='own', \n",
    "                            grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                            title=f\"Cluster {cluster} \\nLabels distribution along recorders using recorders as rows\",\n",
    "                            colormap=cm.autumn_r, fade=False)\n",
    "            joypy.joyplot(df, by=\"recorder\", column=\"hour\", range_style='own', \n",
    "                                grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                                title=f\"Cluster {cluster} \\nLabels distribution along recorders using hours as rows\",\n",
    "                                colormap=cm.autumn_r)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "\n",
    "    def histograms(self):\n",
    "        bins = list(self._n_labels)\n",
    "        num_rows, num_cols = num_rows_cols(self._n_clusters)\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 14))\n",
    "        if self._n_clusters <= 3:\n",
    "                axes = np.expand_dims(axes,0)\n",
    "                fig.set_figheight(6)\n",
    "                fig.set_figwidth(12)\n",
    "                if self._n_clusters == 1:\n",
    "                    axes = np.expand_dims(axes,0)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        for hist in range(self._n_clusters):\n",
    "            aux = self.one_cluster_eval(hist)\n",
    "            ax_0, ax_1 = get_row_col(hist, num_cols)\n",
    "            if self._hist_library == \"plt\":\n",
    "                axes[ax_0][ax_1].hist(aux, histtype=\"bar\",\n",
    "                                      color=\"paleturquoise\", cumulative=False,\n",
    "                                      edgecolor='black', \n",
    "                                      linewidth=1.2, bins=bins, stacked=False)\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)\n",
    "            elif self._hist_library == \"sns\":\n",
    "                sns.distplot(aux,bins=np.arange(aux.min(), aux.max()+1),\n",
    "                             hist_kws=dict(edgecolor=\"black\", linewidth=1), \n",
    "                             ax=axes[ax_0, ax_1])\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)              \n",
    "            else:\n",
    "                raise Exception(f\"Library {self._hist_library} unused\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# Kmeans = KMeans(n_clusters=8, random_state=0).fit(X_scaled)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"recorder\", hist_library=\"plt\")\n",
    "Kmeans_Results.joyplot()"
   ],
   "metadata": {
    "id": "lRNaEDUzv8gM",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1677780989847,
     "user_tz": 300,
     "elapsed": 1,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmOS128eHkrS",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1677780989848,
     "user_tz": 300,
     "elapsed": 2,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "S0kf_muhHt6j",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1677780989848,
     "user_tz": 300,
     "elapsed": 2,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
