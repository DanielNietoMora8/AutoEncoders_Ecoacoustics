import torch.nn as nn
from enum import Enum
import sys


class LinearAE(nn.Module):

    """
    Linear layer autoencoder made to reconstruct 1d signals corresponding to ecoacustics audio.
    """

    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, d),
            nn.Tanh()
        )

        self.decoder = nn.Sequential(
            nn.Linear(d, 28 * 28),
            nn.Tanh()
        )

    def forward(self, y):
        h = self.encoder(y)
        y_hat = self.decoder(h)
        return y_hat

    def forward(self, x):
        """
        Method to compute a signal output based on the performed model.

        :param x: Input signal as a tensors.
        :type x: torch.tensor
        :return: Reconstructed signal
        """
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


class LinearVAE(nn.Module):
    """
    A variational autoencoder that uses a linear architecture
    """

    def __init__(self):
        """
        Constructor of the Conv Variational autoencoder model.
        """
        super(LinearVAE, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(784, d**2),
            nn.ReLU(),
            nn.Linear(d**2, d*2),
        )

        self.decoder = nn.Sequential(
            nn.Linear(d, d**2),
            nn.ReLU(),
            nn.Linear(d**2, 784),
            nn.Sigmoid()
        )

    def reparameterize(self, mu, logvar):
        """
        This method computes the variational part of the model. Here it is sampled an instance from the distribution..

        :param mu: Median of the distribution.
        :type mu: torch tensor
        :param logvar: logarithm of the deviation.
        :type logvar: torch tensor
        :return: z
        """

        if self.training:
            std = logvar.mul(0.5).exp_()
            eps = std.data.new(std.size()).normal_()
            return eps.mul(std).add_(mu)
        else:
            return mu

    def forward(self, y):
        """
       This is the forward pass.

       :param y: Spectrogram as a tensor.
       :type y: torch tensor
       :return: Reconstructed image
       :type: torch tensor
       """
        mu_logvar = self.encoder(y.view(-1, 784)).view(-1, 2, d)
        mu = mu_logvar[:, 0, :]
        logvar = mu_logvar[:, 1, :]
        z = self.reparameterize(mu, logvar)
        return self.decoder(z), mu, logvar


class ConvAE(nn.Module):

    """
    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.
    """

    def __init__(self):
        """
        Constructor of the convolutional autoencoder model.
        """
        super().__init__()
        # TODO: To design the final architechture considering the spectrograms sizes.
        # TODO: To correct the current sizes of the decoder.

        self.encoder = nn.Sequential(
            nn.Conv2d(1, 8, (2, 32), stride=(2, 4), padding=0),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(8, 16, (2, 32), stride=(2, 4), padding=0),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(16, 32, (2, 32), stride=(2, 4), padding=0),  # N, 512, 125,969
            nn.ReLU(),
            nn.Conv2d(32, 64, (2, 32), stride=(2, 4), padding=0),  # N, 512, 125,969
        )
        self.decoder = nn.Sequential(  # This is like go in opposite direction respect the encoder
            nn.ConvTranspose2d(64, 32, (2, 32), stride=(2, 4), padding=0, output_padding=(0, 2)),  # N, 32, 126,8000
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, (2, 32), stride=(2, 4), padding=0, output_padding=(0, 2)),  # N, 32, 126,8000
            nn.ReLU(),
            nn.ConvTranspose2d(16, 8, (2, 32), stride=(2, 4), padding=0, output_padding=(0, 0)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.ConvTranspose2d(8, 1, (2, 32), stride=(2, 4), padding=0, output_padding=(1, 3)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.Sigmoid()

        )

    def forward(self, x):
        """
        Method to compute an image output based on the performed architecture.

        :param x: Input spectrogram images as tensors.
        :type x: torch.tensor
        :return: Reconstructed images
        """
        print("---------------------------------------------------")
        print(f"x_shape:{x.shape}")
        encoded = self.encoder(x)
        print(encoded.shape)
        decoded = self.decoder(encoded)
        print(decoded.shape)
        return decoded


class ConvAE2(nn.Module):

    """
    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.
    """

    def __init__(self):
        """
        Constructor of the convolutional autoencoder model.
        """
        super().__init__()
        # TODO: To design the final architechture considering the spectrograms sizes.
        # TODO: To correct the current sizes of the decoder.

        self.encoder = nn.Sequential(
            nn.Conv2d(1, 8, (8, 32), stride=(2, 2), padding=1),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(8, 16, (4, 16), stride=(2, 2), padding=1),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(16, 32, (2, 4), stride=(2, 2), padding=1),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(128, 164, (8, 2), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(164, 228, (3, 1), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(128, 160, (29, 4), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(160, 200, (2, 1), stride=(1, 1), padding=0),  # N, 512, 125,969
        )
        self.decoder = nn.Sequential(  # This is like go in opposite direction respect to the encoder
            # nn.ConvTranspose2d(200, 160, (2, 1), stride=(1, 1), padding=0, output_padding=(0, 1)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(160, 128, (29, 4), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(228, 164, (3, 1), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(164, 128, (8, 2), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            nn.ConvTranspose2d(32, 16, (2, 4), stride=(2, 2), padding=1, output_padding=(1, 0)),  # N, 32, 126,8000
            nn.ReLU(),
            nn.ConvTranspose2d(16, 8, (4, 16), stride=(2, 2), padding=1, output_padding=(0, 1)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.ConvTranspose2d(8, 1, (8, 32), stride=(2, 2), padding=1, output_padding=(1, 1)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.Sigmoid()

        )

class ConvAE3(nn.Module):

    """
    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.
    """

    def __init__(self):
        """
        Constructor of the convolutional autoencoder model.
        """
        super().__init__()
        # TODO: To design the final architechture considering the spectrograms sizes.
        # TODO: To correct the current sizes of the decoder.

        self.encoder = nn.Sequential(
            nn.Conv2d(1, 8, (8, 32), stride=(2, 2), padding=1),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(8, 16, (4, 16), stride=(2, 2), padding=1),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(16, 32, (2, 4), stride=(2, 2), padding=1),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(128, 164, (8, 2), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(164, 228, (3, 1), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(128, 160, (29, 4), stride=(1, 1), padding=0),  # N, 512, 125,969
            # nn.ReLU(),
            # nn.Conv2d(160, 200, (2, 1), stride=(1, 1), padding=0),  # N, 512, 125,969
        )
        self.decoder = nn.Sequential(  # This is like go in opposite direction respect to the encoder
            # nn.ConvTranspose2d(200, 160, (2, 1), stride=(1, 1), padding=0, output_padding=(0, 1)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(160, 128, (29, 4), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(228, 164, (3, 1), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            # nn.ConvTranspose2d(164, 128, (8, 2), stride=(1, 1), padding=0, output_padding=(0, 0)),  # N, 32, 126,8000
            # nn.ReLU(),
            nn.ConvTranspose2d(32, 16, (2, 4), stride=(2, 2), padding=1, output_padding=(1, 0)),  # N, 32, 126,8000
            nn.ReLU(),
            nn.ConvTranspose2d(16, 8, (4, 16), stride=(2, 2), padding=1, output_padding=(0, 1)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.ConvTranspose2d(8, 1, (8, 32), stride=(2, 2), padding=1, output_padding=(1, 1)),  # N, 32, 127,64248
            nn.ReLU(),
            nn.Sigmoid()

        )

    def forward(self, x):

        """
        Method to compute an image output based on the performed model.

        :param x: Input spectrogram images as tensors.
        :type x: torch.tensor
        :return: Reconstructed images
        """
        # print("-------------working-------------------------")
        #print(f"x_shape:{x.shape}")
        encoded = self.encoder(x)
        #print(f"encoded_shape:{encoded.shape}")
        #print(f"Bytes after encoding: {sys.getsizeof(encoded.storage())}")
        decoded = self.decoder(encoded)
        #print(f"decoder_shape: {decoded.shape}")
        return decoded


class CnnVAE(nn.Module):

    """
    A Convolutional Variational Autoencoder
    """
    def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=256):
        """
        Constructor of the Conv Variational autoencoder model.
        """
        super(CnnVAE, self).__init__()

        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder
        self.encConv1 = nn.Conv2d(imgChannels, 16, 5)
        self.encConv2 = nn.Conv2d(16, 32, 5)
        self.encFC1 = nn.Linear(featureDim, zDim)
        self.encFC2 = nn.Linear(featureDim, zDim)

        # Initializing the fully-connected layer and 2 convolutional layers for decoder
        self.decFC1 = nn.Linear(zDim, featureDim)
        self.decConv1 = nn.ConvTranspose2d(32, 16, 5)
        self.decConv2 = nn.ConvTranspose2d(16, imgChannels, 5)

    def encoder(self, x):
        """
        Encoding part

        :param x: Input image
        :type x: torch tensor
        """

        # Input is fed into 2 convolutional layers sequentially
        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)
        # Mu and logVar are used for generating middle representation z and KL divergence loss
        x = F.relu(self.encConv1(x))
        x = F.relu(self.encConv2(x))
        print(f"previously redim: {x.shape}")
        x = x.view(-1, 32*20*20)
        print(f"encoder redim: {x.shape}")
        mu = self.encFC1(x)
        logVar = self.encFC2(x)
        return mu, logVar


    def reparameterize(self, mu, logVar):
        """
        This method computes the variational part of the model. Here it is sampled an instance from the distribution..

        :param mu: Median of the distribution.
        :type mu: torch tensor
        :param logvar: logarithm of the deviation.
        :type logvar: torch tensor
        :return: z
        """
        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps
        std = torch.exp(logVar/2)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decoder(self, z):
        """
        Encoding part

        :param z: Embedding
        :type z: torch tensor
        """
        # z is fed back into a fully-connected layers and then into two transpose convolutional layers
        # The generated output is the same size of the original input
        x = F.relu(self.decFC1(z))
        x = x.view(-1, 32, 20, 20)
        print(f"redim = {x.shape}")
        x = F.relu(self.decConv1(x))
        x = torch.sigmoid(self.decConv2(x))
        return x

    def forward(self, x):
        """
        Forward pass
        """
        print(f'x shape is: {x.shape}')
        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder
        # output, mu, and logVar are returned for loss computation
        mu, logVar = self.encoder(x)
        z = self.reparameterize(mu, logVar)
        out = self.decoder(z)
        return out, mu, logVar


    def forward(self, x):

        """
        Method to compute an image output based on the performed model.

        :param x: Input spectrogram images as tensors.
        :type x: torch.tensor
        :return: Reconstructed images
        """
        # print("-------------working-------------------------")
        #print(f"x_shape:{x.shape}")
        encoded = self.encoder(x)
        #print(f"encoded_shape:{encoded.shape}")
        #print(f"Bytes after encoding: {sys.getsizeof(encoded.storage())}")
        decoded = self.decoder(encoded)
        #print(f"decoder_shape: {decoded.shape}")
        return decoded


class VAE(nn.Module):
    def __init__(self, in_channels=1, num_hiddens: int = 64, zDim: int = 128):
        super(VAE, self).__init__()

        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder
        self.zDim = zDim
        self.num_hiddens = num_hiddens

        self.encConv1 = nn.Conv2d(in_channels=in_channels,
                                  out_channels=self.num_hiddens // 8,
                                  kernel_size=(4, 4),
                                  stride=(1, 1), padding=(0, 0))

        self.encConv2 = nn.Conv2d(in_channels=self.num_hiddens // 8,
                                  out_channels=self.num_hiddens // 4,
                                  kernel_size=(4, 4),
                                  stride=(2, 2), padding=(0, 0))

        self.encConv3 = nn.Conv2d(in_channels=self.num_hiddens // 4,
                                  out_channels=self.num_hiddens //2,
                                  kernel_size=(4, 4),
                                  stride=(2, 2), padding=(0, 0))

        self.encConv4 = nn.Conv2d(in_channels=self.num_hiddens // 2,
                                  out_channels=self.num_hiddens,
                                  kernel_size=(4, 4),
                                  stride=(2, 2), padding=(0, 0))

        # Initializing the fully-connected layer and 2 convolutional layers for decoder

        self.decConv1 = nn.ConvTranspose2d(in_channels=self.num_hiddens,
                                           out_channels=self.num_hiddens // 2,
                                           kernel_size=(4, 4),
                                           stride=(2, 2), padding=(0, 0))

        self.decConv2 = nn.ConvTranspose2d(in_channels=self.num_hiddens // 2,
                                           out_channels=self.num_hiddens // 4,
                                           kernel_size=(4, 4),
                                           stride=(2, 2), padding=(0, 0))

        self.decConv3 = nn.ConvTranspose2d(in_channels=self.num_hiddens // 4,
                                           out_channels=self.num_hiddens // 8,
                                           kernel_size=(4, 4),
                                           stride=(2, 2), padding=(0, 0))

        self.decConv4 = nn.ConvTranspose2d(in_channels=self.num_hiddens // 8,
                                           out_channels=in_channels,
                                           kernel_size=(4, 4),
                                           stride=(1, 1), padding=(0, 0))

    def encoder(self, x):
        # Input is fed into 2 convolutional layers sequentially
        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)
        # Mu and logVar are used for generating middle representation z and KL divergence loss
        print(f"inputs shape {x.shape}")
        x = F.relu(self.encConv1(x))
        print(f"conv1: {x.shape}")
        x = F.relu(self.encConv2(x))
        print(f"conv2: {x.shape}")
        x = F.relu(self.encConv3(x))
        print(f"conv3: {x.shape}")
        x = F.relu(self.encConv4(x))
        print(f"conv4: {x.shape}")

        self.xshape = x.shape
        self.featureDim = x.shape[1]*x.shape[2]*x.shape[3]
        self.encFC1 = nn.Linear(self.featureDim, self.zDim)
        self.encFC2 = nn.Linear(self.featureDim, self.zDim)
        x = x.view(-1, self.featureDim)

        print(f"x.view: {x.shape}")
        mu = self.encFC1(x)
        logVar = self.encFC2(x)
        return mu, logVar

    def reparameterize(self, mu, logVar):
        # Reparameterization takes in the input mu and logVar and sample the mu + std * eps
        std = torch.exp(logVar / 2)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decoder(self, z):
        # z is fed back into a fully-connected layers and then into two transpose convolutional layers
        # The generated output is the same size of the original input
        self.decFC1 = nn.Linear(self.zDim, self.featureDim)
        x = F.relu(self.decFC1(z))
        print(f"decFC1: {x.shape}")
        x = x.view(-1, self.num_hiddens, self.xshape[2], self.xshape[3])
        x = F.relu(self.decConv1(x))
        x = F.relu(self.decConv2(x))
        x = F.relu(self.decConv3(x))
        x = F.relu(self.decConv4(x))
        print(f"deconv2: {x.shape}")
        #x = torch.sigmoid(self.decConv2(x))
        return x

    def forward(self, x):
        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder
        # output, mu, and logVar are returned for loss computation
        mu, logVar = self.encoder(x)
        z = self.reparameterize(mu, logVar)
        out = self.decoder(z)
        return out, mu, logVar


class Encoder(nn.Module):
    def __init__(self, in_channels, num_hiddens):
        super(Encoder, self).__init__()

        self._conv_1 = nn.Conv2d(in_channels=in_channels,
                                 out_channels=num_hiddens // 4,
                                 kernel_size=(4, 4),
                                 stride=(1, 1), padding=(0, 0))
        self._conv_2 = nn.Conv2d(in_channels=num_hiddens // 4,
                                 out_channels=num_hiddens,
                                 kernel_size=(4, 4),
                                 stride=(2, 2), padding=(0, 0))
        # self._conv_3 = nn.Conv2d(in_channels=num_hiddens // 4,
        #                          out_channels=num_hiddens // 2,
        #                          kernel_size=(4, 4),
        #                          stride=(2, 2), padding=(0, 0))
        # self._conv_4 = nn.Conv2d(in_channels=num_hiddens // 2,
        #                          out_channels=num_hiddens,
        #                          kernel_size=(4, 4),
        #                          stride=(2, 2), padding=(0, 0))
        # self._residual_stack = ResidualStack(in_channels=num_hiddens,
        #                                      num_hiddens=num_hiddens,
        #                                      num_residual_layers=num_residual_layers,
        #                                      num_residual_hiddens=num_residual_hiddens)

        self.pooling = nn.MaxPool2d(2)

    def forward(self, inputs):

        # print("Working with new encoder")
        print(f"inputs:{inputs.shape}")
        x = self._conv_1(inputs)
        x = F.leaky_relu(x)
        print(f"conv1: {x.shape}")

        # x = self.pooling(x)
        # print(f"pooling1: {x.shape}")

        x = self._conv_2(x)
        x = F.leaky_relu(x)
        print(f"conv2: {x.shape}")

        # x = self.pooling(x)
        # print(f"pooling2: {x.shape}")

        # x = self._conv_3(x)
        # x = F.relu(x)
        # # print(f"conv3: {x.shape}")

        # x = self._conv_4(x)
        # x = F.relu(x)
        # # print(f"conv4: {x.shape}")

        return x


class Decoder(nn.Module):
    def __init__(self, embedding_dim, num_hiddens):
        super(Decoder, self).__init__()

        # self._conv_1 = nn.Conv2d(in_channels=embedding_dim,
        #                          out_channels=num_hiddens,
        #                          kernel_size=(4, 4),
        #                          stride=(2, 2), padding=(0, 0))

        # self._residual_stack = ResidualStack(in_channels=num_hiddens,
        #                                      num_hiddens=num_hiddens,
        #                                      num_residual_layers=num_residual_layers,
        #                                      num_residual_hiddens=num_residual_hiddens)

        # self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,
        #                                         out_channels=embedding_dim,
        #                                         kernel_size=(4, 4),
        #                                         stride=(2, 2), padding=(0, 0))

        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=embedding_dim,
                                                out_channels=num_hiddens,
                                                kernel_size=(4, 4),
                                                stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        self._conv_trans_3 = nn.ConvTranspose2d(in_channels=num_hiddens,
                                                out_channels=num_hiddens // 4,
                                                kernel_size=(4, 4),
                                                stride=(2, 2), padding=(0, 0),  output_padding=(0, 0))

        # self._conv_trans_4 = nn.ConvTranspose2d(in_channels=num_hiddens // 2,
        #                                         out_channels=num_hiddens // 4,
        #                                         kernel_size=(4, 4),
        #                                         stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        # self._conv_trans_5 = nn.ConvTranspose2d(in_channels=num_hiddens // 4,
        #                                         out_channels=num_hiddens // 8,
        #                                         kernel_size=(4, 4),
        #                                         stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        self._conv_trans_6 = nn.ConvTranspose2d(in_channels=num_hiddens // 4,
                                                out_channels=1,
                                                kernel_size=(4, 4),
                                                stride=(1, 1), padding=(0, 0), output_padding=(0, 0))

        self.transpooling = nn.UpsamplingBilinear2d(scale_factor=2)

        self.Sigmoid = nn.Sigmoid()

    def forward(self, inputs):

        #print("Working with new decoder")
        # print(f"inputs: {inputs.shape}")

        # x = self._conv_1(inputs)
        # x = F.relu(x)
        # print(f"conv1: {x.shape}")

        # x = self._conv_trans_1(x)
        # x = F.relu(x)
        # print(f"convtr1: {x.shape}")

        x = self._conv_trans_2(inputs)
        x = F.leaky_relu(x)
        print(f"convtr2: {x.shape}")

        # x = self.transpooling(x)
        # print(f"transpooling: {x.shape}")

        x = self._conv_trans_3(x)
        x = F.leaky_relu(x)
        print(f"convtr3: {x.shape}")

        # x = self.transpooling(x)
        # print(f"transpooling: {x.shape}")

        # x = self._conv_trans_4(x)
        # x = F.relu(x)
        # # print(f"convtr4: {x.shape}")

        # x = self._conv_trans_5(x)
        # x = F.relu(x)
        # # print(f"convtr5: {x.shape}")

        x = self._conv_trans_6(x)
        x = F.leaky_relu(x)
        print(f"convtr6: {x.shape}")

        x = self.Sigmoid(x)

        return x


class VectorQuantizer(nn.Module):

    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super(VectorQuantizer, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)
        self._commitment_cost = commitment_cost

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)

        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self._commitment_cost * e_latent_loss

        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class VectorQuantizerEMA(nn.Module):

    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):
        super(VectorQuantizerEMA, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost

        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))
        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))
        self._ema_w.data.normal_()

        self._decay = decay
        self._epsilon = epsilon

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)

        # Use EMA to update the embedding vectors
        if self.training:
            self._ema_cluster_size = self._ema_cluster_size * self._decay + \
                                     (1 - self._decay) * torch.sum(encodings, 0)

            # Laplace smoothing of the cluster size
            n = torch.sum(self._ema_cluster_size.data)
            self._ema_cluster_size = (
                    (self._ema_cluster_size + self._epsilon)
                    / (n + self._num_embeddings * self._epsilon) * n)

            dw = torch.matmul(encodings.t(), flat_input)
            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)

            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))

        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self._commitment_cost * e_latent_loss

        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class Model(nn.Module):
    def __init__(self, num_hiddens, num_embeddings, embedding_dim, commitment_cost, decay=0):
        super(Model, self).__init__()

        self._encoder = Encoder(1, num_hiddens)
        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,
                                      out_channels=embedding_dim,
                                      padding=0,
                                      kernel_size=(4, 4),
                                      stride=(2, 2))
        if decay > 0.0:
            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,
                                              commitment_cost, decay)
        else:
            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,
                                           commitment_cost)
        self._decoder = Decoder(embedding_dim,
                                num_hiddens)

    def forward(self, x):
        z = self._encoder(x)
        z = self._pre_vq_conv(z)
        # print(f"Z shape: {z.shape}")

        loss, quantized, perplexity, _ = self._vq_vae(z)
        x_recon = self._decoder(quantized)

        return loss, x_recon, perplexity



class ModelName(str, Enum):

    """
    ModelName is an enumeration and it contains the model names as enumeration members. For more information
    visit the nex website https://docs.python.org/3/library/enum.html
    """
    LinearAE = "LinearAE"
    ConvAE = "ConvAE"
    LinearVAE = "LinearVAE"
    CnnVAE = "CnnVAE"


