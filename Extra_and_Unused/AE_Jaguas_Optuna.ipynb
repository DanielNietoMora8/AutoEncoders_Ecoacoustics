{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"rccfrca51UIR","executionInfo":{"status":"ok","timestamp":1657845223698,"user_tz":300,"elapsed":36248,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["from google.colab import drive, output\n","drive.mount('/content/drive')\n","import sys\n","%cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n","#sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n","#sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n","# sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n","sys.path.append('/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Acceso_Datos_Humboldt/ensayo.xlsx')\n","\n","%load_ext autoreload\n","%autoreload 1\n","!pip install torchaudio\n","!pip install optuna\n","!pip install wandb --upgrade\n","!wandb login\n","output.clear()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_EBNUB8-NwZo","colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"status":"error","timestamp":1657845244405,"user_tz":300,"elapsed":20718,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}},"outputId":"7121bbbb-2009-4a72-ce65-651b334e06a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n","  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"]},{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-25d7bac737d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlertLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'AlertLevel' from 'wandb' (unknown location)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from __future__ import print_function\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa\n","import librosa.display\n","\n","from six.moves import xrange\n","\n","#import umap\n","import wandb\n","import torch\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","\n","from scipy import signal\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","from torch.utils.data import random_split\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","import torch.nn.functional as F\n","\n","#from ResidualStack import ResidualStack\n","#from Residual import Residual\n","\n","#from Project.Modules.Dataloader import EcoDataTesisReduced\n","from Jaguas_DataLoader import SoundscapeData\n","#from Models import VAE\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = xm.xla_device()\n","print(device)\n","\n","from datetime import timedelta\n","import wandb\n","from wandb import AlertLevel\n","\n","import optuna\n","\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buqqA9PWp6BE","executionInfo":{"status":"aborted","timestamp":1657845244402,"user_tz":300,"elapsed":8,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["def display_images(model_outputs, epoch):\n","\n","    \"\"\"\n","    Plots the original image and reconstructed image by the machine learning model. This function receives\n","    a list with a length of 3 like [epochs, original images, reconstructed images].\n","\n","    Original images and reconstructed images have a shape of [num_images, num_channels, rows, columns].\n","\n","    :param model_outputs: Outputs of the machine learning model.\n","    :param epoch: Desire epoch to visualize.\n","    :type epoch: int\n","    :return: matplotlib plot\n","    \"\"\"\n","\n","    recon = model_outputs[epoch][2].to(\"cpu\").detach().numpy()\n","    img = model_outputs[epoch][1].to(\"cpu\").detach().numpy()\n","    for index in range(recon.shape[0]):\n","        ax1 = plt.subplot(2, recon.shape[0], index+1)\n","        ax2 = plt.subplot(2, recon.shape[0], index+recon.shape[0]+1)\n","\n","        librosa.display.specshow(librosa.power_to_db(recon[index, 0], ref=np.max),\n","                                 y_axis='mel', fmax=10000,\n","                                 x_axis='time', ax=ax1)\n","\n","        librosa.display.specshow(librosa.power_to_db(img[index, 0], ref=np.max),\n","                                 y_axis='mel', fmax=10000,\n","                                 x_axis='time', ax=ax2)\n","    plt.show()\n","\n","def plot_reconstructions(imgs_original, imgs_reconstruction, num_views: int=8):\n","    output = torch.cat((imgs_original[0:num_views], imgs_reconstruction[0:num_views]), 0)\n","    img_grid = make_grid(output, nrow=8, pad_value=20)\n","    fig, ax = plt.subplots(figsize=(20,5))\n","    ax.imshow(img_grid[1,:,:].cpu(), vmin=0, vmax=1)\n","    ax.axis(\"off\")\n","    plt.show()\n","    return fig\n","\n","def testModel(model, iterator):\n","    model.eval()\n","    (valid_originals, _,_) = next( iterator)\n","    valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1], \n","                                                  valid_originals.shape[2], valid_originals.shape[3]))\n","    valid_originals = torch.unsqueeze(valid_originals,1)\n","\n","    valid_originals = valid_originals.to(device)\n","\n","    vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n","    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n","    valid_reconstructions = model._decoder(valid_quantize)\n","    \n","    fig = plot_reconstructions(valid_originals, valid_reconstructions, 8)\n","\n","    recon_error = F.mse_loss(valid_originals, valid_reconstructions)\n","\n","    return fig, recon_error\n","\n","def define_encoder(trial):\n","    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n","    layers = []\n","\n","    in_features = 1\n","    for i in range(n_layers):\n","        #out_features = i+1**2\n","        out_features = trial.suggest_int(\"n_units_l{}\".format(i), (i+2)**3, 2*(i+2)**3, log=True)\n","        layers.append(nn.Conv2d(in_channels=in_features,\n","                                out_channels=out_features,\n","                                kernel_size=(4, 4),\n","                                padding=(0, 0),\n","                                stride=(1, 1)\n","                                )\n","                      )\n","        if i != n_layers - 1:\n","            layers.append(nn.ReLU())\n","        p = trial.suggest_float(\"dropout_{}\".format(i), 0.2, 0.5)\n","        #layers.append(nn.Dropout(p))\n","        encoder = layers\n","        in_features = out_features\n","\n","    return nn.Sequential(*encoder)\n","\n","\n","def define_decoder(trial):\n","\n","    decoder = []\n","    encoder = define_encoder(trial)\n","    decoder_counter = 0\n","    for i in range(len(encoder) - 1, -1, -1):\n","        decoder.append(encoder[i])\n","        if type(decoder[decoder_counter]) == torch.nn.modules.conv.Conv2d:\n","            decoder[decoder_counter] = nn.ConvTranspose2d(in_channels=encoder[i].out_channels,\n","                                                          out_channels=encoder[i].in_channels,\n","                                                          kernel_size=encoder[i].kernel_size,\n","                                                          stride=encoder[i].stride,\n","                                                          padding=encoder[i].padding)\n","        decoder_counter += 1\n","\n","    decoder.append(nn.ReLU())\n","    #decoder.append(nn.Sigmoid())\n","    decoder = nn.Sequential(*decoder)\n","\n","    return decoder\n","\n","\n","class define_model(nn.Module):\n","    def __init__(self, trial):\n","        super().__init__()\n","        self.encoder = define_encoder(trial)\n","        self.decoder = define_decoder(trial)\n","        #self.autoencoder = define_autoencoder(trial)\n","\n","    def forward(self, x):\n","\n","        encoded = self.encoder(x)\n","        # print(f\"shape {encoded.shape}\" )\n","        decoder = self.decoder(encoded)\n","\n","        return decoder\n","\n","def train_model(model, optimizer, num_images, dataloader):\n","\n","    model.train()\n","    criterion = nn.MSELoss()\n","\n","    i = 0\n","    for (img, _, _) in dataloader:\n","        print(f\"i:{i}\")\n","        img = img.reshape(-1, img.shape[2], img.shape[3])\n","        img = torch.unsqueeze(img, 1)\n","        img_cuda = img.to(device)\n","        recon = model(img_cuda)\n","        loss = criterion(recon, img_cuda)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        i += 1\n","\n","        if i >= num_images:\n","            print(\"saliendo porque entró en el código\")\n","            break\n","\n","        if (i+1) % 20  == 0:\n","            print(\"reconstruction\")\n","            recon = recon.to(\"cpu\")\n","            fig = plot_reconstructions(img, recon)\n","            images = wandb.Image(fig, caption= f\"recon_error: {np.round(loss.item(),4)}\")\n","            wandb.log({\"examples\": images})\n","            \n","        wandb.log({\"loss\":loss.item()}\n","                 )\n","    return loss, img, recon\n","\n","def objective(trial):\n","\n","    config = {\n","    \"batch_size\" : 20,\n","    \"num_epochs\": 15,\n","    # \"num_training_updates\" : len(dataset_train),\n","    \"num_hiddens\" : 64,\n","    \"embedding_dim\" : 128,\n","    \"zDim\" : 128,\n","    \"commitment_cost\" : 0.25,\n","    \"decay\" : 0.99,\n","    \"learning_rate\" : 1e-3,\n","    \"dataset\": \"Audios Jaguas\",\n","    \"architecture\": \"AE\",\n","              }\n","\n","    wandb.config = config\n","    num_images = 2000\n","\n","    print(\"-----------Running-----------\")\n","    root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n","    \n","    dataset = SoundscapeData(root_path, 1, \"wav\")\n","    dataset_train, dataset_test = random_split(dataset,\n","                                                [round(len(dataset)*0.20),\n","                                                 len(dataset) - round(len(dataset)*0.20)], \n","                                                generator=torch.Generator().manual_seed(1024))\n","    training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle = False)\n","    test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n","\n","    model = define_model(trial).to(device)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(), trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True))\n","    \n","    \n","    wandb.init(project=\"AE-Jaguas\", config=config)\n","    wandb.watch(model, F.mse_loss, log=\"all\", log_freq=1)\n","\n","    outputs = []\n","\n","    for epoch in range(config['num_epochs']):\n","        loss, img, recon= train_model(model=model, optimizer=optimizer, num_images=num_images, dataloader=training_loader)\n","        recon = recon.to('cpu')\n","        print(f'Epoch:{epoch + 1}, Loss: {loss.item():.4f}')\n","        outputs.append((epoch, img, recon))\n","        fig = plot_reconstructions(img,recon)   \n","        # display_images(outputs, epoch)\n","        images = wandb.Image(fig, caption= f\"recon_error: {np.round(loss.item(),4)}\")\n","        wandb.log({\"examples\": images})\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bap9XkrWtD_n","executionInfo":{"status":"aborted","timestamp":1657845244403,"user_tz":300,"elapsed":8,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["wandb.finish()\n","study = optuna.create_study(directions=[\"minimize\"])\n","study.optimize(objective, n_trials=5, timeout=300)\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q5q9_NeODaS","executionInfo":{"status":"aborted","timestamp":1657845244404,"user_tz":300,"elapsed":9,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":[" root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n","\n","\n","dataset = SoundscapeData(root_path, 1, \"wav\")\n","dataset_train, dataset_test = random_split(dataset,\n","                                           [round(len(dataset)*0.0250), len(dataset) - round(len(dataset)*0.0250)], \n","                                           generator=torch.Generator().manual_seed(1024))\n","\n","config = {\n","    \"batch_size\" : 20,\n","    \"num_epochs\": 15,\n","    \"num_training_updates\" : len(dataset_train),\n","    \"num_hiddens\" : 64,\n","    \"embedding_dim\" : 128,\n","    \"zDim\" : 128,\n","    \"commitment_cost\" : 0.25,\n","    \"decay\" : 0.99,\n","    \"learning_rate\" : 1e-3,\n","    \"dataset\": \"Audios Jaguas\",\n","    \"architecture\": \"VQ-VAE\",\n","}\n","\n","training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n","test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n","\n","\n","model = VAE(num_hiddens=config[\"num_hiddens\"],\n","              zDim=config[\"zDim\"]).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n","\n","\n","wandb.finish()\n","wandb.init(project=\"VAE-Jaguas\", config=config)\n","wandb.watch(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4zo23F2Li8a","executionInfo":{"status":"aborted","timestamp":1657845244405,"user_tz":300,"elapsed":10,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["def plot_reconstructions(imgs_original, imgs_reconstruction, num_views: int=8):\n","    output = torch.cat((imgs_original[0:num_views], imgs_reconstruction[0:num_views]), 0)\n","    img_grid = make_grid(output, nrow=8, pad_value=20)\n","    fig, ax = plt.subplots(figsize=(20,5))\n","    ax.imshow(img_grid[1,:,:].cpu(), vmin=0, vmax=1)\n","    ax.axis(\"off\")\n","    plt.show()\n","\n","def testModel(model, iterator):\n","    model.eval()\n","    (valid_originals, _,_) = next( iterator)\n","    valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1], \n","                                                  valid_originals.shape[2], valid_originals.shape[3]))\n","    valid_originals = torch.unsqueeze(valid_originals,1)\n","\n","    valid_originals = valid_originals.to(device)\n","\n","    mu, logvar = model.encoder(valid_originals)\n","    z = model.reparameterize(mu, logvar)\n","    valid_reconstructions = model.decoder(z)\n","    plot_reconstructions(valid_originals, valid_reconstructions)\n","    recon_error = F.mse_loss(valid_originals, valid_reconstructions)\n","    return fig, recon_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXVgqtR5NVPd","executionInfo":{"status":"aborted","timestamp":1657845244405,"user_tz":300,"elapsed":10,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["model.train()\n","train_res_recon_error = []\n","train_res_perplexity = []\n","iterator = iter(test_loader)\n","wandb.watch(model, F.mse_loss, log=\"all\", log_freq=1)\n","error_files = []\n","\n","for epoch in range(config[\"num_epochs\"]):\n","  for i in xrange(config[\"num_training_updates\"]):\n","      model.train()\n","      try:\n","         (data, _,_) = next(iter(training_loader))\n","      except:\n","        error_files.append[i]\n","        continue\n","          \n","      data = torch.reshape(data, (data.shape[0] * data.shape[1], data.shape[2], data.shape[3]))\n","      data = torch.unsqueeze(data,1)\n","      data = data.to(device)\n","      print(data.shape)\n","      optimizer.zero_grad()\n","      data_recon, mu, logvar = model(data)\n","      print(data_recon.shape)\n","      \n","      recon_error = F.mse_loss(data_recon, data) #/ data_variance\n","      loss = recon_error\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","      print(f'epoch: {epoch} of {config[\"num_epochs\"]} \\t iteration: {(i+1)} of {config[\"num_training_updates\"]} \\t loss: {np.round(loss.item(),4)}')\n","\n","      if (i+1) % 5 == 0:\n","        #torch.save(model.state_dict(),f'model_{epoch}_{i}.pkl')\n","        fig, test_error = testModel(model, iterator)\n","        images = wandb.Image(fig, caption= f\"recon_error: {np.round(test_error.item(),4)}\")\n","        wandb.log({\"examples\": images})\n","\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"AE_Jaguas_Optuna.ipynb","provenance":[],"mount_file_id":"1tGUIBVbWKFSGUDrciztVrEPV343i03GI","authorship_tag":"ABX9TyNkvre15br86StVWZCTni3W"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}