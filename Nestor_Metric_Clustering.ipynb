{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/temporal')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Extra_and_Unused')\n",
    "    root = \"/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project\"\n",
    "else:\n",
    "    print(\"Running local\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Jo5E4cmabyk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, stats\n",
    "from scipy.spatial import distance\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lumrCsRYbMvs"
   },
   "outputs": [],
   "source": [
    "def uncertainity_mean(X_train1,pred,means,covariances,weights,km,numberofdimens):\n",
    "    X_train1[\"pred\"]=pred\n",
    "    meanss=means\n",
    "    weiths=weights\n",
    "    variances=[]\n",
    "    covariancematrix=covariances\n",
    "    for i in covariancematrix: #The diagonal of cov is the variance of each gmm\n",
    "        diag=np.diag(i)\n",
    "        variances.append(diag)\n",
    "    variances=np.array(variances)\n",
    "    countiterations=0\n",
    "    unc=[]\n",
    "    un=[]\n",
    "\n",
    "    sep=[]\n",
    "    for g in range(0,len(meanss)):    ##Frechet distance\n",
    "        for t in range(0,len(meanss)):\n",
    "            if (t==g):\n",
    "                continue\n",
    "            else:\n",
    "                sep.append(frechetDistance(meanss[t],meanss[g],covariancematrix[t],covariancematrix[g]))\n",
    "    separation=np.array(sep)\n",
    "    Dmin=np.array(separation).min()\n",
    "    Dmax=np.array(separation).max()\n",
    "    Sep=(Dmax/Dmin)*(1/separation.sum())\n",
    "\n",
    "    for i in range(0,len(meanss)):    #Uncertainty    ##i run each cluster\n",
    "        cweight=weiths[i]\n",
    "        cmean=meanss[i]\n",
    "        cvar=variances[i]\n",
    "        lim_inf=cmean-(km*cvar)\n",
    "        lim_sup=cmean+(km*cvar)\n",
    "        unc=[]\n",
    "        o=X_train1[X_train1[\"pred\"]==i].iloc[:,:numberofdimens].copy()\n",
    "\n",
    "        for j in range(len(o)): # j run each data\n",
    "            DM=distance.mahalanobis(np.array(o.iloc[j]), cmean, np.linalg.inv(covariancematrix[i]))\n",
    "            varbool=(DM>=km)\n",
    "            varunc=[]\n",
    "            #print(\"varbool\",varbool)\n",
    "            if (varbool):\n",
    "                varunc.append(2*km*DM)\n",
    "            else:\n",
    "                vs=((DM**2)+(km*DM)+(km**2)/2)\n",
    "                varunc.append(vs)\n",
    "        un.append(np.array(varunc))\n",
    "    return np.sum(un)/Sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "q5CMBV6pUcIo"
   },
   "outputs": [],
   "source": [
    "def merguncertain (xtrain,y,Means,covariances,weights,numberofdimens):\n",
    "    X_train11=xtrain\n",
    "    X_train11[\"pred\"]=y\n",
    "    UNIndex=[]\n",
    "    len(Means)\n",
    "    Pairwise=np.ones((len(Means), len(Means)))\n",
    "    labels=[]\n",
    "    for i in range(len(Means)):\n",
    "        labels.append(i)\n",
    "        for j in range(len(Means)):\n",
    "                if (i==j):\n",
    "                    continue\n",
    "                Pairwise[j,i]=(frechetDistance(Means[i],Means[j],covariances[i],covariances[j]))\n",
    "\n",
    "    while len(Pairwise) > 2 :\n",
    "            UNIndex.append(uncertainity_mean(X_train11.iloc[:,:numberofdimens],X_train11[\"pred\"],Means,covariances,weights,1,numberofdimens))\n",
    "\n",
    "            Similar_clusters=np.unravel_index(Pairwise.argmin(),Pairwise.shape)\n",
    "            Similar_clusters_labels=( labels[Similar_clusters[0]],labels[Similar_clusters[1]])\n",
    "\n",
    "            #update Mean\n",
    "            data_size_1=len(X_train11[X_train11[\"pred\"]==Similar_clusters_labels[0]])\n",
    "            data_size_2=len(X_train11[X_train11[\"pred\"]==Similar_clusters_labels[1]])\n",
    "            mean_1=Means[Similar_clusters[0]]\n",
    "            mean_2=Means[Similar_clusters[1]]\n",
    "            new_mean = (data_size_1* mean_1 + data_size_2*mean_2)/(data_size_1 + data_size_2)\n",
    "            Means[Similar_clusters[0]]=new_mean\n",
    "\n",
    "            #Update labels\n",
    "            X_train11[\"pred\"]=X_train11[\"pred\"].replace(Similar_clusters_labels[1], Similar_clusters_labels[0])\n",
    "            #print(X_train11[\"pred\"].unique())\n",
    "            NewCovariance=X_train11[X_train11[\"pred\"]==Similar_clusters_labels[0]].iloc[:,:numberofdimens].cov()\n",
    "            covariances[Similar_clusters[0]]=NewCovariance\n",
    "            weights[Similar_clusters[0]]=weights[Similar_clusters[0]]+weights[Similar_clusters[1]]\n",
    "\n",
    "            updpairwisecolum=[]\n",
    "            for j in range(0,len(Means)):\n",
    "                    updpairwisecolum.append((frechetDistance(Means[Similar_clusters[0]],Means[j],covariances[Similar_clusters[0]],covariances[j])))\n",
    "            #print(updpairwisecolum)\n",
    "            Pairwise[:,Similar_clusters[0]]=updpairwisecolum\n",
    "            Pairwise[Similar_clusters[0],:]=updpairwisecolum\n",
    "\n",
    "            Means= np.delete(Means, (Similar_clusters[1]), axis=0)\n",
    "            weights= np.delete(weights, (Similar_clusters[1]), axis=0)\n",
    "            covariances=np.delete(covariances, (Similar_clusters[1]), axis=0)\n",
    "            labels.remove(Similar_clusters_labels[1])\n",
    "            Pairwise= np.delete(Pairwise, (Similar_clusters[1]), axis=0)\n",
    "            Pairwise= np.delete(Pairwise, (Similar_clusters[1]), axis=1)\n",
    "            np.fill_diagonal(Pairwise, 1)\n",
    "    return UNIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F7SrKoKiyvkP"
   },
   "outputs": [],
   "source": [
    "def frechetDistance(u1,u2,E1,E2):\n",
    "    return (LA.norm(np.absolute(u1-u2)**2))+np.trace(E1+E2-(2*(E1*E2)**(0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load(f'{root}/Jaguas/temporal/Features/AE_No_rain_98/AE_features_day_11_hour_21_No_rain_Audios_Jaguas.pth', map_location=\"cpu\")\n",
    "df = pd.DataFrame(X.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Normalizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "Normalizer_ = Normalizer().fit(X)\n",
    "X_norm = Normalizer_.transform(X)\n",
    "PCA_ = PCA(n_components=60).fit(X_norm)\n",
    "X_PCA = PCA_.transform(X_norm)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_PCA)\n",
    "reducer = umap.UMAP(min_dist=0.9, n_components=2)\n",
    "X_UMAP = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmM3oDe6UhOt"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(f'{root}/Jaguas/temporal/Features/AE_No_rain_98/AE_features_day_11_hour_21_No_rain_Audios_Jaguas.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfJfcKP7UhRG"
   },
   "outputs": [],
   "source": [
    "df=df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeXg56ASq5pa"
   },
   "source": [
    "## Approach 1: Calculating performance in base of index UF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms37QYslqC2n"
   },
   "outputs": [],
   "source": [
    "Index=[]\n",
    "liminfierior=2\n",
    "limsuperior=20\n",
    "for i in range(2,20):\n",
    "\n",
    "    X_train111=df.copy()\n",
    "    GM = GaussianMixture(n_components=i, covariance_type=\"full\",random_state=1).fit(X_train111)\n",
    "    X_train111[\"pred\"]=GM.predict(X_train111)\n",
    "    Means=GM.means_\n",
    "    covariances=GM.covariances_\n",
    "    weights=GM.weights_\n",
    "    numberofdimens=X_train111.shape[1]-1\n",
    "    y=X_train111[\"pred\"]\n",
    "    Index.append(uncertainity_mean(X_train111.iloc[:,:numberofdimens],X_train111[\"pred\"],Means,covariances,weights,1,numberofdimens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "z6G-8oV0q4Vd",
    "outputId": "9f060f85-a211-48cd-9ee3-debb9f18a8a2"
   },
   "outputs": [],
   "source": [
    "change=pd.DataFrame(Index,columns=[\"UFindex\"])\n",
    "change[\"ind\"]=np.linspace(2,len(change)+2,len(change))\n",
    "\n",
    "change.plot(\"ind\",[\"UFindex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Y7mEwGRnn-Y",
    "outputId": "d0d40edd-5db5-4d84-ed1d-859dc4ad2c0e"
   },
   "outputs": [],
   "source": [
    "change[\"UFindex\"].argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-y4yVx4dkrQ"
   },
   "source": [
    "## Approach 2: Meging methodology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "IEFl9INY5OU6",
    "outputId": "a166a1bb-5236-4c6b-bdd8-c96955f7375a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df\n",
    "X_train111=df.copy()\n",
    "GM = GaussianMixture(n_components=20, covariance_type=\"full\", random_state=2).fit(X_train111)\n",
    "X_train111[\"pred\"]=GM.predict(X_train111)\n",
    "Means=GM.means_\n",
    "covariances=GM.covariances_\n",
    "weights=GM.weights_\n",
    "y=X_train111[\"pred\"]\n",
    "index=merguncertain (X,y,Means,covariances,weights,X.shape[1])\n",
    "index.reverse()\n",
    "change=pd.DataFrame(index,columns=[\"a\"])\n",
    "change[\"ind\"]=np.linspace(0,len(change),len(change))\n",
    "\n",
    "change.plot(\"ind\",[\"a\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggCZKnZcDt71"
   },
   "outputs": [],
   "source": [
    "change=change.iloc[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0tmChDa-9gH",
    "outputId": "16fc5166-d4f9-4614-ca31-96a2b331b8e4"
   },
   "outputs": [],
   "source": [
    "recomended=change[\"a\"].argmin()+2\n",
    "GMM_test= GaussianMixture(n_components=recomended, covariance_type=\"full\",random_state=0).fit(X)\n",
    "predy=GMM_test.predict(X)\n",
    "recomended"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
