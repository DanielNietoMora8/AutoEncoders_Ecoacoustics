import torch
import torch.nn as nn
import torch.nn.functional as F
from Modules.Utils import compute_size as cs
from enum import Enum


class ConvAE(nn.Module):

    """
    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.
    """

    def __init__(self, num_hiddens: int=64):
        """
        Constructor of the convolutional autoencoder model.
        """
        super().__init__()
        # TODO: To design the final architechture considering the spectrograms sizes.
        # TODO: To correct the current sizes of the decoder.

        self.encoder = nn.Sequential(
            nn.Conv2d(1, num_hiddens // 8, kernel_size=8, stride=3, padding=0),  # N, 256, 127, 8004
            nn.ReLU(),
            nn.Conv2d(num_hiddens // 8, num_hiddens // 4, kernel_size=8, stride=3, padding=0),  # N, 512, 125,969
            nn.ReLU(),
            nn.Conv2d(num_hiddens // 4, num_hiddens // 2, kernel_size=4, stride=3, padding=0),  # N, 512, 125,969
            nn.ReLU(),
            nn.Conv2d(num_hiddens // 2, num_hiddens, kernel_size=2, stride=2, padding=0),  # N, 512, 125,969
            nn.ReLU()
             )
        self.decoder = nn.Sequential(  # This is like go in opposite direction respect the encoder
            nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, kernel_size=2, stride=2, padding=0, output_padding=0),  # N, 32, 126,8000
            nn.ReLU(),
            nn.ConvTranspose2d(num_hiddens // 2, num_hiddens // 4, kernel_size=4, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248
            nn.ReLU(),
            nn.ConvTranspose2d(num_hiddens // 4, num_hiddens // 8, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248
            nn.ReLU(),
            nn.ConvTranspose2d(num_hiddens // 8, 1, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248
            nn.Sigmoid()

        )

    def forward(self, x):
        """
        Method to compute an image output based on the performed model.

        :param x: Input spectrogram images as tensors.
        :type x: torch.tensor
        :return: Reconstructed images
        """
        #print(f"x_shape:{x.shape}")
        encoded = self.encoder(x)
        print("encoder_shape: ", encoded.shape)
        decoded = self.decoder(encoded)
        print("decoder_shape: ",decoded.shape)
        return decoded


class VAE(nn.Module):
    def __init__(self, in_channels=1, num_hiddens: int = 64, zDim: int = 128):
        super(VAE, self).__init__()

        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder
        self.zDim = zDim
        self.num_hiddens = num_hiddens

        self.encConv1 = nn.Conv2d(in_channels=in_channels,
                                  out_channels=self.num_hiddens // 4,
                                  kernel_size=(4, 4),
                                  stride=(1, 1), padding=(0, 0))

        self.encConv2 = nn.Conv2d(in_channels=self.num_hiddens // 4,
                                  out_channels=self.num_hiddens,
                                  kernel_size=(4, 4),
                                  stride=(2, 2), padding=(0, 0))

        self.encConv3 = nn.Conv2d(in_channels=self.num_hiddens,
                                  out_channels=self.num_hiddens * 2,
                                  kernel_size=(4, 4),
                                  stride=(2, 2), padding=(0, 0))
        #
        # self.encConv4 = nn.Conv2d(in_channels=self.num_hiddens // 2,
        #                           out_channels=self.num_hiddens,
        #                           kernel_size=(4, 4),
        #                           stride=(2, 2), padding=(0, 0))

        self.flatten = nn.Flatten()

        # Initializing the fully-connected layer and 2 convolutional layers for decoder

        self.decConv1 = nn.ConvTranspose2d(in_channels=self.num_hiddens * 2,
                                           out_channels=self.num_hiddens,
                                           kernel_size=(4, 4),
                                           stride=(2, 2), padding=(0, 0), output_padding=(1, 1))

        self.decConv2 = nn.ConvTranspose2d(in_channels=self.num_hiddens,
                                           out_channels=self.num_hiddens // 4,
                                           kernel_size=(4, 4),
                                           stride=(2, 2), padding=(0, 0))

        # self.decConv3 = nn.ConvTranspose2d(in_channels=self.num_hiddens // 4,
        #                                    out_channels=self.num_hiddens // 8,
        #                                    kernel_size=(4, 4),
        #                                    stride=(2, 2), padding=(0, 0))

        self.decConv4 = nn.ConvTranspose2d(in_channels=self.num_hiddens // 4,
                                           out_channels=in_channels,
                                           kernel_size=(4, 4),
                                           stride=(1, 1), padding=(0, 0))

    def encoder(self, x):
        # Input is fed into 2 convolutional layers sequentially
        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)
        # Mu and logVar are used for generating middle representation z and KL divergence loss
        print(f"inputs shape {x.shape}")
        x = F.relu(self.encConv1(x))
        print(f"conv1: {x.shape}")
        x = F.relu(self.encConv2(x))
        print(f"conv2: {x.shape}")
        x = F.relu(self.encConv3(x))
        print(f"conv3: {x.shape}")
        # x = F.relu(self.encConv4(x))
        # print(f"conv4: {x.shape}")
        #x = self.flatten(x)

        self.xshape = x.shape
        self.featureDim = x.shape[1]*x.shape[2]*x.shape[3]
        print(x.device)
        self.encFC1 = nn.Linear(self.featureDim, self.zDim)
        self.encFC2 = nn.Linear(self.featureDim, self.zDim)
        x = x.view(-1, self.featureDim)

        print(f"x.view: {x.shape}")
        mu = self.encFC1(x.to('cpu'))
        logVar = self.encFC2(x.to('cpu'))
        return mu, logVar

    def reparameterize(self, mu, logVar):
        # Reparameterization takes in the input mu and logVar and sample the mu + std * eps
        std = torch.exp(logVar / 2)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decoder(self, z):
        # z is fed back into a fully-connected layers and then into two transpose convolutional layers
        # The generated output is the same size of the original input
        self.decFC1 = nn.Linear(self.zDim, self.featureDim)
        x = F.relu(self.decFC1(z))
        print(f"decFC1: {x.shape}")
        x = x.view(-1, self.num_hiddens*2, self.xshape[2], self.xshape[3])
        print(f"view: {x.shape}")
        x = F.relu(self.decConv1(x))
        print(f"decC1: {x.shape}")
        x = F.relu(self.decConv2(x))
        print(f"decC2: {x.shape}")
        # x = F.relu(self.decConv3(x))
        x = F.relu(self.decConv4(x))
        print(f"deconv2: {x.shape}")
        #x = torch.sigmoid(self.decConv2(x))
        return x

    def forward(self, x):
        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder
        # output, mu, and logVar are returned for loss computation
        mu, logVar = self.encoder(x)
        z = self.reparameterize(mu, logVar)
        out = self.decoder(z)
        return out, mu, logVar


class Reshape(nn.Module):
    def __init__(self, *args):
        super().__init__()
        self.shape = args

    def forward(self, x):
        return x.view(self.shape)


class VAE2(nn.Module):
    def __init__(self, imgChannels=1, num_hiddens: int = 64, zDim: int=128):
        super(VAE2, self).__init__()

        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder
        self.num_hiddens = num_hiddens
        self.encConv1 = nn.Conv2d(imgChannels, num_hiddens // 4, 8, 3)
        self.encConv2 = nn.Conv2d(num_hiddens // 4, num_hiddens, 4, 2)
        # self.encConv3 = nn.Conv2d(num_hiddens // 2, num_hiddens // 2, 3, 2)
        # self.encConv4 = nn.Conv2d(num_hiddens // 2, num_hiddens, 3, 2)
        # self.encConv5 = nn.Conv2d(num_hiddens, num_hiddens, 3, 2)
        # self.encConv6 = nn.Conv2d(num_hiddens // 2, num_hiddens, 3, 2)

        self.featureDim = num_hiddens*84*84
        self.encFC1 = nn.Linear(self.featureDim, zDim)
        self.encFC2 = nn.Linear(self.featureDim, zDim)

        # Initializing the fully-connected layer and 2 convolutional layers for decoder
        self.decFC1 = nn.Linear(zDim, self.featureDim)
        self.decConv1 = nn.ConvTranspose2d(num_hiddens, num_hiddens // 4, 4, 2, output_padding=0)
        self.decConv2 = nn.ConvTranspose2d(num_hiddens // 4, imgChannels, 8, 3, output_padding=0)
        # self.decConv3 = nn.ConvTranspose2d(num_hiddens // 2, num_hiddens // 2, 3, 2, output_padding=0)
        # self.decConv4 = nn.ConvTranspose2d(num_hiddens // 2, num_hiddens // 4, 4, 2, output_padding=0)
        # self.decConv5 = nn.ConvTranspose2d(num_hiddens // 4, imgChannels, 4, 2, output_padding=1)
        # self.decConv5 = nn.ConvTranspose2d(num_hiddens // 4, num_hiddens // 4, 3, 2)
        # self.decConv5 = nn.ConvTranspose2d(num_hiddens // 4, imgChannels, 3, 2)

        self.Sigmoid = nn.Sigmoid()

    def encoder(self, x):

        # Input is fed into 2 convolutional layers sequentially
        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)
        # Mu and logVar are used for generating middle representation z and KL divergence loss
        # print(x.shape)
        x = F.relu(self.encConv1(x))
        # print(f'conv1{x.shape}')
        x = F.relu(self.encConv2(x))
        # print(f'conv2{x.shape}')
        # x = F.relu(self.encConv3(x))
        # x = F.relu(self.encConv4(x))
        # x = F.relu(self.encConv5(x))
        # print(f'conv3{x.shape}')
        x = x.view(-1, self.featureDim)
        # print(f'encx.view{x.shape}')
        mu = self.encFC1(x)
        logVar = self.encFC2(x)
        # print(f'mu: {mu.shape}')
        return mu, logVar

    def reparameterize(self, mu, logVar):

        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps
        std = torch.exp(logVar/2)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decoder(self, z):

        # z is fed back into a fully-connected layers and then into two transpose convolutional layers
        # The generated output is the same size of the original input
        # print(f'z:{z.shape}')
        x = F.relu(self.decFC1(z))
        # print(f'decFC1: {x.shape}')
        x = x.view(-1, self.num_hiddens, 84, 84)
        # print(f'x.view: {x.shape}')
        x = F.relu(self.decConv1(x))
        # print(f'x.decConv1: {x.shape}')
        # x = F.relu(self.decConv2(x))
        # x = F.relu(self.decConv3(x))
        # x = F.relu(self.decConv4(x))
        # print(f'x.decConv2: {x.shape}')
        x = self.Sigmoid(self.decConv2(x))
        # print(f'output: {x.shape}')
        return x

    def forward(self, x):

        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder
        # output, mu, and logVar are returned for loss computation
        mu, logVar = self.encoder(x)
        z = self.reparameterize(mu, logVar)
        out = self.decoder(z)
        return out, mu, logVar


class Encoder(nn.Module):
    def __init__(self, in_channels, num_hiddens):
        super(Encoder, self).__init__()

        self._conv_1 = nn.Conv2d(in_channels=in_channels,
                                 out_channels=num_hiddens // 4,
                                 kernel_size=(3, 3),
                                 stride=(2, 2), padding=(0, 0))
        # self._conv_2 = nn.Conv2d(in_channels=num_hiddens // 4,
        #                          out_channels=num_hiddens // 2,
        #                          kernel_size=(8, 8),
        #                          stride=(2, 2), padding=(0, 0))

        self._conv_3 = nn.Conv2d(in_channels=num_hiddens // 4,
                                 out_channels=num_hiddens,
                                 kernel_size=(3, 3),
                                 stride=(2, 2), padding=(0, 0))

        # self._conv_4 = nn.Conv2d(in_channels=num_hiddens // 2,
        #                          out_channels=num_hiddens,
        #                          kernel_size=(4, 4),
        #                          stride=(2, 2), padding=(0, 0))
        # self._residual_stack = ResidualStack(in_channels=num_hiddens,
        #                                      num_hiddens=num_hiddens,
        #                                      num_residual_layers=num_residual_layers,
        #                                      num_residual_hiddens=num_residual_hiddens)

        self.pooling = nn.MaxPool2d(2)

    def forward(self, inputs):

        # print("Working with new encoder")
        print(f"inputs:{inputs.shape}")
        x = self._conv_1(inputs)
        x = F.leaky_relu(x)
        # print(f"conv1: {x.shape}")

        # x = self.pooling(x)
        # print(f"pooling1: {x.shape}")

        # x = self._conv_2(x)
        # x = F.leaky_relu(x)
        # # print(f"conv2: {x.shape}")

        # x = self.pooling(x)
        # print(f"pooling2: {x.shape}")

        x = self._conv_3(x)
        x = F.leaky_relu(x)
        # print(f"conv3: {x.shape}")

        # x = self._conv_4(x)
        # x = F.relu(x)
        # # print(f"conv4: {x.shape}")

        return x


class Decoder(nn.Module):
    def __init__(self, embedding_dim, num_hiddens):
        super(Decoder, self).__init__()

        # self._conv_1 = nn.Conv2d(in_channels=embedding_dim,
        #                          out_channels=num_hiddens,
        #                          kernel_size=(4, 4),
        #                          stride=(2, 2), padding=(0, 0))

        # self._residual_stack = ResidualStack(in_channels=num_hiddens,
        #                                      num_hiddens=num_hiddens,
        #                                      num_residual_layers=num_residual_layers,
        #                                      num_residual_hiddens=num_residual_hiddens)

        # self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,
        #                                         out_channels=embedding_dim,
        #                                         kernel_size=(4, 4),
        #                                         stride=(2, 2), padding=(0, 0))

        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=embedding_dim,
                                                out_channels=num_hiddens,
                                                kernel_size=(4, 4),
                                                stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        self._conv_trans_3 = nn.ConvTranspose2d(in_channels=num_hiddens,
                                                out_channels=num_hiddens // 4,
                                                kernel_size=(3, 3),
                                                stride=(2, 2), padding=(0, 0),  output_padding=(0, 0))

        # self._conv_trans_4 = nn.ConvTranspose2d(in_channels=num_hiddens // 2,
        #                                         out_channels=num_hiddens // 4,
        #                                         kernel_size=(4, 4),
        #                                         stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        # self._conv_trans_5 = nn.ConvTranspose2d(in_channels=num_hiddens // 2,
        #                                         out_channels=num_hiddens // 4,
        #                                         kernel_size=(8, 8),
        #                                         stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        self._conv_trans_6 = nn.ConvTranspose2d(in_channels=num_hiddens // 4,
                                                out_channels=1,
                                                kernel_size=(3, 3),
                                                stride=(2, 2), padding=(0, 0), output_padding=(0, 0))

        self.dropout = nn.Dropout2d(p=0.2)

        self.transpooling = nn.MaxUnpool2d(2)

        self.Sigmoid = nn.Sigmoid()
        self.Tanh = nn.Tanh()

    def forward(self, inputs):

        #print("Working with new decoder")
        # print(f"inputs: {inputs.shape}")

        # x = self._conv_1(inputs)
        # x = F.relu(x)
        # print(f"conv1: {x.shape}")

        # x = self._conv_trans_1(x)
        # x = F.relu(x)
        # print(f"convtr1: {x.shape}")

        x = self._conv_trans_2(inputs)
        x = F.leaky_relu(x)
        # print(f"convtr2: {x.shape}")

        # x = self.transpooling(x)
        # print(f"transpooling: {x.shape}")

        x = self._conv_trans_3(x)
        x = F.leaky_relu(x)
        # print(f"convtr3: {x.shape}")

        # x = self.transpooling(x)
        # print(f"transpooling: {x.shape}")

        # x = self._conv_trans_4(x)
        # x = F.relu(x)
        # # print(f"convtr4: {x.shape}")

        # x = self._conv_trans_5(x)
        # x = F.relu(x)
        # # print(f"convtr5: {x.shape}")

        x = self._conv_trans_6(x)
        # x = F.leaky_relu(x)
        # print(f"convtr6: {x.shape}")

        x = self.Sigmoid(x)

        return x


class VectorQuantizer(nn.Module):

    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super(VectorQuantizer, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)
        self._commitment_cost = commitment_cost

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)

        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self._commitment_cost * e_latent_loss

        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class VectorQuantizerEMA(nn.Module):

    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):
        super(VectorQuantizerEMA, self).__init__()

        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)
        self._embedding.weight.data.normal_()
        self._commitment_cost = commitment_cost

        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))
        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))
        self._ema_w.data.normal_()

        self._decay = decay
        self._epsilon = epsilon

    def forward(self, inputs):
        # convert inputs from BCHW -> BHWC
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        # Flatten input
        flat_input = inputs.view(-1, self._embedding_dim)

        # Calculate distances
        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)
                     + torch.sum(self._embedding.weight ** 2, dim=1)
                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))

        # Encoding
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        # Quantize and unflatten
        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)

        # Use EMA to update the embedding vectors
        if self.training:
            self._ema_cluster_size = self._ema_cluster_size * self._decay + \
                                     (1 - self._decay) * torch.sum(encodings, 0)

            # Laplace smoothing of the cluster size
            n = torch.sum(self._ema_cluster_size.data)
            self._ema_cluster_size = (
                    (self._ema_cluster_size + self._epsilon)
                    / (n + self._num_embeddings * self._epsilon) * n)

            dw = torch.matmul(encodings.t(), flat_input)
            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)

            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))

        # Loss
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self._commitment_cost * e_latent_loss

        # Straight Through Estimator
        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))

        # convert quantized from BHWC -> BCHW
        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings


class Model(nn.Module):
    def __init__(self, num_hiddens, num_embeddings, embedding_dim, commitment_cost, decay=0):
        super(Model, self).__init__()

        self._encoder = Encoder(1, num_hiddens)
        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,
                                      out_channels=embedding_dim,
                                      padding=0,
                                      kernel_size=(4, 4),
                                      stride=(2, 2))
        if decay > 0.0:
            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,
                                              commitment_cost, decay)
        else:
            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,
                                           commitment_cost)
        self._decoder = Decoder(embedding_dim,
                                num_hiddens)

    def forward(self, x):
        z = self._encoder(x)
        z = self._pre_vq_conv(z)
        # print(f"Z shape: {z.shape}")

        loss, quantized, perplexity, _ = self._vq_vae(z)
        x_recon = self._decoder(quantized)

        return loss, x_recon, perplexity


class ClusterlingLayer(nn.Module):
    def __init__(self, in_features=10, out_features=10, alpha=1.0):
        super(ClusterlingLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.alpha = alpha
        self.weight = nn.Parameter(torch.Tensor(self.out_features, self.in_features))
        self.weight = nn.init.xavier_uniform_(self.weight)

    def forward(self, x):
        x = x.unsqueeze(1) - self.weight
        x = torch.mul(x, x)
        x = torch.sum(x, dim=2)
        x = 1.0 + (x / self.alpha)
        x = 1.0 / x
        x = x ** ((self.alpha +1.0) / 2.0)
        x = torch.t(x) / torch.sum(x, dim=1)
        x = torch.t(x)
        return x

    def extra_repr(self):
        return 'in_features={}, out_features={}, alpha={}'.format(
            self.in_features, self.out_features, self.alpha
        )

    def set_weight(self, tensor):
        self.weight = nn.Parameter(tensor)


class ModelName(str, Enum):

    """
    ModelName is an enumeration and it contains the model names as enumeration members. For more information
    visit the nex website https://docs.python.org/3/library/enum.html
    """
    ConvAE = "ConvAE"
