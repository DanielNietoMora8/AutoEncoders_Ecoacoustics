{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23048,
     "status": "ok",
     "timestamp": 1677472521991,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "KwQ8Zp3llyi-",
    "outputId": "efca6254-1663-423d-9181-d85a8ab2fdd4"
   },
   "outputs": [],
   "source": [
    "print(f\"ipython: {str(get_ipython())}\")\n",
    "from IPython.display import clear_output\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = 'media/mirp_ai/DATA1/Jaguas_2018'\n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7618,
     "status": "ok",
     "timestamp": 1677472529606,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "ZeYXtNUUhRWh",
    "outputId": "5c077add-05e0-4f3d-d436-eab3d0e8e810"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Jaguas_DataLoader import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11249,
     "status": "ok",
     "timestamp": 1677472540849,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "cMG_87FKb26h",
    "outputId": "df88d76b-c50b-4d9b-a711-667c5cb526d9"
   },
   "outputs": [],
   "source": [
    "dataset = SoundscapeData(root_path, audio_length=12, ext=\"wav\", win_length=1028)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.7), len(dataset) - round(len(dataset)*0.7)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"AE-Jaguas\",\n",
    "    \"audio_length\": dataset.audio_length,\n",
    "    \"batch_size\" : 14,\n",
    "    \"num_epochs\": 10,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"gamma_lr\" : 0.1,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\" : \"Audios Jaguas\",\n",
    "    \"architecture\": \"AE\",\n",
    "    \"win_length\" : dataset.win_length,\n",
    "    \"step_size\": 5,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = config[\"step_size\"], gamma = config[\"gamma_lr\"] )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"num_training_updates\"] = len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "160civndsHbJG1kDUZVvHIReG-ccyN830"
    },
    "id": "6YtKCwnNnDYP",
    "outputId": "d5292f0e-2945-4d1b-c15c-25211c656186",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Training = TrainModel(model)\n",
    "model, logs, run_name = Training.fordward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "torch.save(config,f'temporal/configs/config_{run_name}_day_{time.day}_hour_{time.hour}.pth')\n",
    "torch.save(dataset_test, f\"temporal/datasets/dataset_test_ae_jaguas_{time.day}_70%.pth\")\n",
    "torch.save(dataset_train, f\"temporal/datasets/dataset_train_ae_jaguas_{time.day}_70%.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5sjhlDScl3G"
   },
   "outputs": [],
   "source": [
    "dataset_test = torch.load(f'temporal/dataset_test_ae_jaguas')\n",
    "dataset_train = torch.load(f'temporal/dataset_train_ae_jaguas')\n",
    "model.load_state_dict(torch.load(f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBePtMUv5MD7"
   },
   "outputs": [],
   "source": [
    "# root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Porce_2019'\n",
    "\n",
    "\n",
    "# dataset = SoundscapeData(root_path, audio_length=12, ext=\"WAV\", win_length=1028)\n",
    "# dataset_train, dataset_test = random_split(dataset,\n",
    "#                                            [round(len(dataset)*0.7), len(dataset) - round(len(dataset)*0.7)], \n",
    "#                                            generator=torch.Generator().manual_seed(1024))\n",
    "# Dataset_train = DataLoader(dataset_train, batch_size=54, shuffle=True)\n",
    "# Dataset = DataLoader(dataset_test, batch_size=54, shuffle=True)\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=100)\n",
    "test_loader = DataLoader(dataset_test, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWREIMjnUf9x"
   },
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, label, loss= testing.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFQvOLxb7Ytc"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin[0:4],2)\n",
    "testing.plot_psd(wav_origin[10:14],2)\n",
    "#testing.plot_psd(wav_origin[18:22],2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons[0:4],2)\n",
    "testing.plot_psd(wav_recons[10:14],2)\n",
    "#testing.plot_psd(wav_recons[18:22],2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBEMoCDtUhDZ"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin,2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons,2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0E_jW1OyNlm"
   },
   "outputs": [],
   "source": [
    "plt.plot(wav_origin[12,0])\n",
    "plt.plot(wav_recons[12,0], color='red', alpha = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68wxgeupqxBv"
   },
   "outputs": [],
   "source": [
    "# encodings_size = [64,9,9]\n",
    "# plt.figure(figsize=(18, 18))\n",
    "# model.to(\"cpu\")\n",
    "# for i, spec in enumerate(kmeans.cluster_centers_):\n",
    "#     encodings = spec.reshape(encodings_size)\n",
    "#     encodings = torch.tensor(encodings).float()\n",
    "#     decodings = model.decoder(encodings).detach().numpy()\n",
    "#     plt.subplot(9, 9, i + 1)\n",
    "#     plt.imshow(decodings[0,:,:], cmap=\"inferno\", interpolation=\"nearest\",vmin=0, vmax=0.02)\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
