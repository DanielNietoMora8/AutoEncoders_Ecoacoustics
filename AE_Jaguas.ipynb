{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":16988,"status":"ok","timestamp":1650147664883,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"},"user_tz":300},"id":"rccfrca51UIR"},"outputs":[],"source":["from google.colab import drive, output\n","drive.mount('/content/drive')\n","import sys\n","%cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n","#sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n","#sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n","# sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n","sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n","sys.path.append('/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Acceso_Datos_Humboldt/ensayo.xlsx')\n","\n","%load_ext autoreload\n","%autoreload 1\n","!pip install torchaudio\n","!pip install optuna\n","!pip install wandb --upgrade\n","!wandb login\n","output.clear()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4317,"status":"ok","timestamp":1650147669195,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"},"user_tz":300},"id":"_EBNUB8-NwZo","outputId":"d12d0aa9-317a-478e-c718-989a50d6653e"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielnieto\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["from __future__ import print_function\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa\n","import librosa.display\n","\n","from six.moves import xrange\n","\n","#import umap\n","import wandb\n","import torch\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","\n","from scipy import signal\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","from torch.utils.data import random_split\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid\n","import torch.nn.functional as F\n","\n","#from ResidualStack import ResidualStack\n","#from Residual import Residual\n","\n","#from Project.Modules.Dataloader import EcoDataTesisReduced\n","from Jaguas_DataLoader import SoundscapeData\n","#from Models import VAE\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = xm.xla_device()\n","print(device)\n","\n","from datetime import timedelta\n","import wandb\n","from wandb import AlertLevel\n","\n","import optuna\n","\n","wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":514,"status":"ok","timestamp":1650147669706,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"},"user_tz":300},"id":"buqqA9PWp6BE"},"outputs":[],"source":["def display_images(model_outputs, epoch):\n","\n","    \"\"\"\n","    Plots the original image and reconstructed image by the machine learning model. This function receives\n","    a list with a length of 3 like [epochs, original images, reconstructed images].\n","\n","    Original images and reconstructed images have a shape of [num_images, num_channels, rows, columns].\n","\n","    :param model_outputs: Outputs of the machine learning model.\n","    :param epoch: Desire epoch to visualize.\n","    :type epoch: int\n","    :return: matplotlib plot\n","    \"\"\"\n","\n","    recon = model_outputs[epoch][2].to(\"cpu\").detach().numpy()\n","    img = model_outputs[epoch][1].to(\"cpu\").detach().numpy()\n","    for index in range(recon.shape[0]):\n","        ax1 = plt.subplot(2, recon.shape[0], index+1)\n","        ax2 = plt.subplot(2, recon.shape[0], index+recon.shape[0]+1)\n","\n","        librosa.display.specshow(librosa.power_to_db(recon[index, 0], ref=np.max),\n","                                 y_axis='mel', fmax=10000,\n","                                 x_axis='time', ax=ax1)\n","\n","        librosa.display.specshow(librosa.power_to_db(img[index, 0], ref=np.max),\n","                                 y_axis='mel', fmax=10000,\n","                                 x_axis='time', ax=ax2)\n","    plt.show()\n","\n","def plot_reconstructions(imgs_original, imgs_reconstruction, num_views: int=8):\n","    output = torch.cat((imgs_original[0:num_views], imgs_reconstruction[0:num_views]), 0)\n","    img_grid = make_grid(output, nrow=8, pad_value=20)\n","    fig, ax = plt.subplots(figsize=(20,5))\n","    ax.imshow(img_grid[1,:,:].cpu(), vmin=0, vmax=1)\n","    ax.axis(\"off\")\n","    plt.show()\n","    return fig\n","\n","def testModel(model, iterator):\n","    model.eval()\n","    (valid_originals, _,_) = next( iterator)\n","    valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1], \n","                                                  valid_originals.shape[2], valid_originals.shape[3]))\n","    valid_originals = torch.unsqueeze(valid_originals,1)\n","\n","    valid_originals = valid_originals.to(device)\n","\n","    vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n","    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n","    valid_reconstructions = model._decoder(valid_quantize)\n","    \n","    fig = plot_reconstructions(valid_originals, valid_reconstructions, 8)\n","\n","    recon_error = F.mse_loss(valid_originals, valid_reconstructions)\n","\n","    return fig, recon_error\n","\n","def define_encoder(trial):\n","    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n","    layers = []\n","\n","    in_features = 1\n","    for i in range(n_layers):\n","        #out_features = i+1**2\n","        out_features = trial.suggest_int(\"n_units_l{}\".format(i), (i+2)**3, 2*(i+2)**3, log=True)\n","        layers.append(nn.Conv2d(in_channels=in_features,\n","                                out_channels=out_features,\n","                                kernel_size=(4, 4),\n","                                padding=(0, 0),\n","                                stride=(1, 1)\n","                                )\n","                      )\n","        if i != n_layers - 1:\n","            layers.append(nn.ReLU())\n","        p = trial.suggest_float(\"dropout_{}\".format(i), 0.2, 0.5)\n","        #layers.append(nn.Dropout(p))\n","        encoder = layers\n","        in_features = out_features\n","\n","    return nn.Sequential(*encoder)\n","\n","\n","def define_decoder(trial):\n","\n","    decoder = []\n","    encoder = define_encoder(trial)\n","    decoder_counter = 0\n","    for i in range(len(encoder) - 1, -1, -1):\n","        decoder.append(encoder[i])\n","        if type(decoder[decoder_counter]) == torch.nn.modules.conv.Conv2d:\n","            decoder[decoder_counter] = nn.ConvTranspose2d(in_channels=encoder[i].out_channels,\n","                                                          out_channels=encoder[i].in_channels,\n","                                                          kernel_size=encoder[i].kernel_size,\n","                                                          stride=encoder[i].stride,\n","                                                          padding=encoder[i].padding)\n","        decoder_counter += 1\n","\n","    decoder.append(nn.ReLU())\n","    decoder.append(nn.Sigmoid())\n","    decoder = nn.Sequential(*decoder)\n","\n","    return decoder\n","\n","\n","class define_model(nn.Module):\n","    def __init__(self, trial):\n","        super().__init__()\n","        self.encoder = define_encoder(trial)\n","        self.decoder = define_decoder(trial)\n","        #self.autoencoder = define_autoencoder(trial)\n","\n","    def forward(self, x):\n","\n","        encoded = self.encoder(x)\n","        # print(f\"shape {encoded.shape}\" )\n","        decoder = self.decoder(encoded)\n","\n","        return decoder\n","\n","def train_model(model, optimizer, num_images, dataloader):\n","\n","    model.train()\n","    criterion = nn.MSELoss()\n","\n","    i = 0\n","    for (img, _, _) in dataloader:\n","        print(f\"i{i}\")\n","        img = img.reshape(-1, img.shape[2], img.shape[3])\n","        img = torch.unsqueeze(img, 1)\n","\n","        if i >= num_images:\n","            break\n","\n","        img_cuda = img.to(device)\n","        recon = model(img_cuda)\n","        loss = criterion(recon, img_cuda)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        i += 1\n","\n","        wandb.log({\"loss\":loss.item()}\n","                 )\n","    return loss, img, recon\n","\n","def objective(trial):\n","\n","    config = {\n","    \"batch_size\" : 20,\n","    \"num_epochs\": 15,\n","    # \"num_training_updates\" : len(dataset_train),\n","    \"num_hiddens\" : 64,\n","    \"embedding_dim\" : 128,\n","    \"zDim\" : 128,\n","    \"commitment_cost\" : 0.25,\n","    \"decay\" : 0.99,\n","    \"learning_rate\" : 1e-3,\n","    \"dataset\": \"Audios Jaguas\",\n","    \"architecture\": \"AE\",\n","              }\n","\n","    wandb.config = config\n","    num_images = 2000\n","\n","    print(\"-----------Running-----------\")\n","    root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n","    \n","    dataset = SoundscapeData(root_path, 1, \"wav\")\n","    dataset_train, dataset_test = random_split(dataset,\n","                                                [round(len(dataset)*0.10),\n","                                                 len(dataset) - round(len(dataset)*0.10)], \n","                                                generator=torch.Generator().manual_seed(1024))\n","    training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle = False)\n","    test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n","\n","    model = define_model(trial).to(device)\n","    optimizer = torch.optim.Adam(\n","        model.parameters(), trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True))\n","    \n","    \n","    wandb.init(project=\"AE-Jaguas\", config=config)\n","    wandb.watch(model, F.mse_loss, log=\"all\", log_freq=1)\n","\n","    outputs = []\n","\n","    for epoch in range(config['num_epochs']):\n","        loss, img, recon= train_model(model=model, optimizer=optimizer, num_images=num_images, dataloader=training_loader)\n","        recon = recon.to('cpu')\n","        print(f'Epoch:{epoch + 1}, Loss: {loss.item():.4f}')\n","        outputs.append((epoch, img, recon))\n","        fig = plot_reconstructions(img,recon)   \n","        # display_images(outputs, epoch)\n","        images = wandb.Image(fig, caption= f\"recon_error: {np.round(loss.item(),4)}\")\n","        wandb.log({\"examples\": images})\n","\n","    return loss"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":925},"id":"bap9XkrWtD_n","outputId":"b865f305-b184-4adb-d695-1fed68ba885c","executionInfo":{"status":"error","timestamp":1650147686997,"user_tz":300,"elapsed":17295,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-04-16 22:21:17,742]\u001b[0m A new study created in memory with name: no-name-6fb73c16-cdf0-4c15-a687-5a1fde73fe99\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["-----------Running-----------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.14"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/wandb/run-20220416_222122-ismdfbge</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/danielnieto/AE-Jaguas/runs/ismdfbge\" target=\"_blank\">fast-sky-19</a></strong> to <a href=\"https://wandb.ai/danielnieto/AE-Jaguas\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["i0\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[33m[W 2022-04-16 22:21:33,991]\u001b[0m Trial 0 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 15.90 GiB total capacity; 13.02 GiB already allocated; 1.88 GiB free; 13.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\u001b[0m\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-3-6666988fe913>\", line 187, in objective\n","    loss, img, recon= train_model(model=model, optimizer=optimizer, num_images=num_images, dataloader=training_loader)\n","  File \"<ipython-input-3-6666988fe913>\", line 135, in train_model\n","    recon = model(img_cuda)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1120, in _call_impl\n","    result = forward_call(*input, **kwargs)\n","  File \"<ipython-input-3-6666988fe913>\", line 116, in forward\n","    decoder = self.decoder(encoded)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\", line 98, in forward\n","    return F.relu(input, inplace=self.inplace)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1299, in relu\n","    result = torch.relu(input)\n","RuntimeError: CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 15.90 GiB total capacity; 13.02 GiB already allocated; 1.88 GiB free; 13.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d2650e79ee16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6666988fe913>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch:{epoch + 1}, Loss: {loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6666988fe913>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, num_images, dataloader)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mimg_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mrecon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6666988fe913>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# print(f\"shape {encoded.shape}\" )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 15.90 GiB total capacity; 13.02 GiB already allocated; 1.88 GiB free; 13.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["wandb.finish()\n","study = optuna.create_study(directions=[\"minimize\"])\n","study.optimize(objective, n_trials=5, timeout=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-q5q9_NeODaS","executionInfo":{"status":"aborted","timestamp":1650147686995,"user_tz":300,"elapsed":175,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Jaguas_2018'\n","\n","\n","dataset = SoundscapeData(root_path, 1, \"wav\")\n","dataset_train, dataset_test = random_split(dataset,\n","                                           [round(len(dataset)*0.0250), len(dataset) - round(len(dataset)*0.0250)], \n","                                           generator=torch.Generator().manual_seed(1024))\n","\n","config = {\n","    \"batch_size\" : 20,\n","    \"num_epochs\": 15,\n","    \"num_training_updates\" : len(dataset_train),\n","    \"num_hiddens\" : 64,\n","    \"embedding_dim\" : 128,\n","    \"zDim\" : 128,\n","    \"commitment_cost\" : 0.25,\n","    \"decay\" : 0.99,\n","    \"learning_rate\" : 1e-3,\n","    \"dataset\": \"Audios Jaguas\",\n","    \"architecture\": \"VQ-VAE\",\n","}\n","\n","training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n","test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n","\n","\n","model = VAE(num_hiddens=config[\"num_hiddens\"],\n","              zDim=config[\"zDim\"]).to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n","\n","\n","wandb.finish()\n","wandb.init(project=\"VAE-Jaguas\", config=config)\n","wandb.watch(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4zo23F2Li8a","executionInfo":{"status":"aborted","timestamp":1650147686996,"user_tz":300,"elapsed":6,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["def plot_reconstructions(imgs_original, imgs_reconstruction, num_views: int=8):\n","    output = torch.cat((imgs_original[0:num_views], imgs_reconstruction[0:num_views]), 0)\n","    img_grid = make_grid(output, nrow=8, pad_value=20)\n","    fig, ax = plt.subplots(figsize=(20,5))\n","    ax.imshow(img_grid[1,:,:].cpu(), vmin=0, vmax=1)\n","    ax.axis(\"off\")\n","    plt.show()\n","\n","def testModel(model, iterator):\n","    model.eval()\n","    (valid_originals, _,_) = next( iterator)\n","    valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1], \n","                                                  valid_originals.shape[2], valid_originals.shape[3]))\n","    valid_originals = torch.unsqueeze(valid_originals,1)\n","\n","    valid_originals = valid_originals.to(device)\n","\n","    mu, logvar = model.encoder(valid_originals)\n","    z = model.reparameterize(mu, logvar)\n","    valid_reconstructions = model.decoder(z)\n","    plot_reconstructions(valid_originals, valid_reconstructions)\n","    recon_error = F.mse_loss(valid_originals, valid_reconstructions)\n","    return fig, recon_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXVgqtR5NVPd","executionInfo":{"status":"aborted","timestamp":1650147686996,"user_tz":300,"elapsed":6,"user":{"displayName":"DANIEL ALEXIS NIETO MORA","userId":"09305600849699039845"}}},"outputs":[],"source":["model.train()\n","train_res_recon_error = []\n","train_res_perplexity = []\n","iterator = iter(test_loader)\n","wandb.watch(model, F.mse_loss, log=\"all\", log_freq=1)\n","error_files = []\n","\n","for epoch in range(config[\"num_epochs\"]):\n","  for i in xrange(config[\"num_training_updates\"]):\n","      model.train()\n","      try:\n","         (data, _,_) = next(iter(training_loader))\n","      except:\n","        error_files.append[i]\n","        continue\n","          \n","      data = torch.reshape(data, (data.shape[0] * data.shape[1], data.shape[2], data.shape[3]))\n","      data = torch.unsqueeze(data,1)\n","      data = data.to(device)\n","      print(data.shape)\n","      optimizer.zero_grad()\n","      data_recon, mu, logvar = model(data)\n","      print(data_recon.shape)\n","      \n","      recon_error = F.mse_loss(data_recon, data) #/ data_variance\n","      loss = recon_error\n","      loss.backward()\n","\n","      optimizer.step()\n","\n","      print(f'epoch: {epoch} of {config[\"num_epochs\"]} \\t iteration: {(i+1)} of {config[\"num_training_updates\"]} \\t loss: {np.round(loss.item(),4)}')\n","\n","      if (i+1) % 5 == 0:\n","        #torch.save(model.state_dict(),f'model_{epoch}_{i}.pkl')\n","        fig, test_error = testModel(model, iterator)\n","        images = wandb.Image(fig, caption= f\"recon_error: {np.round(test_error.item(),4)}\")\n","        wandb.log({\"examples\": images})\n","\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"AE_Jaguas.ipynb","provenance":[],"mount_file_id":"1tGUIBVbWKFSGUDrciztVrEPV343i03GI","authorship_tag":"ABX9TyObYX3DrKuhD7k8lim5rCFp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}