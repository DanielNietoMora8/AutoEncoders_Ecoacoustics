{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T01:25:58.412360850Z"
    },
    "id": "KwQ8Zp3llyi-",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipython: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f51a239a200>\n",
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "print(f\"ipython: {str(get_ipython())}\")\n",
    "from IPython.display import clear_output\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = 'media/mirp_ai/Seagate Desktop Drive/Jaguas_2018'\n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZeYXtNUUhRWh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdanielnieto\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\n"
     ]
    }
   ],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.98), len(dataset) - round(len(dataset)*0.98)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"AE-Jaguas\",\n",
    "    \"audio_length\": dataset.audio_length,\n",
    "    \"batch_size\" : 14,\n",
    "    \"num_epochs\": 10,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"gamma_lr\" : 0.1,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\" : \"Audios Jaguas\",\n",
    "    \"architecture\": \"AE\",\n",
    "    \"win_length\" : dataset.win_length,\n",
    "    \"step_size\": 5,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = config[\"step_size\"], gamma = config[\"gamma_lr\"] )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"num_training_updates\"] = len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6YtKCwnNnDYP",
    "outputId": "d5292f0e-2945-4d1b-c15c-25211c656186",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Jaguas/wandb/run-20231127_202751-12cr6y54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielnieto/AE-Jaguas/runs/12cr6y54' target=\"_blank\">serene-bird-11</a></strong> to <a href='https://wandb.ai/danielnieto/AE-Jaguas' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielnieto/AE-Jaguas' target=\"_blank\">https://wandb.ai/danielnieto/AE-Jaguas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielnieto/AE-Jaguas/runs/12cr6y54' target=\"_blank\">https://wandb.ai/danielnieto/AE-Jaguas/runs/12cr6y54</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m Training \u001B[38;5;241m=\u001B[39m TrainModel(model)\n\u001B[0;32m----> 2\u001B[0m model, logs, run_name \u001B[38;5;241m=\u001B[39m \u001B[43mTraining\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfordward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(),\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtemporal/models/model_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_day_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mday\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_hour_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mhour\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_final.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/AE_training_functions.py:200\u001B[0m, in \u001B[0;36mTrainModel.fordward\u001B[0;34m(self, training_loader, test_loader, config)\u001B[0m\n\u001B[1;32m    197\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    199\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 200\u001B[0m data_recon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmse_loss(data_recon, data)\n\u001B[1;32m    203\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/module.py:1208\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1205\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks)\n\u001B[1;32m   1206\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m-> 1208\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n",
      "File \u001B[0;32m~/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Models.py:100\u001B[0m, in \u001B[0;36mConvAE2.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03mMethod to compute an image output based on the performed model.\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03m:return: Reconstructed images\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m#print(f\"x_shape:{x.shape}\")\u001B[39;00m\n\u001B[0;32m--> 100\u001B[0m encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_shape: \u001B[39m\u001B[38;5;124m\"\u001B[39m, encoded\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    102\u001B[0m decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(encoded)\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/DANM/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "Training = TrainModel(model)\n",
    "model, logs, run_name = Training.fordward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "torch.save(config,f'temporal/configs/config_{run_name}_day_{time.day}_hour_{time.hour}.pth')\n",
    "# torch.save(dataset_test, f\"temporal/datasets/dataset_test_ae_jaguas_{time.day}_70%.pth\")\n",
    "# torch.save(dataset_train, f\"temporal/datasets/dataset_train_ae_jaguas_{time.day}_70%.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5sjhlDScl3G"
   },
   "outputs": [],
   "source": [
    "dataset_test = torch.load(f'temporal/dataset_test_ae_jaguas')\n",
    "dataset_train = torch.load(f'temporal/dataset_train_ae_jaguas')\n",
    "model.load_state_dict(torch.load(f'temporal/models/model_{run_name}_day_{time.day}_hour_{time.hour}_final.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBePtMUv5MD7"
   },
   "outputs": [],
   "source": [
    "# root_path = '/content/drive/Shareddrives/ConservacionBiologicaIA/Datos/Porce_2019'\n",
    "\n",
    "\n",
    "# dataset = SoundscapeData(root_path, audio_length=12, ext=\"WAV\", win_length=1028)\n",
    "# dataset_train, dataset_test = random_split(dataset,\n",
    "#                                            [round(len(dataset)*0.7), len(dataset) - round(len(dataset)*0.7)], \n",
    "#                                            generator=torch.Generator().manual_seed(1024))\n",
    "# Dataset_train = DataLoader(dataset_train, batch_size=54, shuffle=True)\n",
    "# Dataset = DataLoader(dataset_test, batch_size=54, shuffle=True)\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=100)\n",
    "test_loader = DataLoader(dataset_test, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWREIMjnUf9x"
   },
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, label, loss= testing.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFQvOLxb7Ytc"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin[0:4],2)\n",
    "testing.plot_psd(wav_origin[10:14],2)\n",
    "#testing.plot_psd(wav_origin[18:22],2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons[0:4],2)\n",
    "testing.plot_psd(wav_recons[10:14],2)\n",
    "#testing.plot_psd(wav_recons[18:22],2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBEMoCDtUhDZ"
   },
   "outputs": [],
   "source": [
    "wav_origin=testing.waveform_generator(spec=originals)\n",
    "print(wav_origin[0:1])\n",
    "wav_origin = np.interp(wav_origin, (wav_origin.min(), wav_origin.max()), (-1, +1))\n",
    "print(wav_origin[0:1])\n",
    "wav_recons=testing.waveform_generator(spec=reconstructions)\n",
    "wav_recons= np.interp(wav_recons, (wav_recons.min(), wav_recons.max()), (-1, +1))\n",
    "testing.plot_psd(wav_origin,2)\n",
    "plt.savefig(\"original_psd.pdf\")\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_recons,2)\n",
    "plt.savefig(\"recon_psd.pdf\")\n",
    "wav_diff = wav_origin-wav_recons\n",
    "plt.figure()\n",
    "testing.plot_psd(wav_diff,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0E_jW1OyNlm"
   },
   "outputs": [],
   "source": [
    "plt.plot(wav_origin[12,0])\n",
    "plt.plot(wav_recons[12,0], color='red', alpha = 0.4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
