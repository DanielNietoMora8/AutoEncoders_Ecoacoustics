{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36603,
     "status": "ok",
     "timestamp": 1678389399232,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "KwQ8Zp3llyi-",
    "is_executing": true,
    "outputId": "fd97b0c2-58fc-4afe-b939-c9890c909c2e"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "\n",
    "    !pip install joypy\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Jaguas\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\"\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3008,
     "status": "ok",
     "timestamp": 1678389402237,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "ZeYXtNUUhRWh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Normalizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMG_87FKb26h"
   },
   "outputs": [],
   "source": [
    "model_type = \"AE\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 11\n",
    "hour = 21\n",
    "month = 6\n",
    "folder = \"AE_No_rain_98\"\n",
    "date_format = f\"day_{day}_hour_{hour}\"\n",
    "\n",
    "model_name = f\"{root}/temporal/models/model_{model_type}_{identifier}_{date_format}_final.pth\"\n",
    "config = torch.load(f'temporal/configs/config_{model_type}_{identifier}_{date_format}.pth', map_location=torch.device('cpu'))\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "# dataset_test = torch.load(f'temporal/datasets/dataset_test_ae_jaguas_9_70%.pth')\n",
    "# dataset_train = torch.load(f'temporal/datasets/dataset_train_ae_jaguas_9_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(f\"temporal/Features/{folder}/AE_labels_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(f\"temporal/Features/{folder}/AE_features_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "y_path = torch.load(f\"temporal/Features/{folder}/AE_test_path_samples_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path_flat = [item for sublist in y_path for item in sublist]\n",
    "path_flat = np.asarray(path_flat)\n",
    "print(X.shape)\n",
    "y[\"recorder\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "Normalizer_ = Normalizer().fit(X)\n",
    "X_norm = Normalizer_.transform(X)\n",
    "PCA_ = PCA(n_components=60).fit(X_norm)\n",
    "X_PCA = PCA_.transform(X_norm)\n",
    "# X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_PCA)\n",
    "reducer = umap.UMAP(min_dist=0.9, n_components=60)\n",
    "X_UMAP = reducer.fit_transform(X_norm)\n",
    "X_batch = np.reshape(X_UMAP, (X_UMAP.shape[0]//5,5,X_UMAP.shape[1]))\n",
    "# X_UMAP_Norm = Normalizer().fit_transform(X_UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_audios = f\"{root}/Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\"\n",
    "root_recorders = f\"{root}/Complementary_Files/df_grabadoras_reg.csv\"\n",
    "root_clusters = f\"{root}/temporal/clusters\"\n",
    "root_ai = f\"{root}/Complementary_Files/Acoustic_Indices/AI_Jaguas.csv\"\n",
    "ecological_integrity = f\"{root}/Complementary_Files/Indice_Integridad_Ecologica.xlsx\"\n",
    "\n",
    "audios = pd.read_csv(root_audios, index_col=0)\n",
    "recorders = pd.read_csv(root_recorders)\n",
    "ei  = pd.read_excel(ecological_integrity)\n",
    "ai = pd.read_csv(root_ai)\n",
    "ai.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "ai.drop(columns=[\"Date\"], inplace=True)\n",
    "ai.dropna(inplace=True)\n",
    "\n",
    "ai.set_index(\"file\",inplace=True, drop=True)\n",
    "audios.set_index(\"Filename\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acoustic Indices Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ = np.asarray(y_path)\n",
    "y_2 = y_[:,0]\n",
    "for i in range(len(y_2)):\n",
    "    y_2[i] = y_2[i][0:-2] \n",
    "y_3 = list(y_2)\n",
    "\n",
    "X_ai = []\n",
    "remove = []\n",
    "for i in range(len(y_3)):\n",
    "    try:\n",
    "        X_ai.append(ai.loc[y_3[i]])\n",
    "#         print(i, \" \", np.asarray(np.min(X_ai)))\n",
    "    except:\n",
    "        remove.append(y_3[i])\n",
    "for i in range(len(remove)):\n",
    "    y_3.remove(remove[i])\n",
    "X_ai = np.asarray(X_ai)\n",
    "\n",
    "labels_ai = []\n",
    "# audios.set_index(\"Filename\", inplace=True)\n",
    "for i in range(len(y_3)):\n",
    "    labels_ai.append(audios.loc[y_3[i], \"Habitat\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ai, labels_ai, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(path_flat)//5)\n",
    "print(len(X_c)//5)\n",
    "\n",
    "len(labels_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 75921\n",
    "(X[i]==f[i//5][i%5]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders Features and Labels using independent segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# audios.set_index(\"Filename\", inplace=True)\n",
    "labels = []\n",
    "for i in range(len(path_flat)):\n",
    "    labels.append(audios.loc[path_flat[i][0:-2], \"Habitat\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders Features and Labels using 5 segments of the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = X_UMAP\n",
    "y_c = list(path_flat)\n",
    "\n",
    "# for i in range(len(y_c)):\n",
    "#     split = y_c[i].split(\"_\")[0:3]\n",
    "#     y_c[i] = f\"{split[0]}_{split[1]}_{split[2]}\"\n",
    "    \n",
    "for i in range(len(remove)):\n",
    "    for j in range(1,6):\n",
    "        y_c.remove(f\"{remove[i]}_{j}\")\n",
    "        X_c = np.delete(X_c, i, axis=0)\n",
    "X_batch = np.reshape(X_c, (X_c.shape[0]//5,5,X_c.shape[1]))\n",
    "y_c = np.asarray(y_c)\n",
    "y_path = np.reshape(y_c, (y_c.shape[0]//5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_batch, y_path, test_size=0.2,random_state=0)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "y_train = y_train.reshape(y_train.shape[0]*y_train.shape[1])\n",
    "y_test = y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
    "\n",
    "labels_train = []\n",
    "for i in range(len(y_train)):\n",
    "    labels_train.append(audios.loc[y_train[i][0:-2], \"Habitat\"])\n",
    "labels_test = []\n",
    "for i in range(len(y_test)):\n",
    "    labels_test.append(audios.loc[y_test[i][0:-2], \"Habitat\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders Features and Labels using voting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "clf_rf = RandomForestClassifier(max_depth=16, random_state=0, n_jobs=-1)\n",
    "clf_rf.fit(X_train, labels_train)\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(labels_test, y_pred_rf)\n",
    "f1_score = metrics.f1_score(labels_test, y_pred_rf, average=\"macro\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"f1:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf2 = np.asarray(y_pred_rf)\n",
    "y_pred_rf2 = np.reshape(y_pred_rf2,(y_pred_rf2.shape[0]//5,5))\n",
    "y_test2 = np.asarray(labels_test)\n",
    "y_test2 = np.reshape(y_test2,(y_test2.shape[0]//5,5))\n",
    "\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "labels_test2 = []\n",
    "labels_pred = []\n",
    "for i in range(len(y_pred_rf2)):\n",
    "    labels_pred.append(most_frequent(list(y_pred_rf2[i])))\n",
    "    labels_test2.append(most_frequent(list(y_test2[i])))\n",
    "accuracy = metrics.accuracy_score(labels_test2, labels_pred)\n",
    "f1_score = metrics.f1_score(labels_test2, labels_pred, average=\"macro\")\n",
    "print(accuracy)\n",
    "print(f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(100), random_state=0)\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = clf_mlp.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_mlp))\n",
    "print(\"f1:\",metrics.f1_score(y_test, y_pred_mlp, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = [] # [ai, norm, PCA, UMAP]\n",
    "# accuracies = []\n",
    "f1_scores.append(f1_score)\n",
    "accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"f1_scores_fair\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(f1_scores, fp)\n",
    "    \n",
    "with open(\"accuracies_fair\", \"wb\") as fp2:   #Pickling\n",
    "    pickle.dump(accuracy, fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (9,6))\n",
    "ax[0].bar([\"Acoustic\\nIndices\", \"Normalized\", \"PCA\", \"UMAP\"], f1_scores, color=\"royalblue\")\n",
    "ax[0].set_ylim(0.7,1)\n",
    "ax[0].set_title(\"F1 score\")\n",
    "ax[1].bar([f\"Acoustic\\nIndices\", \"Normalized\", \"PCA\", \"UMAP\"], accuracies, color = \"indianred\")\n",
    "ax[1].set_ylim(0.7,1)\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "plt.savefig(f\"{root}/temporal/classification_results/calssification_fair.pdf\", format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "15FiCdrQdIiLbRJdrnLgeGSWmPZR-eIZZ",
     "timestamp": 1668314866472
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
