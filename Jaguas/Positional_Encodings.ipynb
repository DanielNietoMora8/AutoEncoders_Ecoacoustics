{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b74df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipython: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f3eb88c9a20>\n",
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "print(f\"ipython: {str(get_ipython())}\")\n",
    "from IPython.display import clear_output\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = 'media/mirp_ai/Seagate Desktop Drive/Jaguas_2018'\n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17a2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: positional-encodings[pytorch] in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from positional-encodings[pytorch]) (1.23.1)\n",
      "Requirement already satisfied: torch in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from positional-encodings[pytorch]) (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from torch->positional-encodings[pytorch]) (4.3.0)\n",
      "Requirement already satisfied: wandb in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (0.15.4)\n",
      "Requirement already satisfied: PyYAML in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (4.21.9)\n",
      "Requirement already satisfied: pathtools in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: setproctitle in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/mirp_ai/anaconda3/envs/DANM/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielnieto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielnieto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision \n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import random_split\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "!pip install positional-encodings[pytorch]\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer, PositionalEncodingPermute2D\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "!pip install wandb --upgrade\n",
    "!wandb login\n",
    "import wandb\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "wandb.login()\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfe3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2d(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int=64, height: int = 9, width: int =9, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x, index: int, dropout: bool=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x.to(\"cuda\")\n",
    "        self.pe = self.pe.to(\"cuda\")\n",
    "#         print(x.shape, self.pe.shape)\n",
    "        x = x + self.pe[index]\n",
    "        if dropout:\n",
    "            x = self.dropout(x)\n",
    "        else:\n",
    "            x = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2da9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class posautoencoding_m1(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens: int = 64):\n",
    "        \"\"\"\n",
    "        Constructor of the convolutional autoencoder model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: To design the final architechture considering the spectrograms sizes.\n",
    "        # TODO: To correct the current sizes of the decoder.\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, num_hiddens // 8, kernel_size=8, stride=3, padding=0),  # N, 256, 127, 8004\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 8, num_hiddens // 4, kernel_size=8, stride=3, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 4, num_hiddens // 2, kernel_size=4, stride=3, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 2, num_hiddens, kernel_size=2, stride=2, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU()\n",
    "             )\n",
    "        self.decoder = nn.Sequential(  # This is like go in opposite direction respect the encoder\n",
    "            nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, kernel_size=2, stride=2, padding=0, output_padding=0),  # N, 32, 126,8000\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 2, num_hiddens // 4, kernel_size=4, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 4, num_hiddens // 8, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 8, 1, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.Sigmoid()\n",
    "\n",
    "            )\n",
    "            \n",
    "            \n",
    "    def forward(self, x, y, max_len=24):\n",
    "        \n",
    "        \"\"\"\n",
    "        Method to compute an image output based on the performed model.\n",
    "\n",
    "        :param x: Input spectrogram images as tensors.\n",
    "        :type x: torch.tensor\n",
    "        :return: Reconstructed images\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(f\"x_shape:{x.shape}\")\n",
    "        encoded = self.encoder(x)\n",
    "#         print(\"encoded: \", encoded.shape)\n",
    "        pos_encoder = PositionalEncoding2d(64, dropout = 0.1, max_len = 24).to(\"cuda\")\n",
    "        posencoding_2d = pos_encoder(encoded.permute(1,0,2,3), y)\n",
    "#         print(posencoding_2d)\n",
    "        posencoding_2d = posencoding_2d.permute(1,0,2,3)\n",
    "#         print(\"encoder_shape: \", encoded.shape)\n",
    "        decoded = self.decoder(posencoding_2d)\n",
    "#         print(\"decoder_shape: \",decoded.shape)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af697ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\n"
     ]
    }
   ],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.8), len(dataset) - round(len(dataset)*0.8)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"positionalAE-Jaguas_Hour\",\n",
    "    \"audio_length\": dataset.audio_length,\n",
    "    \"batch_size\" : 14,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"gamma_lr\" : 0.1,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\" : \"Audios Jaguas\",\n",
    "    \"architecture\": \"PositionalAE\",\n",
    "    \"win_length\" : dataset.win_length,\n",
    "    \"step_size\": 5,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = posautoencoding_m1(num_hiddens=config[\"num_hiddens\"]).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = config[\"step_size\"], gamma = config[\"gamma_lr\"] )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"num_training_updates\"] = len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f8fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "class TestModel:\n",
    "    \"\"\"\n",
    "            Class to test convolutional autoencoder models.\n",
    "\n",
    "            :param index: index indicates the number of data to return.\n",
    "            :returns:\n",
    "                :spec: Spectrogram of the indexed audios.\n",
    "                :type spec: torch.tensor\n",
    "                :record: Array of indexed audios in monophonic format.\n",
    "                :type record: numpy.array\n",
    "                :label: Dictionary of labels including recorder, hour, minute and second keys.\n",
    "                :type label: Dictionary\n",
    "                :path_index: File directory.\n",
    "                :type path index: String\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, iterator, num_views=8, device=\"cuda\"):\n",
    "\n",
    "        \"\"\"\n",
    "            :param model: Deep learning model.\n",
    "            :type model:\n",
    "            :param iterator: dataloader iterator.\n",
    "            :type iterator:\n",
    "            :param num_views: Specify the number of samples to visualize previously and after the reconstruction.\n",
    "            :type num_views:\n",
    "            :param device: Specify the device to do calculus.\n",
    "            :type device:\n",
    "            Todo:\n",
    "                Check the args type.\n",
    "            \"\"\"\n",
    "\n",
    "        self._model = model\n",
    "        self._iterator = iterator\n",
    "        self.num_views = num_views\n",
    "        self.device = device\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def save_waveform(self, waveform, directory=None):\n",
    "        scaled = np.int16(waveform[0, 0] / np.max(np.abs(waveform[0, 0])) * 32767)\n",
    "        write(directory + '.wav', 22050, scaled)\n",
    "\n",
    "    def plot_waveform(self, waveform, n_rows=4):\n",
    "        fig, axs = plt.subplots(n_rows, figsize=(10, 6), constrained_layout=True)\n",
    "        for i in range(n_rows):\n",
    "            axs[i].plot(waveform[i, 0])\n",
    "        plt.show()\n",
    "\n",
    "    def waveform_generator(self, spec, n_fft=1028, win_length=1028, audio_length=12, base_win=256):\n",
    "        spec = spec.cdouble()\n",
    "        spec = spec.to(\"cpu\")\n",
    "        # hop_length = int(np.round(base_win/win_length * 172.3))\n",
    "        transformation = audio_transform.InverseSpectrogram(n_fft=n_fft, win_length=win_length)\n",
    "        waveform = transformation(spec)\n",
    "        waveform = waveform.cpu().detach().numpy()\n",
    "        return waveform\n",
    "\n",
    "    def plot_psd(self, waveform, n_wavs=1):\n",
    "        for i in range(n_wavs):\n",
    "            plt.psd(waveform[i][0])\n",
    "            plt.xlabel(\"Frequency\", fontsize=16)\n",
    "            plt.ylabel(\"Power Spectral Density\", fontsize=16)\n",
    "            plt.xticks(fontsize=16)\n",
    "            plt.yticks(fontsize=16)\n",
    "\n",
    "    def plot_reconstructions(self, imgs_original, imgs_reconstruction):\n",
    "        output = torch.cat((imgs_original[0:self.num_views], imgs_reconstruction[0:self.num_views]), 0)\n",
    "        img_grid = make_grid(output, nrow=self.num_views, pad_value=20)\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        ax.imshow(librosa.power_to_db(img_grid[1, :, :].cpu()), origin=\"lower\")\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "    def reconstruct(self):\n",
    "        self._model.eval()\n",
    "        (valid_originals, _, label, path) = next(self._iterator)\n",
    "        valid_originals = torch.reshape(valid_originals, (valid_originals.shape[0] * valid_originals.shape[1]\n",
    "                                                          * valid_originals.shape[2], valid_originals.shape[3],\n",
    "                                                          valid_originals.shape[4]))\n",
    "        valid_originals = torch.unsqueeze(valid_originals, 1)\n",
    "        valid_originals = valid_originals.to(self.device)\n",
    "        \n",
    "        \n",
    "        valid_encodings = self._model.encoder(valid_originals)\n",
    "        pos_encoder = PositionalEncoding2d(64, dropout = 0.1, max_len = 4).to(\"cuda\")\n",
    "        posencoding_2d = pos_encoder(valid_encodings.permute(1,0,2,3), label[\"recorder\"].reshape(valid_originals.shape[0]))\n",
    "        valid_encodings = posencoding_2d.permute(1,0,2,3)\n",
    "\n",
    "        valid_reconstructions = self._model.decoder(valid_encodings)\n",
    "\n",
    "        valid_originals_nonorm = torch.expm1(valid_originals)\n",
    "        valid_reconstructions_nonorm = torch.expm1(valid_reconstructions)\n",
    "\n",
    "        BCE = F.mse_loss(valid_reconstructions, valid_originals)\n",
    "        loss = BCE\n",
    "\n",
    "        return valid_originals, valid_reconstructions, valid_encodings, label, loss, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f73fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import xrange\n",
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._model.to(self.device)\n",
    "        print(self.device)\n",
    "        \n",
    "    def wandb_init(self, config, keys=[\"batch_size\", \"num_hiddens\"]):\n",
    "        try:\n",
    "            run_name = str(config[\"architecture\"]+\"_\")\n",
    "            for key in keys:\n",
    "                if key in config.keys():\n",
    "                    run_name = run_name + key + \"_\" + str(config[key]) + \"_\"\n",
    "                else:\n",
    "                    run_name = run_name + str(key)\n",
    "\n",
    "            wandb.login()\n",
    "            wandb.finish()\n",
    "            wandb.init(project=config[\"project\"], config=config)\n",
    "            wandb.run.name = run_name\n",
    "            wandb.run.save()\n",
    "            wandb.watch(self._model, F.mse_loss, log=\"all\", log_freq=1)\n",
    "            is_wandb_enable = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            is_wandb_enable = False\n",
    "\n",
    "        return is_wandb_enable, run_name\n",
    "\n",
    "    def wandb_logging(self, dict):\n",
    "        for keys in dict:\n",
    "            wandb.log({keys: dict[keys]})\n",
    "            \n",
    "\n",
    "    def forward(self, training_loader, test_loader, config):\n",
    "        iterator = iter(test_loader)\n",
    "        wandb_enable, run_name = self.wandb_init(config)\n",
    "        optimizer = config[\"optimizer\"]\n",
    "        scheduler = config[\"scheduler\"]\n",
    "        \n",
    "        logs = []\n",
    "        # best_loss = 10000\n",
    "\n",
    "        for epoch in range(config[\"num_epochs\"]):\n",
    "            iterator_train = iter(training_loader)\n",
    "            for i in xrange(config[\"num_training_updates\"]):\n",
    "                self._model.train()\n",
    "                try:\n",
    "                    data, _, label, _ = next(iterator_train)\n",
    "                except Exception as e:\n",
    "                    print(\"error\")\n",
    "                    print(e)\n",
    "                    logs.append(e)\n",
    "                    continue\n",
    "\n",
    "                data = torch.reshape(data, (data.shape[0] * data.shape[1] * data.shape[2], data.shape[3], data.shape[4]))\n",
    "                print(data.shape)\n",
    "                data = torch.unsqueeze(data, 1)\n",
    "                print(data.shape)\n",
    "                data = data.to(\"cuda\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                data_recon = self._model(data, label[\"hour\"].reshape(data.shape[0]))\n",
    "\n",
    "                loss = F.mse_loss(data_recon, data)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                print(\n",
    "                    f'epoch: {epoch + 1} of {config[\"num_epochs\"]} \\t iteration: {(i + 1)} of {config[\"num_training_updates\"]} \\t loss: {np.round(loss.item(), 4)}')\n",
    "                dict = {\"loss\": loss.item()}\n",
    "                self.wandb_logging(dict)\n",
    "                \n",
    "                period = 200\n",
    "                if (i + 1) % period == 0:\n",
    "                    try:\n",
    "                        test_ = TestModel(self._model, iterator, 8, device=torch.device(\"cuda\"))\n",
    "                        # torch.save(model.state_dict(),f'model_{epoch}_{i}.pkl')\n",
    "                        originals, reconstructions, encodings, labels, test_error, path = test_.reconstruct()\n",
    "                        fig = test_.plot_reconstructions(originals, reconstructions)\n",
    "                        images = wandb.Image(fig, caption=f\"recon_error: {np.round(test_error.item(), 4)}\")\n",
    "                        self.wandb_logging({\"examples\": images, \"step\": (i + 1) // period})\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"error; {e}\")\n",
    "                        logs.append(e)\n",
    "                        continue\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "            scheduler.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            time = datetime.datetime.now()\n",
    "            torch.save(self._model.state_dict(), f'{run_name}_month_{time.month}_day_{time.day}_hour_{time.hour}_epoch_{epoch + 1}.pkl')\n",
    "            clear_output()\n",
    "            print(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "\n",
    "        wandb.finish()\n",
    "        return self._model, logs, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32261255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▄▆▂▃▃▂▅▃▂▄▅▃▂▁▃▃▃▂▂▄▂▁▁▅▂▁▆▂▅▅▁▁▂▂█▂▃▁▂▁</td></tr><tr><td>step</td><td>▁▃▆█▁▃▆█▁▃▆█▁▃▆█▁▃▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.10089</td></tr><tr><td>step</td><td>4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-monkey-6</strong> at: <a href='https://wandb.ai/danielnieto/positionalAE-Jaguas_Hour/runs/ougkfuz4' target=\"_blank\">https://wandb.ai/danielnieto/positionalAE-Jaguas_Hour/runs/ougkfuz4</a><br/>Synced 7 W&B file(s), 20 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230628_194625-ougkfuz4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Training = TrainModel(model=model.to(\"cuda\"))\n",
    "model, logs, run_name = Training.forward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'temporal/models/model_{run_name}_month_{time.month}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "torch.save(config,f'temporal/configs/config_{run_name}_day_{time.day}_hour_{time.hour}.pth')\n",
    "torch.save(dataset_test, f\"temporal/datasets/dataset_test_posae_jaguas_{time.day}_70%.pth\")\n",
    "torch.save(dataset_train, f\"temporal/datasets/dataset_train_posae_jaguas_{time.day}_70%.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterador = iter(training_loader)\n",
    "data, b, c, d = next(iterador)\n",
    "data = torch.reshape(data, (data.shape[0] * data.shape[1] *data.shape[2], data.shape[3], data.shape[4]))\n",
    "data = torch.unsqueeze(data, 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d08a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(dataset_train, batch_size=1)\n",
    "test_loader = DataLoader(dataset_test, batch_size=2)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f910cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, loss, label, path = testing.reconstruct()\n",
    "testing.plot_reconstructions(originals, reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, loss, label, path = testing.reconstruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341f7c9",
   "metadata": {},
   "source": [
    "# Featurer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f18a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"temporal/models/model_PositionalAE_hour_batch_size_14_num_hiddens_64__day_10_hour_21_final.pth\"\n",
    "config = torch.load(f'temporal/configs/config_PositionalAE_hour_batch_size_14_num_hiddens_64__day_10_hour_21.pth', map_location=torch.device('cpu'))\n",
    "model = posautoencoding_m1(num_hiddens=config[\"num_hiddens\"]).to(\"cuda\")\n",
    "dataset_test = torch.load(f'temporal/datasets/dataset_test_posae_jaguas_22_70%.pth')\n",
    "dataset_train = torch.load(f'temporal/datasets/dataset_train_posae_jaguas_22_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4282a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import xrange\n",
    "training_loader = DataLoader(dataset_train, batch_size=1)\n",
    "test_loader = DataLoader(dataset_test, batch_size=1)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n",
    "encodings_size = encodings[0].shape\n",
    "\n",
    "training_recorder_list = []\n",
    "training_hour_list = []\n",
    "training_minute_list = []\n",
    "delete_samples = []\n",
    "training_path_samples = []\n",
    "training_samples_list_torch = []\n",
    "for id in range(len(test_loader)):\n",
    "#     if (id+1)%5 == 0:\n",
    "#         print(\"finished\")\n",
    "#         break\n",
    "    if (id+1)% 500 == 0:\n",
    "        print(f\"id: {id + 1} of {len(dataset_test)}\")\n",
    "    try:\n",
    "        originals, reconstructions, encodings, label, loss, path = testing.reconstruct()\n",
    "    except:\n",
    "        print(f\"error id: {id}\")\n",
    "        delete_samples.append(id)\n",
    "        continue\n",
    "\n",
    "    encodings_size = encodings[0].shape\n",
    "    encodings = encodings.to(\"cuda\").detach()\n",
    "    encodings = encodings.reshape(encodings.shape[0],\n",
    "                                encodings.shape[1]*encodings.shape[2]*encodings.shape[3])\n",
    "    encoding = encodings.squeeze(dim=0)\n",
    "    training_samples_list_torch.append(encodings)\n",
    "    training_recorder_list.append(label[\"recorder\"].reshape(label[\"recorder\"].shape[0]*label[\"recorder\"].shape[1]))\n",
    "    training_hour_list.append(label[\"hour\"].reshape(label[\"hour\"].shape[0]*label[\"hour\"].shape[1]))\n",
    "    training_minute_list.append(label[\"minute\"].reshape(label[\"minute\"].shape[0]*label[\"minute\"].shape[1]))\n",
    "    \n",
    "    \n",
    "    path = np.asarray(path)\n",
    "    path = np.repeat(path, 5)\n",
    "    training_path_samples.append(path)\n",
    "\n",
    "training_recorder_list = torch.cat(training_recorder_list,dim=0)\n",
    "training_hour_list = torch.cat(training_hour_list,dim=0)\n",
    "training_minute_list = torch.cat(training_minute_list,dim=0)\n",
    "training_samples_list_torch = torch.cat(training_samples_list_torch, dim=0)\n",
    "\n",
    "torch.save(training_path_samples, \"Features/test_path_samples_posae_hour.pth\")\n",
    "torch.save(training_samples_list_torch, \"Features/test_samples_list_torch_70%_posae_hour.pth\")\n",
    "torch.save(training_recorder_list, \"Features/test_recorder_list_70%_posae_hour.pth\")\n",
    "torch.save(training_hour_list, \"Features/test_hour_list_70%_posae_hour.pth\")\n",
    "torch.save(training_minute_list, \"Features/test_minute_list_70%_posae_hour.pth\")\n",
    "training_labels_list = {\"recorder\": training_recorder_list, \"hour\": training_hour_list, \"minute\": training_minute_list}\n",
    "torch.save(training_labels_list, \"Features/test_labels_list_70%_posae_hour.pth\")\n",
    "torch.save(delete_samples, \"Features/test_corrupted_samples_list_70%_posae_hour.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6451b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestModel(model, iterador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcdc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = PositionalEncoding2d() \n",
    "x = posenc(encodes.permute(1,0,2,3), c[\"recorder\"].reshape(14*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = PositionalEncoding2d() \n",
    "encodes = model.encoder(data.to(\"cuda\"))\n",
    "print(encodes.shape)\n",
    "posencoding_2d = posenc(data, c[\"recorder\"].reshape(14*5))\n",
    "decoded = model.decoder(posencoding_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f84db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
