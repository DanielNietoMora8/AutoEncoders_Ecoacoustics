{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b74df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ipython: {str(get_ipython())}\")\n",
    "from IPython.display import clear_output\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root_path = 'media/mirp_ai/Seagate Desktop Drive/Jaguas_2018'\n",
    "else:\n",
    "    print(\"Running in personal pc\")\n",
    "    root_path = 'ConservacionBiologicaIA/Datos/Jaguas_2018'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision \n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import random_split\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from PosAE_training_functions import posautoencoding_m1,TestModel, TrainModel\n",
    "!pip install positional-encodings[pytorch]\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer, PositionalEncodingPermute2D\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "!pip install wandb --upgrade\n",
    "!wandb login\n",
    "import wandb\n",
    "from IPython.display import clear_output\n",
    "import datetime\n",
    "wandb.login()\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742351f",
   "metadata": {},
   "source": [
    "class PositionalEncoding2d(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int=64, height: int = 9, width: int =9, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x, index: int, dropout: bool=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x.to(\"cuda\")\n",
    "        self.pe = self.pe.to(\"cuda\")\n",
    "#         print(x.shape, self.pe.shape)\n",
    "        x = x + self.pe[index]\n",
    "        if dropout:\n",
    "            x = self.dropout(x)\n",
    "        else:\n",
    "            x = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e278b",
   "metadata": {},
   "source": [
    "class posautoencoding_m1(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Convolutional autoencoder made to reconstruct the audios spectrograms generated by the EcoDataTesis dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens: int = 64):\n",
    "        \"\"\"\n",
    "        Constructor of the convolutional autoencoder model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: To design the final architechture considering the spectrograms sizes.\n",
    "        # TODO: To correct the current sizes of the decoder.\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, num_hiddens // 8, kernel_size=8, stride=3, padding=0),  # N, 256, 127, 8004\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 8, num_hiddens // 4, kernel_size=8, stride=3, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 4, num_hiddens // 2, kernel_size=4, stride=3, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_hiddens // 2, num_hiddens, kernel_size=2, stride=2, padding=0),  # N, 512, 125,969\n",
    "            nn.ReLU()\n",
    "             )\n",
    "        self.decoder = nn.Sequential(  # This is like go in opposite direction respect the encoder\n",
    "            nn.ConvTranspose2d(num_hiddens, num_hiddens // 2, kernel_size=2, stride=2, padding=0, output_padding=0),  # N, 32, 126,8000\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 2, num_hiddens // 4, kernel_size=4, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 4, num_hiddens // 8, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_hiddens // 8, 1, kernel_size=8, stride=3, padding=0, output_padding=0),  # N, 32, 127,64248\n",
    "            nn.Sigmoid()\n",
    "\n",
    "            )\n",
    "            \n",
    "            \n",
    "    def forward(self, x, y, max_len=24):\n",
    "        \n",
    "        \"\"\"\n",
    "        Method to compute an image output based on the performed model.\n",
    "\n",
    "        :param x: Input spectrogram images as tensors.\n",
    "        :type x: torch.tensor\n",
    "        :return: Reconstructed images\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(f\"x_shape:{x.shape}\")\n",
    "        encoded = self.encoder(x)\n",
    "#         print(\"encoded: \", encoded.shape)\n",
    "        pos_encoder = PositionalEncoding2d(64, dropout = 0.1, max_len = 24).to(\"cuda\")\n",
    "        posencoding_2d = pos_encoder(encoded.permute(1,0,2,3), y)\n",
    "#         print(posencoding_2d)\n",
    "        posencoding_2d = posencoding_2d.permute(1,0,2,3)\n",
    "#         print(\"encoder_shape: \", encoded.shape)\n",
    "        decoded = self.decoder(posencoding_2d)\n",
    "#         print(\"decoder_shape: \",decoded.shape)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "dataset_train, dataset_test = random_split(dataset,\n",
    "                                           [round(len(dataset)*0.8), len(dataset) - round(len(dataset)*0.8)], \n",
    "                                           generator=torch.Generator().manual_seed(1024))\n",
    "\n",
    "config = {\n",
    "    \"project\" : \"positionalAE-Jaguas_Hour\",\n",
    "    \"audio_length\": dataset.audio_length,\n",
    "    \"batch_size\" : 14,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_hiddens\" : 64,\n",
    "    \"gamma_lr\" : 0.1,\n",
    "    \"learning_rate\" : 1e-3,\n",
    "    \"dataset\" : \"Audios Jaguas\",\n",
    "    \"architecture\": \"PositionalAE\",\n",
    "    \"win_length\" : dataset.win_length,\n",
    "    \"step_size\": 5,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, batch_size=config[\"batch_size\"])\n",
    "\n",
    "model = posautoencoding_m1(num_hiddens=config[\"num_hiddens\"]).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], amsgrad=False)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = config[\"step_size\"], gamma = config[\"gamma_lr\"] )\n",
    "\n",
    "config[\"optimizer\"] = optimizer\n",
    "config[\"scheduler\"] = scheduler\n",
    "config[\"num_training_updates\"] = len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32261255",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training = TrainModel(model=model.to(\"cuda\"))\n",
    "model, logs, run_name = Training.forward(training_loader, test_loader, config)\n",
    "time = datetime.datetime.now()\n",
    "torch.save(model.state_dict(),f'temporal/models/model_{run_name}_month_{time.month}_day_{time.day}_hour_{time.hour}_final.pth')\n",
    "torch.save(config,f'temporal/configs/config_{run_name}_day_{time.day}_hour_{time.hour}.pth')\n",
    "torch.save(dataset_test, f\"temporal/datasets/dataset_test_posae_jaguas_{time.day}_70%.pth\")\n",
    "torch.save(dataset_train, f\"temporal/datasets/dataset_train_posae_jaguas_{time.day}_70%.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterador = iter(training_loader)\n",
    "data, b, c, d = next(iterador)\n",
    "data = torch.reshape(data, (data.shape[0] * data.shape[1] *data.shape[2], data.shape[3], data.shape[4]))\n",
    "data = torch.unsqueeze(data, 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d08a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(dataset_train, batch_size=1)\n",
    "test_loader = DataLoader(dataset_test, batch_size=2)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ed0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "originals, reconstructions, encodings, loss, label, path = testing.reconstruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341f7c9",
   "metadata": {},
   "source": [
    "# Featurer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f18a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"temporal/models/model_PositionalAE_hour_batch_size_14_num_hiddens_64__day_10_hour_21_final.pth\"\n",
    "config = torch.load(f'temporal/configs/config_PositionalAE_hour_batch_size_14_num_hiddens_64__day_10_hour_21.pth', map_location=torch.device('cpu'))\n",
    "model = posautoencoding_m1(num_hiddens=config[\"num_hiddens\"]).to(\"cuda\")\n",
    "dataset_test = torch.load(f'temporal/datasets/dataset_test_posae_jaguas_22_70%.pth')\n",
    "dataset_train = torch.load(f'temporal/datasets/dataset_train_posae_jaguas_22_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4282a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import xrange\n",
    "training_loader = DataLoader(dataset_train, batch_size=1)\n",
    "test_loader = DataLoader(dataset_test, batch_size=1)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))\n",
    "encodings_size = encodings[0].shape\n",
    "\n",
    "training_recorder_list = []\n",
    "training_hour_list = []\n",
    "training_minute_list = []\n",
    "delete_samples = []\n",
    "training_path_samples = []\n",
    "training_samples_list_torch = []\n",
    "for id in range(len(test_loader)):\n",
    "#     if (id+1)%5 == 0:\n",
    "#         print(\"finished\")\n",
    "#         break\n",
    "    if (id+1)% 500 == 0:\n",
    "        print(f\"id: {id + 1} of {len(dataset_test)}\")\n",
    "    try:\n",
    "        originals, reconstructions, encodings, label, loss, path = testing.reconstruct()\n",
    "    except:\n",
    "        print(f\"error id: {id}\")\n",
    "        delete_samples.append(id)\n",
    "        continue\n",
    "\n",
    "    encodings_size = encodings[0].shape\n",
    "    encodings = encodings.to(\"cuda\").detach()\n",
    "    encodings = encodings.reshape(encodings.shape[0],\n",
    "                                encodings.shape[1]*encodings.shape[2]*encodings.shape[3])\n",
    "    encoding = encodings.squeeze(dim=0)\n",
    "    training_samples_list_torch.append(encodings)\n",
    "    training_recorder_list.append(label[\"recorder\"].reshape(label[\"recorder\"].shape[0]*label[\"recorder\"].shape[1]))\n",
    "    training_hour_list.append(label[\"hour\"].reshape(label[\"hour\"].shape[0]*label[\"hour\"].shape[1]))\n",
    "    training_minute_list.append(label[\"minute\"].reshape(label[\"minute\"].shape[0]*label[\"minute\"].shape[1]))\n",
    "    \n",
    "    \n",
    "    path = np.asarray(path)\n",
    "    path = np.repeat(path, 5)\n",
    "    training_path_samples.append(path)\n",
    "\n",
    "training_recorder_list = torch.cat(training_recorder_list,dim=0)\n",
    "training_hour_list = torch.cat(training_hour_list,dim=0)\n",
    "training_minute_list = torch.cat(training_minute_list,dim=0)\n",
    "training_samples_list_torch = torch.cat(training_samples_list_torch, dim=0)\n",
    "\n",
    "torch.save(training_path_samples, \"Features/test_path_samples_posae_hour.pth\")\n",
    "torch.save(training_samples_list_torch, \"Features/test_samples_list_torch_70%_posae_hour.pth\")\n",
    "torch.save(training_recorder_list, \"Features/test_recorder_list_70%_posae_hour.pth\")\n",
    "torch.save(training_hour_list, \"Features/test_hour_list_70%_posae_hour.pth\")\n",
    "torch.save(training_minute_list, \"Features/test_minute_list_70%_posae_hour.pth\")\n",
    "training_labels_list = {\"recorder\": training_recorder_list, \"hour\": training_hour_list, \"minute\": training_minute_list}\n",
    "torch.save(training_labels_list, \"Features/test_labels_list_70%_posae_hour.pth\")\n",
    "torch.save(delete_samples, \"Features/test_corrupted_samples_list_70%_posae_hour.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6451b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestModel(model, iterador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcdc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = PositionalEncoding2d() \n",
    "x = posenc(encodes.permute(1,0,2,3), c[\"recorder\"].reshape(14*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posenc = PositionalEncoding2d() \n",
    "encodes = model.encoder(data.to(\"cuda\"))\n",
    "print(encodes.shape)\n",
    "posencoding_2d = posenc(data, c[\"recorder\"].reshape(14*5))\n",
    "decoded = model.decoder(posencoding_2d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
