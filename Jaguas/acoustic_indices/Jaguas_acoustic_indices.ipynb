{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffa7768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T14:46:59.382906591Z",
     "start_time": "2023-07-17T14:46:59.377104032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/temporal')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Extra_and_Unused')\n",
    "    root = \"/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project\"\n",
    "else:\n",
    "    print(\"Running local\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52205a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "# from AE_training_functions import TestModel, TrainModel # For AE\n",
    "# from PosAE_training_functions import posautoencoding_m1 as AE, TestModel, TrainModel # For PosAE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "print(device)\n",
    "\n",
    "from datetime import timedelta\n",
    "import joypy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632f8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Single methods\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e728fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = os.listdir(path_to_dir)\n",
    "    return sorted([ filename for filename in filenames if filename.endswith( suffix ) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34390dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_path = f\"{root}/Jaguas/Complementary_Files/Acoustic_Indices\"\n",
    "files = find_csv_filenames(AI_path, suffix=\".csv\")\n",
    "files = files[1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b40134b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G04_m_AIs.csv\n",
      "G06_m_AIs.csv\n",
      "G07_m_AIs.csv\n",
      "G08_m_AIs.csv\n",
      "G09_m_AIs.csv\n",
      "G13_m_AIs.csv\n",
      "G15_m_AIs.csv\n",
      "G17_m_AIs.csv\n",
      "G19_m_AIs.csv\n",
      "G23_m_AIs.csv\n",
      "G24_m_AIs.csv\n",
      "G25_m_AIs.csv\n",
      "G27_m_AIs.csv\n",
      "G28_m_AIs.csv\n",
      "G29_m_AIs.csv\n",
      "G34_m_AIs.csv\n",
      "G35_m_AIs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G36_m_AIs.csv\n",
      "G37_m_AIs.csv\n",
      "G40_m_AIs.csv\n",
      "G41_m_AIs.csv\n",
      "G46_m_AIs.csv\n",
      "G47_m_AIs.csv\n",
      "G49_m_AIs.csv\n",
      "G50_m_AIs.csv\n",
      "G51_m_AIs.csv\n",
      "G52_m_AIs.csv\n",
      "G54_m_AIs.csv\n",
      "G57_m_AIs.csv\n",
      "G58_m_AIs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n",
      "/tmp/ipykernel_775009/194938973.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  AI = AI.append(df_file)\n"
     ]
    }
   ],
   "source": [
    "AI = pd.read_csv(f\"{AI_path}/{files[0]}\", index_col=0)\n",
    "total = len(AI)\n",
    "for file in files[1::]:\n",
    "    df_file = pd.read_csv(f\"{AI_path}/{file}\")\n",
    "    total += len(df_file)\n",
    "    print(file)\n",
    "    AI = AI.append(df_file)\n",
    "for i in range(len(AI)):\n",
    "    AI.iloc[i,0] = AI.iloc[i,0].split(\"/\")[-1]\n",
    "AI.reset_index(inplace=True)\n",
    "AI.drop(columns=\"index\", inplace=True)\n",
    "AI.to_csv(f\"{root}/Jaguas/Complementary_Files/Acoustic_Indices/AI_Jaguas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be21c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f\"{root}/Jaguas/Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\"\n",
    "base = pd.read_csv(base_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f58867",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted = []\n",
    "for i in range(len(base)):\n",
    "    file = base.iloc[i,4]\n",
    "    columns = np.arange(2,62)\n",
    "    try:\n",
    "        base[AI.columns[columns]] = AI[AI[\"file\"] == file].iloc[0,columns]\n",
    "    except:\n",
    "        corrupted.append(i)\n",
    "        print(file)\n",
    "base = base.drop(corrupted)\n",
    "# base.to_csv(f\"{root}/Jaguas/Complementary_Files/Acoustic_Indices/AI_Jaguas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "base2 = base.drop(columns=[\"Time\", \"Recording\", \"Habitat\",\n",
    "                           \"Recorder\", \"Date\", \"Municipio\",\n",
    "                           \"Vereda\", \"Latitud\", \"Rain_occurrence\",\n",
    "                           \"Rain_Intesity\", \"Intensity_Category\",\n",
    "                           \"Longitud\", \"Elevacion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02572948",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330abcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_array = base2.drop(columns=\"Filename\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea53831",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f70af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(base[\"Time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_recorder = list(base[\"Recorder\"])\n",
    "y_hour = list(base[\"Time\"])\n",
    "for i in range(len(y_hour)):\n",
    "    y_hour[i] = int(y_hour[i].split(\":\")[0])\n",
    "    y_recorder[i] = int(y_recorder[i][1:3])\n",
    "print(len(y_hour), len(y_recorder))\n",
    "y = {\"hour\": y_hour, \"recorder\": y_recorder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f74664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "import librosa\n",
    "def plot_silhouette( X, cluster_labels, n_clusters, silhouette_avg, method, extra=\"\", save=False):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0,\n",
    "                    0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Silhouette_plot_{n_clusters}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "\n",
    "def plot_centroids(cluster_centers, model, method, extra=\"\", save=True):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    testing._model.to(\"cpu\")\n",
    "    for i, spec in enumerate(cluster_centers):\n",
    "        encodings = spec.reshape(64,9,9)\n",
    "        encodings = torch.tensor(encodings).float()\n",
    "        decodings = testing._model.decoder(encodings).detach().numpy()\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.imshow(librosa.power_to_db(decodings[0, :, :]), origin=\"lower\", cmap=\"viridis\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    n_cluster = len(cluster_centers)\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Centroids_plot_{n_cluster}_{extra}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "def num_rows_cols(num_elements):\n",
    "    num_rows = int(math.sqrt(num_elements))\n",
    "    num_cols = (num_elements + num_rows - 1) // num_rows\n",
    "    return (num_rows, num_cols)\n",
    "\n",
    "def get_row_col(pos, cols):\n",
    "    row = pos // cols\n",
    "    col = pos % cols\n",
    "    return row, col\n",
    "\n",
    "class Clustering_Results:\n",
    "    def __init__(self, model, y, y_label=\"hour\", hist_library=\"plt\"):\n",
    "        self._labels_cluster = None\n",
    "        self._hist_library = hist_library\n",
    "        self._label = y_label\n",
    "        self._model = model\n",
    "        self._n_clusters = len(set(model.labels_))\n",
    "        self.y = y\n",
    "        self._y = np.asarray(y[self._label])\n",
    "        self._n_labels = set(self._y)\n",
    "\n",
    "\n",
    "    def one_cluster_eval(self, cluster):\n",
    "        index = np.where(self._model.labels_ == cluster)\n",
    "        index = list(index[0])\n",
    "        self._labels_cluster = self._y[index]\n",
    "        return self._labels_cluster\n",
    "    \n",
    "    def tagger(self, samples):\n",
    "        labels = []\n",
    "        labels_all_clusters = []\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "            labels.append(samples[index])\n",
    "        return labels\n",
    "\n",
    "    def joyplot(self):\n",
    "        labels_all_clusters = []\n",
    "        size_x = 8\n",
    "        size_y = 6\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "                labels_cluster.append(y_aux[i][index])\n",
    "            df = pd.DataFrame({'recorder':labels_cluster[0], \"hour\":labels_cluster[1]})\n",
    "            joypy.joyplot(df, by=\"hour\", column=\"recorder\", range_style='own', \n",
    "                            grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                            title=f\"Cluster {cluster} \\nLabels distribution along recorders using recorders as rows\",\n",
    "                            colormap=cm.autumn_r, fade=False)\n",
    "            joypy.joyplot(df, by=\"recorder\", column=\"hour\", range_style='own', \n",
    "                                grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                                title=f\"Cluster {cluster} \\nLabels distribution along recorders using hours as rows\",\n",
    "                                colormap=cm.autumn_r)\n",
    "            labels_all_clusters.append(index)\n",
    "            plt.show()\n",
    "#             print(len(labels_cluster))\n",
    "#             print(labels_cluster[1].shape)\n",
    "#             print(labels_cluster[0:10])\n",
    "#             print(index[0:20])\n",
    "        \n",
    "        return labels_all_clusters\n",
    "            \n",
    "\n",
    "    def histograms(self):\n",
    "        bins = list(self._n_labels)\n",
    "        num_rows, num_cols = num_rows_cols(self._n_clusters)\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 14))\n",
    "        if self._n_clusters <= 3:\n",
    "                axes = np.expand_dims(axes,0)\n",
    "                fig.set_figheight(6)\n",
    "                fig.set_figwidth(12)\n",
    "                if self._n_clusters == 1:\n",
    "                    axes = np.expand_dims(axes,0)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        for hist in range(self._n_clusters):\n",
    "            aux = self.one_cluster_eval(hist)\n",
    "            ax_0, ax_1 = get_row_col(hist, num_cols)\n",
    "            if self._hist_library == \"plt\":\n",
    "                axes[ax_0][ax_1].hist(aux, histtype=\"bar\",\n",
    "                                      color=\"paleturquoise\", cumulative=False,\n",
    "                                      edgecolor='black', \n",
    "                                      linewidth=1.2, bins=bins, stacked=False)\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)\n",
    "            elif self._hist_library == \"sns\":\n",
    "                sns.distplot(aux,bins=np.arange(aux.min(), aux.max()+1),\n",
    "                             hist_kws=dict(edgecolor=\"black\", linewidth=1), \n",
    "                             ax=axes[ax_0, ax_1])\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)              \n",
    "            else:\n",
    "                raise Exception(f\"Library {self._hist_library} unused\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = AI_array\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_n = X\n",
    "X_scaled = scaler.transform(X)\n",
    "# X_PCA = PCA(n_components=180).fit_transform(X_scaled)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 60\n",
    "X_PCA_ = PCA(n_components=n_components).fit(X)\n",
    "X_PCA = X_PCA_.transform(X)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0, n_components), X_PCA_.explained_variance_ratio_.cumsum())\n",
    "plt.title(\"Explained variance by components\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cummulative Explained Variance \")\n",
    "plt.axhline(y=0.9, c=\"r\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr = np.random.rand(2068, 60)\n",
    "Xr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273257f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "Kmeans = KMeans(n_clusters=10, random_state=0).fit(X_TSNE)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"recorder\", hist_library=\"plt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0579fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_Results.histograms()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
