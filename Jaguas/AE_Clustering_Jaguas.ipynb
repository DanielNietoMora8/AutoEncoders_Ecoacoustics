{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T22:56:31.022132977Z",
     "start_time": "2023-06-01T22:56:30.995396663Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36603,
     "status": "ok",
     "timestamp": 1678389399232,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "KwQ8Zp3llyi-",
    "outputId": "fd97b0c2-58fc-4afe-b939-c9890c909c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "    !pip install -U ipykernel\n",
    "    !pip install joypy\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Jaguas\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\"\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3008,
     "status": "ok",
     "timestamp": 1678389402237,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "ZeYXtNUUhRWh"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "\n",
    "#from ResidualStack import ResidualStack\n",
    "#from Residual import Residual\n",
    "\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = xm.xla_device()\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cMG_87FKb26h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84820, 5184])\n"
     ]
    }
   ],
   "source": [
    "model_type = \"AE\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 27\n",
    "hour = 19\n",
    "month = 6\n",
    "folder = \"AE_No_rain\"\n",
    "date_format = f\"day_{day}_hour_{hour}\"\n",
    "\n",
    "model_name = f\"{root}/temporal/models/model_{model_type}_{identifier}_{date_format}_final.pth\"\n",
    "config = torch.load(f'temporal/configs/config_{model_type}_{identifier}_{date_format}.pth', map_location=torch.device('cpu'))\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "# dataset_test = torch.load(f'temporal/datasets/dataset_test_ae_jaguas_9_70%.pth')\n",
    "# dataset_train = torch.load(f'temporal/datasets/dataset_train_ae_jaguas_9_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(f\"temporal/Features/{folder}/{model_type}_labels_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(f\"temporal/Features/{folder}/{model_type}_features_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path = torch.load(f\"temporal/Features/{folder}/{model_type}_test_path_samples_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path_flat = [item for sublist in path for item in sublist]\n",
    "path_flat = np.asarray(path_flat)\n",
    "print(X.shape)\n",
    "y[\"recorder\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_n = X\n",
    "X_scaled = scaler.transform(X)\n",
    "X_norm = Normalizer().fit_transform(X_scaled)\n",
    "X_PCA = PCA(n_components=80).fit_transform(X_scaled)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Models import PositionalEncoding2d\n",
    "from Models import posautoencoding_m1\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = \"PositionalAE_hour\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 29\n",
    "hour = 9\n",
    "month = 6\n",
    "folder = \"PosAE_Hour_No_rain\"\n",
    "date_format = f\"month_{month}_day_{day}_hour_{hour}\"\n",
    "model_name = f\"{root}/temporal/models/model_{model}_{identifier}_{date_format}_final.pth\"\n",
    "config = torch.load(f'{root}/temporal/configs/config_{model}_{identifier}_{date_format}.pth', map_location=torch.device('cpu'))\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "# dataset_test = torch.load(f'temporal/datasets/dataset_test_ae_jaguas_9_70%.pth')\n",
    "# dataset_train = torch.load(f'temporal/datasets/dataset_train_ae_jaguas_9_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(f\"temporal/Features/{folder}/PosAE_labels_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(f\"temporal/Features/{folder}/PosAE_features_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path = torch.load(f\"temporal/Features/{folder}/PosAE_test_path_samples_{date_format}_No_rain_Audios_Jaguas.pth\")\n",
    "path_flat = [item for sublist in path for item in sublist]\n",
    "path_flat = np.asarray(path_flat)\n",
    "print(X.shape)\n",
    "y[\"recorder\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_n = X\n",
    "X_scaled = scaler.transform(X)\n",
    "X_norm = Normalizer().fit_transform(X_scaled)\n",
    "X_PCA = PCA(n_components=180).fit_transform(X_scaled)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\n"
     ]
    }
   ],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "test_loader = DataLoader(dataset, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsTUXBDido7n"
   },
   "source": [
    "## **Batch Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUNZFPQNmhEj"
   },
   "outputs": [],
   "source": [
    "clusters = [3, 4, 5, 6, 7, 8, 9, 10, 15, 18, 20, 25, 30 ]\n",
    "for n_cluster in clusters:\n",
    "    print(f\"current cluster: {n_cluster}\")\n",
    "    iterator_Dataset = iter(training_loader)\n",
    "    testing = TestModel(model, iterator_Dataset, device=torch.device(\"cuda\"))\n",
    "    Clustering = AE_Clustering(testing, training_loader, n_clusters=n_cluster)\n",
    "    kmeans = Clustering.fordward()\n",
    "    # Clustering.plot_centroids()\n",
    "    output.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs-21vU9cBBT"
   },
   "outputs": [],
   "source": [
    "Clustering.plot_centroids()\n",
    "plt.savefig(f\"Clustering_Results/Figures/Clustering_centroids_TSNE_7.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJb0miHmCfGq"
   },
   "source": [
    "## **Traditional clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hXhfknkgChqU"
   },
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Single methods\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0av3CxthbxXg"
   },
   "outputs": [],
   "source": [
    "from logging import raiseExceptions\n",
    "import librosa\n",
    "def plot_silhouette( X, cluster_labels, n_clusters, silhouette_avg, method, extra=\"\", save=False):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0,\n",
    "                    0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Silhouette_plot_{n_clusters}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "\n",
    "def plot_centroids(cluster_centers, model, method, extra=\"\", save=True):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    testing._model.to(\"cpu\")\n",
    "    for i, spec in enumerate(cluster_centers):\n",
    "        encodings = spec.reshape(64,9,9)\n",
    "        encodings = torch.tensor(encodings).float()\n",
    "        decodings = testing._model.decoder(encodings).detach().numpy()\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.imshow(librosa.power_to_db(decodings[0, :, :]), origin=\"lower\", cmap=\"viridis\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    n_cluster = len(cluster_centers)\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Centroids_plot_{n_cluster}_{extra}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "def num_rows_cols(num_elements):\n",
    "    num_rows = int(math.sqrt(num_elements))\n",
    "    num_cols = (num_elements + num_rows - 1) // num_rows\n",
    "    return (num_rows, num_cols)\n",
    "\n",
    "def get_row_col(pos, cols):\n",
    "    row = pos // cols\n",
    "    col = pos % cols\n",
    "    return row, col\n",
    "\n",
    "class Clustering_Results:\n",
    "    def __init__(self, model, y, y_label=\"hour\", hist_library=\"plt\"):\n",
    "        self._labels_cluster = None\n",
    "        self._n_labels = None\n",
    "        self._label = y_label\n",
    "        self._model = model\n",
    "        self._n_clusters = len(set(model.labels_))\n",
    "        self.y = y\n",
    "        self._y = self.converter(y[self._label])\n",
    "        self._n_labels = set(self._y)\n",
    "\n",
    "    def converter(self, var):\n",
    "        aux = []\n",
    "        for i in range(len(var)):\n",
    "            aux.append(var[i].item())\n",
    "        return np.array(aux)\n",
    "\n",
    "    def one_cluster_eval(self, cluster):\n",
    "        index = np.where(self._model.labels_ == cluster)\n",
    "        index = list(index[0])\n",
    "        self._labels_cluster = self._y[index]\n",
    "        return self._labels_cluster\n",
    "    \n",
    "    def tagger(self, samples):\n",
    "        labels = []\n",
    "        labels_all_clusters = []\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "            labels.append(samples[index])\n",
    "        return labels\n",
    "\n",
    "    def joyplot(self):\n",
    "        labels_all_clusters = []\n",
    "        size_x = 8\n",
    "        size_y = 6\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "                labels_cluster.append(y_aux[i][index])\n",
    "            df = pd.DataFrame({'recorder':labels_cluster[0], \"hour\":labels_cluster[1]})\n",
    "            joypy.joyplot(df, by=\"hour\", column=\"recorder\", range_style='own', \n",
    "                            grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                            title=f\"Cluster {cluster} \\nLabels distribution along recorders using recorders as rows\",\n",
    "                            colormap=cm.autumn_r, fade=False)\n",
    "            joypy.joyplot(df, by=\"recorder\", column=\"hour\", range_style='own', \n",
    "                                grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                                title=f\"Cluster {cluster} \\nLabels distribution along recorders using hours as rows\",\n",
    "                                colormap=cm.autumn_r)\n",
    "            labels_all_clusters.append(index)\n",
    "            plt.show()\n",
    "#             print(len(labels_cluster))\n",
    "#             print(labels_cluster[1].shape)\n",
    "#             print(labels_cluster[0:10])\n",
    "#             print(index[0:20])\n",
    "        \n",
    "        return labels_all_clusters\n",
    "            \n",
    "\n",
    "    def histograms(self, hist_library=\"plt\"):\n",
    "        bins = list(self._n_labels)\n",
    "        print(bins)\n",
    "        num_rows, num_cols = num_rows_cols(self._n_clusters)\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 14))\n",
    "        if self._n_clusters <= 3:\n",
    "                axes = np.expand_dims(axes,0)\n",
    "                fig.set_figheight(6)\n",
    "                fig.set_figwidth(12)\n",
    "                if self._n_clusters == 1:\n",
    "                    axes = np.expand_dims(axes,0)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        for hist in range(self._n_clusters):\n",
    "            aux = self.one_cluster_eval(hist)\n",
    "            ax_0, ax_1 = get_row_col(hist, num_cols)\n",
    "            if hist_library == \"plt\":\n",
    "                axes[ax_0][ax_1].hist(aux, histtype=\"bar\",\n",
    "                                      color=\"paleturquoise\", cumulative=False,\n",
    "                                      edgecolor='black', \n",
    "                                      linewidth=1.2, bins=bins, stacked=False)\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)\n",
    "            elif hist_library == \"sns\":\n",
    "                sns.distplot(aux,bins=np.arange(aux.min(), aux.max()+1),\n",
    "                             hist_kws=dict(edgecolor=\"black\", linewidth=1), \n",
    "                             ax=axes[ax_0, ax_1])\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)              \n",
    "            else:\n",
    "                raise Exception(f\"Library {self._hist_library} unused\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRNaEDUzv8gM"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Kmeans = KMeans(n_clusters=10, random_state=0).fit(X_norm)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\")\n",
    "sns.displot(Kmeans.labels_)\n",
    "Kmeans_Results.histograms(hist_library=\"sns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Kmeans_Results.tagger(path_flat)\n",
    "torch.save(f,f\"temporal/clusters/kmeans_scaled_clustering_labels_PosAE_rainless_hour_{len(set(Kmeans.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leido= torch.load(f\"Features/clustering_labels_clusters_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(leido[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy767EXfzIpC"
   },
   "outputs": [],
   "source": [
    "list(Kmeans_Results.one_cluster_eval(0)).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysv9uNl1qsez"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "Kmeans_Results.histograms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D62Apeoj9jn8"
   },
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68Jz_szzOux7"
   },
   "outputs": [],
   "source": [
    "clusters = [3, 5, 10, 20]\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "silhouette_score_Kmeans = []\n",
    "# X_normalized = \n",
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_scaled)\n",
    "    silhouette_score_Kmeans.append(metrics.silhouette_score(X_scaled, Kmeans.labels_))\n",
    "    plot_silhouette(X_scaled, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id], \"Kmeans\")\n",
    "    cluster_centers = scaler.inverse_transform(Kmeans.cluster_centers_)\n",
    "    plot_centroids(cluster_centers, testing, \"Kmeans\")\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "    Kmeans_Results.joyplot()\n",
    "with open(f\"temporal/clustering_results/Kmeans/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1qEhX43O0Ux"
   },
   "source": [
    "## Kmeans TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXOW32KJxVdP"
   },
   "outputs": [],
   "source": [
    "clusters = [3, 4, 5, 10, 15]\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "silhouette_score_Kmeans = []\n",
    "# X_normalized = \n",
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_TSNE)\n",
    "    silhouette_score_Kmeans.append(metrics.silhouette_score(X_TSNE, Kmeans.labels_))\n",
    "    plot_silhouette(X_TSNE, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id], \"Kmeans_TSNE\")\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "    Kmeans_Results.joyplot()\n",
    "    # cluster_centers = scaler.inverse_transform(Kmeans.cluster_centers_)\n",
    "    # plot_centroids(cluster_centers, testing, \"Kmeans_TSNE\")\n",
    "with open(f\"Clustering_Results/Kmeans_TSNE/Results/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPytBMrZU_hE"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "clusters = [2, 3, 4, 5, 10, 15, 20, 30]\n",
    "\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "silhouette_score_Kmeans = []\n",
    "# X_normalized = \n",
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_PCA)\n",
    "    silhouette_score_Kmeans.append(metrics.silhouette_score(X_PCA, Kmeans.labels_))\n",
    "    plot_silhouette(X_PCA, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans[id], \"Kmeans_PCA\")\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "    Kmeans_Results.joyplot()\n",
    "    # cluster_centers = scaler.inverse_transform(Kmeans.cluster_centers_)\n",
    "    # plot_centroids(cluster_centers, testing, \"Kmeans_TSNE\")\n",
    "with open(f\"Clustering_Results/Kmeans_PCA/Results/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C57GimN29u3n"
   },
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwYhNZXa2cyi"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "classes = []\n",
    "silhouette_score_DBSCAN = []\n",
    "i = 1\n",
    "EPS = [0.01, 0.1, 0.5, 1, 10, 20, 36, 50, 80, 100]\n",
    "for x in range(0, 9, 1):\n",
    "    eps = EPS[x]\n",
    "    db = DBSCAN(eps=eps, min_samples=4).fit(X_scaled)\n",
    "    # silhouette_score_DBSCAN.append(metrics.silhouette_score(X_PCA, db.labels_))\n",
    "    # plot_silhouette(X_PCA, db.labels_, 15, silhouette_score_DBSCAN[i-1], \"DBSCAN_PCA\")\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X_scaled[:,0], X_scaled[:,1], hue=[\"cluster-{}\".format(x) for x in labels]) \n",
    "    i += 1\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    Kmeans_Results = Clustering_Results(db, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "with open(f\"Clustering_Results/DBSCAN/Results/silhouette_n-clusters_{n_clusters_}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_DBSCAN, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GlIoJ1HqkLG"
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.8, min_samples=100).fit(X_norm)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "metrics.silhouette_score(X_scaled, db.labels_)\n",
    "sns.displot(db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLrRXdWWbE43"
   },
   "source": [
    "## DBSCAN PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyY4oXN3bCdT"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "classes = []\n",
    "silhouette_score_DBSCAN = []\n",
    "i = 1\n",
    "EPS = [0.01, 0.1, 0.5, 1, 10, 20, 36, 50, 80, 100]\n",
    "for eps in EPS:\n",
    "    db = DBSCAN(eps=eps, min_samples=4).fit(X_PCA)\n",
    "    # silhouette_score_DBSCAN.append(metrics.silhouette_score(X_PCA, db.labels_))\n",
    "    # plot_silhouette(X_PCA, db.labels_, 15, silhouette_score_DBSCAN[i-1], \"DBSCAN_PCA\")\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X_PCA[:,0], X_PCA[:,1], hue=[\"cluster-{}\".format(x) for x in labels]) \n",
    "    i += 1\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "    Kmeans_Results = Clustering_Results(db, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "with open(f\"Clustering_Results/DBSCAN_PCA/Results/silhouette_n-clusters_{n_clusters_}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_DBSCAN, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlaAjxquNufA"
   },
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "from kneed import KneeLocator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=10)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "distances = np.sort(distances[:,9], axis=0)\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(f\"The knee is located at: {distances[knee.knee]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR2vGl44c3Gn"
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=12,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=1,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPJwwqh9bx0v"
   },
   "outputs": [],
   "source": [
    "linkage = \"complete\"\n",
    "connectivity = False\n",
    "model = AgglomerativeClustering(\n",
    "                linkage=linkage, connectivity=connectivity, \n",
    "                n_clusters=3)\n",
    "model.fit(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npHhaDOWAsn9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "# X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "#                   [4, 2], [4, 4], [4, 0]])\n",
    "clustering = AgglomerativeClustering().fit(X_scaled)\n",
    "n_clusters_ = len(set(clustering.labels_))\n",
    "print(n_clusters_)\n",
    "print(list(clustering.labels_).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aBvupxmyTaR"
   },
   "outputs": [],
   "source": [
    "print(list(clustering.labels_).count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvOgNz3Q9SPL"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 1500\n",
    "np.random.seed(0)\n",
    "t = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\n",
    "x = t * np.cos(t)\n",
    "y = t * np.sin(t)\n",
    "\n",
    "\n",
    "X = np.concatenate((x, y))\n",
    "X += 0.7 * np.random.randn(2, n_samples)\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW6AhHrW9UE5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as st\n",
    "def get_best_distribution(data):\n",
    "    dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "    dist_results = []\n",
    "    params = {}\n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(st, dist_name)\n",
    "        param = dist.fit(data)\n",
    "\n",
    "        params[dist_name] = param\n",
    "        # Applying the Kolmogorov-Smirnov test\n",
    "        D, p = st.kstest(data, dist_name, args=param)\n",
    "        print(\"p value for \"+dist_name+\" = \"+str(p))\n",
    "        dist_results.append((dist_name, p))\n",
    "\n",
    "    # select the best fitted distribution\n",
    "    best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n",
    "    # store the name of the best fit and its p value\n",
    "\n",
    "    print(\"Best fitting distribution: \"+str(best_dist))\n",
    "    print(\"Best p value: \"+ str(best_p))\n",
    "    print(\"Parameters for the best fit: \"+ str(params[best_dist]))\n",
    "\n",
    "    return best_dist, best_p, params[best_dist]\n",
    "\n",
    "get_best_distribution(X[540,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGSGe4sM94nY"
   },
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "15FiCdrQdIiLbRJdrnLgeGSWmPZR-eIZZ",
     "timestamp": 1668314866472
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
