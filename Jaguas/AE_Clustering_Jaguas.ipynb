{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36603,
     "status": "ok",
     "timestamp": 1678389399232,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "KwQ8Zp3llyi-",
    "is_executing": true,
    "outputId": "fd97b0c2-58fc-4afe-b939-c9890c909c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on MIRP\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    import sys\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    !pip install torchaudio\n",
    "    !pip install wandb --upgrade\n",
    "    # !wandb login\n",
    "    !pip install umap-learn\n",
    "    !pip install umap-learn[plot]\n",
    "    !pip install holoviews\n",
    "    !pip install -U ipykernel\n",
    "    !pip install joypy\n",
    "    # !pip install umap-learn\n",
    "    output.clear()\n",
    "    print(\"Running on colab\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload 1\n",
    "    %cd '/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project'\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/data')\n",
    "    #sys.path.append('/content/drive/MyDrive/Deep Learning/AutoEncoders/Project/VQVAE_Working/models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Dataloader')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Models')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Modules')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Results')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Results/Figures')\n",
    "    sys.path.append('/content/drive/MyDrive/PhD_Thesis_Experiments/DeepLearning/AutoEncoders/Project/Clustering_Result')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    print(\"Running on MIRP\")\n",
    "    root = \"/home/mirp_ai/Documents/Daniel_Nieto/PhD/AutoEncoders_Ecoacoustics/Jaguas\"\n",
    "    root_path = \"media/mirp_ai/Seagate Desktop Drive/Jaguas_2018\"\n",
    "else:\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    print(\"Running local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3008,
     "status": "ok",
     "timestamp": 1678389402237,
     "user": {
      "displayName": "DANIEL ALEXIS NIETO MORA",
      "userId": "09305600849699039845"
     },
     "user_tz": 300
    },
    "id": "ZeYXtNUUhRWh"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import joypy\n",
    "\n",
    "from scipy import signal\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Normalizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchaudio.transforms as audio_transform\n",
    "\n",
    "from Jaguas_DataLoader_rainless import SoundscapeData\n",
    "from Models import ConvAE as AE\n",
    "from AE_training_functions import TestModel, TrainModel\n",
    "from AE_Clustering import AE_Clustering \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "from wandb import AlertLevel\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import pickle as pkl\n",
    "\n",
    "import random\n",
    "def _set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    " \n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "_set_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cMG_87FKb26h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([84820, 5184])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "model_type = \"AE\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 11\n",
    "hour = 21\n",
    "month = 6\n",
    "folder = \"AE_No_rain_98\"\n",
    "date_format = f\"day_{day}_hour_{hour}\"\n",
    "\n",
    "model_name = f\"{root}/temporal/models/model_{model_type}_{identifier}_{date_format}_final.pth\"\n",
    "config = torch.load(f'temporal/configs/config_{model_type}_{identifier}_{date_format}.pth', map_location=torch.device('cpu'))\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(f\"temporal/Features/{folder}/AE_labels_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(f\"temporal/Features/{folder}/AE_features_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path = torch.load(f\"temporal/Features/{folder}/AE_test_path_samples_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path_flat = [item for sublist in path for item in sublist]\n",
    "path_flat = np.asarray(path_flat)\n",
    "print(X.shape)\n",
    "y[\"recorder\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "Normalizer_ = Normalizer().fit(X)\n",
    "X_norm = Normalizer_.transform(X)\n",
    "PCA_ = PCA(n_components=30).fit(X_norm)\n",
    "X_PCA = PCA_.transform(X_norm)\n",
    "# X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_PCA)\n",
    "reducer = umap.UMAP(min_dist=0.9, n_components=2)\n",
    "X_UMAP = reducer.fit_transform(X_norm)\n",
    "# X_UMAP_Norm = Normalizer().fit_transform(X_UMAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# POSAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Models import PositionalEncoding2d\n",
    "from Models import posautoencoding_m1\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = \"PositionalAE_hour\"\n",
    "identifier = \"batch_size_14_num_hiddens_64_\"\n",
    "day = 29\n",
    "hour = 9\n",
    "month = 6\n",
    "folder = \"PosAE_Hour_No_rain\"\n",
    "date_format = f\"month_{month}_day_{day}_hour_{hour}\"\n",
    "model_name = f\"{root}/temporal/models/model_{model}_{identifier}_{date_format}_final.pth\"\n",
    "config = torch.load(f'{root}/temporal/configs/config_{model}_{identifier}_{date_format}.pth', map_location=torch.device('cpu'))\n",
    "model = AE(num_hiddens=config[\"num_hiddens\"]).to(device)\n",
    "# dataset_test = torch.load(f'temporal/datasets/dataset_test_ae_jaguas_9_70%.pth')\n",
    "# dataset_train = torch.load(f'temporal/datasets/dataset_train_ae_jaguas_9_70%.pth')\n",
    "model.load_state_dict(torch.load(f'{model_name}', map_location=torch.device('cpu')))\n",
    "\n",
    "y = torch.load(f\"temporal/Features/{folder}/PosAE_labels_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "X = torch.load(f\"temporal/Features/{folder}/PosAE_features_{date_format}_No_rain_Audios_Jaguas.pth\",  map_location=torch.device('cpu'))\n",
    "path = torch.load(f\"temporal/Features/{folder}/PosAE_test_path_samples_{date_format}_No_rain_Audios_Jaguas.pth\")\n",
    "path_flat = [item for sublist in path for item in sublist]\n",
    "path_flat = np.asarray(path_flat)\n",
    "print(X.shape)\n",
    "y[\"recorder\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_norm = Normalizer().fit_transform(X)\n",
    "PCA_ = PCA(n_components=30).fit(X_norm)\n",
    "X_PCA = PCA_.transform(X_norm)\n",
    "X_TSNE = TSNE(n_components=2, learning_rate=\"auto\", init='random', random_state=0).fit_transform(X_PCA)\n",
    "reducer = umap.UMAP(min_dist=0.9)\n",
    "X_UMAP = reducer.fit_transform(X)\n",
    "X_UMAP_Norm = Normalizer().fit_transform(X_UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"Intensity_Category\": \"No_rain\"}\n",
    "dataset = SoundscapeData(root_path, dataframe_path=\"Complementary_Files/Audios_Jaguas/Audios_Jaguas.csv\",\n",
    "                         audio_length=12, ext=\"wav\",\n",
    "                         win_length=1028, filters=filters)\n",
    "test_loader = DataLoader(dataset, batch_size=100)\n",
    "iterator = iter(test_loader)\n",
    "testing = TestModel(model, iterator, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsTUXBDido7n"
   },
   "source": [
    "## **Batch Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUNZFPQNmhEj"
   },
   "outputs": [],
   "source": [
    "clusters = [3, 4, 5, 6, 7, 8, 9, 10, 15, 18, 20, 25, 30 ]\n",
    "for n_cluster in clusters:\n",
    "    print(f\"current cluster: {n_cluster}\")\n",
    "    iterator_Dataset = iter(training_loader)\n",
    "    testing = TestModel(model, iterator_Dataset, device=torch.device(\"cuda\"))\n",
    "    Clustering = AE_Clustering(testing, training_loader, n_clusters=n_cluster)\n",
    "    kmeans = Clustering.fordward()\n",
    "    # Clustering.plot_centroids()\n",
    "    output.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs-21vU9cBBT"
   },
   "outputs": [],
   "source": [
    "Clustering.plot_centroids()\n",
    "plt.savefig(f\"Clustering_Results/Figures/Clustering_centroids_TSNE_7.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJb0miHmCfGq"
   },
   "source": [
    "## **Traditional clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXhfknkgChqU"
   },
   "outputs": [],
   "source": [
    "# Batch methods\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "\n",
    "# Single methods\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from Modules.Clustering_Utils import plot_silhouette\n",
    "from Modules.Clustering_Utils import plot_centroids\n",
    "from Modules.Clustering_Utils import Clustering_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRNaEDUzv8gM"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Kmeans = KMeans(n_clusters=3, random_state=0).fit(X_norm)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\")\n",
    "sns.displot(Kmeans.labels_)\n",
    "# Kmeans_Results.histograms(hist_library=\"sns\")\n",
    "# Kmeans_Results.histograms(method=\"Kmeans\", root=\"temporal/clustering_results/Kmeans/\")\n",
    "Kmeans_Results.joyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Kmeans_Results.tagger(path_flat)\n",
    "torch.save(f,f\"temporal/clusters/kmeans_UMAP_clustering_labels_PosAE_rainless_{len(set(Kmeans.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leido= torch.load(f\"Features/clustering_labels_clusters_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(leido[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysv9uNl1qsez"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "Kmeans_Results.histograms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D62Apeoj9jn8"
   },
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68Jz_szzOux7"
   },
   "outputs": [],
   "source": [
    "clusters = [3]\n",
    "# mean = X.mean()\n",
    "# std = X.std()\n",
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_norm)\n",
    "    silhouette_score_Kmeans = metrics.silhouette_score(X_norm, Kmeans.labels_)\n",
    "    plot_silhouette(X_norm, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans, method=f\"Kmeans\", save=True)\n",
    "    cluster_centers = Normalizer_.inverse_transform(Kmeans.cluster_centers_)\n",
    "    plot_centroids(cluster_centers, testing, method=\"Kmeans\", save=False)\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms(root=\"temporal/clustering_results/Kmeans/\")#, root=\"temporal/clustering_results/Kmeans/\")\n",
    "    Kmeans_Results.joyplot()\n",
    "#     with open(f\"temporal/clustering_results/Kmeans/silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "#         pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPytBMrZU_hE"
   },
   "outputs": [],
   "source": [
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_PCA)\n",
    "    silhouette_score_Kmeans = metrics.silhouette_score(X_PCA, Kmeans.labels_)\n",
    "    plot_silhouette(X_PCA, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans, \"Kmeans_PCA\", save=True)\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms(root=\"temporal/clustering_results/Kmeans_PCA/\")\n",
    "#     Kmeans_Results.joyplot()\n",
    "    cluster_centers = PCA_.inverse_transform(Kmeans.cluster_centers_)\n",
    "    plot_centroids(cluster_centers, testing, method=\"Kmeans_PCA\")\n",
    "    with open(f\"temporal/clustering_results/Kmeans_PCA/PCA_silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1qEhX43O0Ux"
   },
   "source": [
    "## Kmeans TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXOW32KJxVdP"
   },
   "outputs": [],
   "source": [
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_TSNE)\n",
    "    silhouette_score_Kmeans = metrics.silhouette_score(X_TSNE, Kmeans.labels_)\n",
    "    plot_silhouette(X_TSNE, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans, method=f\"Kmeans_TSNE\", save=True)\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms(root=f\"temporal/clustering_results/Kmeans_TSNE/\")\n",
    "#     Kmeans_Results.joyplot()\n",
    "#     cluster_centers = reducer.inverse_transform(Kmeans.cluster_centers_)\n",
    "#     plot_centroids(cluster_centers, testing, method=\"PosAE_Kmeans_TSNE\")\n",
    "    with open(f\"temporal/clustering_results/Kmeans_TSNE/TSNE_silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXOW32KJxVdP"
   },
   "outputs": [],
   "source": [
    "clusters = [3, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "for id, n_cluster in enumerate(clusters):\n",
    "    Kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(X_UMAP)\n",
    "    silhouette_score_Kmeans = metrics.silhouette_score(X_UMAP, Kmeans.labels_)\n",
    "    plot_silhouette(X_UMAP, Kmeans.labels_, Kmeans.n_clusters, silhouette_score_Kmeans, method=f\"Kmeans_UMAP\", save=True)\n",
    "    Kmeans_Results = Clustering_Results(Kmeans, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms(root=f\"temporal/clustering_results/Kmeans_UMAP/\", save=True)\n",
    "#     Kmeans_Results.joyplot()\n",
    "#     cluster_centers = reducer.inverse_transform(Kmeans.cluster_centers_)\n",
    "#     plot_centroids(cluster_centers, testing, method=f\"Kmeans_UMAP\")\n",
    "    with open(f\"temporal/clustering_results/Kmeans_UMAP/UMAP_60c_silhouette_n-clusters_{Kmeans.n_clusters}\", \"wb\") as file:\n",
    "        pkl.dump(silhouette_score_Kmeans, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsupervised_AutoML:\n",
    "    def __init__(self, estimators=None, transformers=None):\n",
    "        self.estimators = estimators\n",
    "        self.transformers = transformers\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        fit_predict will train given estimator(s) and predict cluster membership for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        # This dictionary will hold predictions for each estimator\n",
    "        predictions = []\n",
    "        performance_metrics = {}\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            labels = estimator['estimator'](*estimator['args'], **estimator['kwargs']).fit_predict(X)\n",
    "            estimator['estimator'].n_clusters_ = len(np.unique(labels))\n",
    "            metrics = self._get_cluster_metrics(estimator['estimator'].__name__, estimator['estimator'].n_clusters_, X, labels, y)\n",
    "            predictions.append({estimator['estimator'].__name__: labels})\n",
    "            performance_metrics[estimator['estimator'].__name__] = metrics\n",
    "\n",
    "            self.predictions = predictions\n",
    "            self.performance_metrics = performance_metrics\n",
    "\n",
    "            return predictions, performance_metrics\n",
    "\n",
    "    # Printing cluster metrics for given arguments\n",
    "    def _get_cluster_metrics(self, name, n_clusters_, X, pred_labels, true_labels=None):\n",
    "        from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score,  adjusted_mutual_info_score, silhouette_score  \n",
    "           \n",
    "        print(\"\"\"################## %s metrics #####################\"\"\" % name)\n",
    "        if len(np.unique(pred_labels)) >= 2:\n",
    "\n",
    "            silh_co = silhouette_score(X, pred_labels)\n",
    "\n",
    "        if true_labels is not None:\n",
    "\n",
    "            h_score = homogeneity_score(true_labels, pred_labels)\n",
    "            c_score = completeness_score(true_labels, pred_labels)\n",
    "            vm_score = v_measure_score(true_labels, pred_labels)\n",
    "            adj_r_score = adjusted_rand_score(true_labels, pred_labels)\n",
    "            adj_mut_info_score = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "\n",
    "            metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "            \"Estimated number of clusters\": n_clusters_,\n",
    "            \"Homogeneity\": h_score,\n",
    "            \"Completeness\": c_score,\n",
    "            \"V-measure\": vm_score,\n",
    "            \"Adjusted Rand Index\": adj_r_score,\n",
    "            \"Adjusted Mutual Information\": adj_mut_info_score}\n",
    "        \n",
    "            for k, v in metrics.items():\n",
    "                print(\"t%s: %0.3f\" % (k, v))\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "        \"Estimated number of clusters\": n_clusters_}\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            print(\"t%s: %0.3f\" % (k, v))\n",
    "\n",
    "        else:\n",
    "            print(\"t# of predicted labels is {}, can not produce metrics. n\".format(np.unique(pred_labels)))\n",
    "            \n",
    "        return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C57GimN29u3n"
   },
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwYhNZXa2cyi"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "classes = []\n",
    "silhouette_score_DBSCAN = []\n",
    "i = 1\n",
    "EPS = [0.01, 0.1, 0.5, 1, 10, 20, 36, 50, 80, 100]\n",
    "for x in range(0, 9, 1):\n",
    "    eps = EPS[x]\n",
    "    db = DBSCAN(eps=eps, min_samples=4).fit(X_scaled)\n",
    "    # silhouette_score_DBSCAN.append(metrics.silhouette_score(X_PCA, db.labels_))\n",
    "    # plot_silhouette(X_PCA, db.labels_, 15, silhouette_score_DBSCAN[i-1], \"DBSCAN_PCA\")\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X_scaled[:,0], X_scaled[:,1], hue=[\"cluster-{}\".format(x) for x in labels]) \n",
    "    i += 1\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    Kmeans_Results = Clustering_Results(db, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "with open(f\"Clustering_Results/DBSCAN/Results/silhouette_n-clusters_{n_clusters_}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_DBSCAN, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GlIoJ1HqkLG"
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.8, min_samples=100).fit(X_norm)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "metrics.silhouette_score(X_scaled, db.labels_)\n",
    "sns.displot(db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLrRXdWWbE43"
   },
   "source": [
    "## DBSCAN PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyY4oXN3bCdT"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=.5, wspace=.2)\n",
    "classes = []\n",
    "silhouette_score_DBSCAN = []\n",
    "i = 1\n",
    "EPS = [0.01, 0.1, 0.5, 1, 10, 20, 36, 50, 80, 100]\n",
    "for eps in EPS:\n",
    "    db = DBSCAN(eps=eps, min_samples=4).fit(X_PCA)\n",
    "    # silhouette_score_DBSCAN.append(metrics.silhouette_score(X_PCA, db.labels_))\n",
    "    # plot_silhouette(X_PCA, db.labels_, 15, silhouette_score_DBSCAN[i-1], \"DBSCAN_PCA\")\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    \n",
    "    print(eps)\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    ax.text(1, 4, \"eps = {}\".format(round(eps, 1)), fontsize=25, ha=\"center\")\n",
    "    sns.scatterplot(X_PCA[:,0], X_PCA[:,1], hue=[\"cluster-{}\".format(x) for x in labels]) \n",
    "    i += 1\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "    Kmeans_Results = Clustering_Results(db, y, y_label=\"hour\", hist_library=\"plt\")\n",
    "    Kmeans_Results.histograms()\n",
    "with open(f\"Clustering_Results/DBSCAN_PCA/Results/silhouette_n-clusters_{n_clusters_}\", \"wb\") as file:\n",
    "    pkl.dump(silhouette_score_DBSCAN, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlaAjxquNufA"
   },
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "from kneed import KneeLocator\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=10)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "distances = np.sort(distances[:,9], axis=0)\n",
    "\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "knee.plot_knee()\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "print(f\"The knee is located at: {distances[knee.knee]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR2vGl44c3Gn"
   },
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=12,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=1,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPJwwqh9bx0v"
   },
   "outputs": [],
   "source": [
    "linkage = \"complete\"\n",
    "connectivity = False\n",
    "model = AgglomerativeClustering(\n",
    "                linkage=linkage, connectivity=connectivity, \n",
    "                n_clusters=3)\n",
    "model.fit(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npHhaDOWAsn9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "# X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "#                   [4, 2], [4, 4], [4, 0]])\n",
    "clustering = AgglomerativeClustering().fit(X_UMAP)\n",
    "n_clusters_ = len(set(clustering.labels_))\n",
    "print(n_clusters_)\n",
    "print(list(clustering.labels_).count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(clustering.labels_).count(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Kmeans_Results = Clustering_Results(clustering, y, y_label=\"hour\")\n",
    "sns.displot(clustering.labels_)\n",
    "Kmeans_Results.histograms(hist_library=\"sns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "complete_clustering = linkage(X_UMAP, method=\"complete\", metric=\"euclidean\")\n",
    "average_clustering = linkage(X_UMAP, method=\"average\", metric=\"euclidean\")\n",
    "single_clustering = linkage(X_UMAP, method=\"single\", metric=\"euclidean\")\n",
    "complete_clustering_labels = fcluster(complete_clustering,t=5,criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(complete_clustering_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "dendrogram(complete_clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "dendrogram(average_clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "dendrogram(single_clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0av3CxthbxXg"
   },
   "source": [
    "# %matplotlib qt\n",
    "from logging import raiseExceptions\n",
    "import librosa\n",
    "def plot_silhouette( X, cluster_labels, n_clusters, silhouette_avg, method=None, extra=None, save=False):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0,\n",
    "                    0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Silhouette_plot_{n_clusters}_{extra}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "\n",
    "def plot_centroids(cluster_centers, model, method, extra=\"\", save=True):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    model._model.to(\"cpu\")\n",
    "    for i, spec in enumerate(cluster_centers):\n",
    "        encodings = spec.reshape(64,9,9)\n",
    "        encodings = torch.tensor(encodings).float()\n",
    "        decodings = model._model.decoder(encodings).detach().numpy()\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.imshow(decodings[0, :, :], origin=\"lower\", cmap=\"viridis\")\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    n_cluster = len(cluster_centers)\n",
    "    if save == True:\n",
    "        plt.savefig(f\"temporal/clustering_results/{method}/Centroids_plot_{n_cluster}_{extra}.pdf\", format=\"pdf\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Ploted!\")\n",
    "        pass\n",
    "    \n",
    "\n",
    "def num_rows_cols(num_elements):\n",
    "    num_rows = int(math.sqrt(num_elements))\n",
    "    num_cols = (num_elements + num_rows - 1) // num_rows\n",
    "    return (num_rows, num_cols)\n",
    "\n",
    "def get_row_col(pos, cols):\n",
    "    row = pos // cols\n",
    "    col = pos % cols\n",
    "    return row, col\n",
    "\n",
    "class Clustering_Results:\n",
    "    def __init__(self, model, y, y_label=\"hour\", hist_library=\"plt\"):\n",
    "        self._labels_cluster = None\n",
    "        self._n_labels = None\n",
    "        self._label = y_label\n",
    "        self._model = model\n",
    "        self._n_clusters = len(set(model.labels_))\n",
    "        self.y = y\n",
    "        self._y = self.converter(y[self._label])\n",
    "        self._n_labels = set(self._y)\n",
    "\n",
    "    def converter(self, var):\n",
    "        aux = []\n",
    "        for i in range(len(var)):\n",
    "            aux.append(var[i].item())\n",
    "        return np.array(aux)\n",
    "\n",
    "    def one_cluster_eval(self, cluster):\n",
    "        index = np.where(self._model.labels_ == cluster)\n",
    "        index = list(index[0])\n",
    "        self._labels_cluster = self._y[index]\n",
    "        return self._labels_cluster\n",
    "    \n",
    "    def tagger(self, samples):\n",
    "        labels = []\n",
    "        labels_all_clusters = []\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "            labels.append(samples[index])\n",
    "        return labels\n",
    "\n",
    "    def joyplot(self):\n",
    "        labels_all_clusters = []\n",
    "        size_x = 8\n",
    "        size_y = 6\n",
    "        joy_vars = [\"hour\", \"recorder\"]\n",
    "        for cluster in range(self._n_clusters):\n",
    "            y_aux = []\n",
    "            labels_cluster = []\n",
    "            for i, label in enumerate(joy_vars):\n",
    "                y_aux.append(self.converter(self.y[label]))\n",
    "                index = np.where(self._model.labels_ == cluster)\n",
    "                index = list(index[0])\n",
    "                labels_cluster.append(y_aux[i][index])\n",
    "            df = pd.DataFrame({'recorder':labels_cluster[0], \"hour\":labels_cluster[1]})\n",
    "            if(self._label == \"hour\"):\n",
    "                joypy.joyplot(df, by=\"hour\", column=\"recorder\", range_style='own', \n",
    "                                grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                                title=f\"Cluster {cluster} \\nLabels distribution along recorders using recorders as rows\",\n",
    "                                colormap=cm.autumn_r, fade=False)\n",
    "                plt.xticks(np.arange(0,24),fontsize=20)\n",
    "                plt.yticks(fontsize=11)\n",
    "                \n",
    "#             plt.fontsize(11)\n",
    "            elif(self._label == \"recorder\"):\n",
    "                joypy.joyplot(df, by=\"recorder\", column=\"hour\", range_style='own', \n",
    "                                    grid=\"y\", hist=False, linewidth=1, legend=False, figsize=(size_x,size_y),\n",
    "                                    title=f\"Cluster {cluster} \\nLabels distribution along recorders using hours as rows\",\n",
    "                                    colormap=cm.autumn_r)\n",
    "\n",
    "#             plt.yticks(fontsize=22)\n",
    "            labels_all_clusters.append(index)\n",
    "            plt.show()\n",
    "#             print(len(labels_cluster))\n",
    "#             print(labels_cluster[1].shape)\n",
    "#             print(labels_cluster[0:10])\n",
    "#             print(index[0:20])\n",
    "        \n",
    "#         return labels_all_clusters\n",
    "            \n",
    "\n",
    "    def histograms(self, hist_library=\"plt\", method=None, root=None):\n",
    "        bins = list(self._n_labels)\n",
    "        print(bins)\n",
    "        num_rows, num_cols = num_rows_cols(self._n_clusters)\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 14))\n",
    "        if self._n_clusters <= 3:\n",
    "                axes = np.expand_dims(axes,0)\n",
    "                fig.set_figheight(6)\n",
    "                fig.set_figwidth(12)\n",
    "                if self._n_clusters == 1:\n",
    "                    axes = np.expand_dims(axes,0)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "        for hist in range(self._n_clusters):\n",
    "            aux = self.one_cluster_eval(hist)\n",
    "            ax_0, ax_1 = get_row_col(hist, num_cols)\n",
    "            if hist_library == \"plt\":\n",
    "                axes[ax_0][ax_1].hist(aux, histtype=\"bar\",\n",
    "                                      color=\"paleturquoise\", cumulative=False,\n",
    "                                      edgecolor='black', \n",
    "                                      linewidth=1.2, bins=bins, stacked=False)\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)\n",
    "            elif hist_library == \"sns\":\n",
    "                sns.distplot(aux,bins=np.arange(aux.min(), aux.max()+1),\n",
    "                             hist_kws=dict(edgecolor=\"black\", linewidth=1), \n",
    "                             ax=axes[ax_0, ax_1])\n",
    "                axes[ax_0][ax_1].set_title(f\"Cluster: {hist}\", size=16)              \n",
    "            else:\n",
    "                raise Exception(f\"Library {self._hist_library} unused\")\n",
    "                \n",
    "            if root != None:\n",
    "                plt.savefig(f\"temporal/clustering_results/{method}/Histograms_plot_{self._n_clusters}.pdf\", format=\"pdf\")\n",
    "            else:\n",
    "                pass\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "15FiCdrQdIiLbRJdrnLgeGSWmPZR-eIZZ",
     "timestamp": 1668314866472
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
